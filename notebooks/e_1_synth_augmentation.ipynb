{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e202683-cc61-40c0-a6bf-58241d6f8126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "[g_1] Using device: mps | N_CORES=4\n",
      "↻ All synthetic datasets already present for all methods and sizes — no work to do.\n",
      "Run completed (idempotent no-op).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_1_synth_augmentation.ipynb\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "Facet-driven generation and diverse selection of synthetic Dutch activity sentences\n",
    "using LLMs, strict validation, and K-center (medoid + D²) sampling to build\n",
    "nested-prefix datasets of sizes N ∈ {96, 192, 384, 768, 1536, 3072} per method\n",
    "(and COMBO when multiple methods are present).\n",
    "\n",
    "This script:\n",
    "1. Loads facet variable blocks\n",
    "   - Reads prompt variable JSONs per subdomain/facet (task, lexical palettes, examples).\n",
    "   - Orders facets deterministically (subdomain → facet) for reproducible streams.\n",
    "2. Configures environment & reproducibility\n",
    "   - Auto-selects CUDA/MPS/CPU and sets deterministic seeds.\n",
    "   - Tunes modest concurrency for generation; provides GPU memory hygiene helpers.\n",
    "3. Generates candidate pools per (facet, method)\n",
    "   - Supports GPT-4o (OpenAI) and local Ollama models (Gemma/Llama3/Mistral).\n",
    "   - Builds compact prompts (fixed rules + facet-specific block) requesting exactly K lines.\n",
    "   - Validates candidates: Dutch language gate, ≤20 tokens (warn >12), normalization,\n",
    "     and high-threshold dedupe (surface + cosine).\n",
    "   - Persists *per-facet* pools incrementally to disk and resumes from prior runs.\n",
    "4. Performs global near-duplicate filtering per method\n",
    "   - Cross-facet dedupe with a PROTECT_FLOOR to avoid starving any facet.\n",
    "   - Precomputes unit embeddings once per facet; cosine-based screening.\n",
    "5. Selects diverse sentences per facet\n",
    "   - K=27 via medoid start + D²-sampling (k-means++ style) on unit embeddings.\n",
    "   - Tracks intrinsic diversity metrics (mean/min pairwise 1−cosine).\n",
    "6. Assembles nested-prefix finals and optional COMBO sets\n",
    "   - Deterministic round-robin interleaving across facets to form global ordered lists.\n",
    "   - Writes g_final_n{N}_{method}.csv for each target size; builds COMBO when >1 method.\n",
    "7. Summarizes and logs\n",
    "   - Writes per-facet intrinsic metrics and a run summary of row counts.\n",
    "   - Streams all raw candidates with validator flags to JSONL; logs progress to file.\n",
    "\n",
    "Idempotency:\n",
    "• If all target finals exist with the required row counts for every method\n",
    "  (g_final_n{96|192|384|768|1536|3072}_{method}.csv), the script performs a\n",
    "  clean no-op and exits — no prompting, no pool building, no selection, no metrics.\n",
    "• If some finals are missing, the script resumes from per-facet pools in\n",
    "  outputs/e_1_synth_augmentation/facet_pools and only calls LLMs for facets\n",
    "  whose pools contain < POOL_MIN_FACET items.\n",
    "• Final writers are idempotent (skip files already complete). COMBO outputs are\n",
    "  written only when multiple methods are present.\n",
    "\n",
    "Inputs:\n",
    "- data/activities.csv, data/activity_scores.csv, data/domains.xlsx (Not in idempotent run)\n",
    "- outputs/e_1_synth_augmentation/prompt_variable_blocks/*.json\n",
    "- (optional) .env with OPENAI_API_KEY\n",
    "- (optional) Local Ollama models installed\n",
    "\n",
    "Outputs:\n",
    "- 'outputs/e_1_synth_augmentation/facet_pools/pool_{method}_{facet_id}.txt'\n",
    "- 'outputs/e_1_synth_augmentation/g_raw_{method}.jsonl'\n",
    "- 'outputs/e_1_synth_augmentation/g_intrinsic_metrics.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n96_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n192_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n384_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n768_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n1536_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n3072_{method}.csv'\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n96_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n192_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n384_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n768_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n1536_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/g_final_n3072_COMBO.csv' (only when multiple methods)\n",
    "- 'outputs/e_1_synth_augmentation/run_summary.csv'\n",
    "- 'outputs/e_1_synth_augmentation/run.log'\n",
    "\"\"\"\n",
    "\n",
    "# ────────────────────────────────────────────\n",
    "# Imports\n",
    "# ────────────────────────────────────────────\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "except Exception:\n",
    "    openai = None\n",
    "\n",
    "try:\n",
    "    import langid\n",
    "    _HAS_LANGID = True\n",
    "except Exception:\n",
    "    _HAS_LANGID = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    _NLP = spacy.load(\"nl_core_news_lg\")\n",
    "    _SPACY_STOPWORDS = _NLP.Defaults.stop_words\n",
    "except Exception:\n",
    "    _NLP = None\n",
    "    _SPACY_STOPWORDS = set()\n",
    "\n",
    "# Silence some warnings\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"The current process just got forked\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sentence_transformers\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Device & cores, seeds\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "DEVICE_STR = device.type\n",
    "N_CORES = 4\n",
    "SEED = 42\n",
    "RAND_SEED = SEED\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize(); torch.cuda.empty_cache()\n",
    "        if DEVICE_STR == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"[g_1] Using device: {DEVICE_STR} | N_CORES={N_CORES}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Paths \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def project_root(marker: str = \"LICENSE\") -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for d in (here, *here.parents):\n",
    "        if (d / marker).is_file():\n",
    "            return d\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "ROOT        = project_root()\n",
    "DATA_DIR    = ROOT / \"data\"\n",
    "\n",
    "PV_BLOCKS   = ROOT / \"config\" / \"prompt_variable_blocks\"\n",
    "\n",
    "G_OUT_DIR   = ROOT / \"outputs\" / \"e_1_synth_augmentation\"\n",
    "POOLS_DIR   = G_OUT_DIR / \"facet_pools\"\n",
    "RUN_SUMMARY = G_OUT_DIR / \"run_summary.csv\"\n",
    "LOG_FILE    = G_OUT_DIR / \"run.log\"\n",
    "\n",
    "for p in (G_OUT_DIR, POOLS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fpath in (RUN_SUMMARY, LOG_FILE):\n",
    "    fpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Logging\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for h in list(logging.root.handlers):\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=str(LOG_FILE),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Constants\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "METHODS = (\"gemma\",)                      # single-method run OK \n",
    "SIZES   = (96, 192, 384, 768, 1536, 3072)\n",
    "\n",
    "# Candidate pool targets per (facet, method)\n",
    "POOL_MIN_FACET = 48\n",
    "POOL_MAX_FACET = 72\n",
    "BLOCK_K        = 16                       # ask the LLM for exactly 16 lines per call\n",
    "POOL_TRIES_MAX = 8\n",
    "K_PER_FACET    = 27\n",
    "\n",
    "# Near-duplicate thresholds (HIGH; keep only near-identical as dup)\n",
    "NEAR_DUP_CHAR_RATIO = 0.975\n",
    "NEAR_DUP_EMB        = 0.995\n",
    "\n",
    "# Length limits\n",
    "MAX_TOKENS_HARD = 20\n",
    "SOFT_WARN_AT    = 12  # >12 = warn, but keep\n",
    "\n",
    "# Concurrency pacing\n",
    "SLEEP = 0.25\n",
    "\n",
    "PROTECT_FLOOR = max(K_PER_FACET, 32)    # protect each facet at 32 during global dedupe\n",
    "\n",
    "_RAW_LOCK = threading.Lock()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  0. Idempotency helpers\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _csv_n_rows(path: Path) -> int:\n",
    "    if not (path.exists() and path.stat().st_size):\n",
    "        return 0\n",
    "    try:\n",
    "        # header + rows; return data rows only\n",
    "        return max(0, sum(1 for _ in open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def finals_complete_for(method: str, sizes=SIZES) -> bool:\n",
    "    \"\"\"All target finals for this method exist and meet their required row count.\"\"\"\n",
    "    return all(_csv_n_rows(G_OUT_DIR / f\"g_final_n{N}_{method}.csv\") >= N for N in sizes)\n",
    "\n",
    "def all_methods_complete(methods=METHODS, sizes=SIZES) -> bool:\n",
    "    \"\"\"Every method has all target finals complete.\"\"\"\n",
    "    return all(finals_complete_for(m, sizes) for m in methods)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  1. Embedding ensemble\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "ENCODERS = [\n",
    "    \"jegormeister/bert-base-dutch-cased-snli\",\n",
    "    # \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "    # \"embaas/sentence-transformers-multilingual-e5-base\",\n",
    "]\n",
    "_MODELS: Dict[str, SentenceTransformer] = {}\n",
    "\n",
    "def _get_model(name: str) -> SentenceTransformer:\n",
    "    if name not in _MODELS:\n",
    "        _MODELS[name] = SentenceTransformer(name, device=DEVICE_STR)\n",
    "    return _MODELS[name]\n",
    "\n",
    "def encode_texts(texts: List[str], name: str, batch_size: int = 128) -> np.ndarray:\n",
    "    m = _get_model(name)\n",
    "    return m.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "def _unit_rows(A: np.ndarray) -> np.ndarray:\n",
    "    return A / np.clip(np.linalg.norm(A, axis=1, keepdims=True), 1e-9, None)\n",
    "\n",
    "def _unit(v: np.ndarray) -> np.ndarray:\n",
    "    n = float(np.linalg.norm(v));  return v / max(n, 1e-9)\n",
    "\n",
    "def ensemble_unit_vecs(per_encoder_vecs: List[np.ndarray]) -> np.ndarray:\n",
    "    U = np.stack([_unit(v) for v in per_encoder_vecs], axis=0)\n",
    "    return _unit(np.mean(U, axis=0))\n",
    "\n",
    "def embed_unit(texts: List[str]) -> np.ndarray:\n",
    "    if not texts:\n",
    "        return np.empty((0, 768), dtype=np.float32)\n",
    "    mats = []\n",
    "    for enc in ENCODERS:\n",
    "        mats.append(_unit_rows(encode_texts(texts, enc)))\n",
    "    # average unit vectors encoder-wise, then renormalize per row\n",
    "    E = np.stack(mats, axis=0).mean(axis=0)\n",
    "    return _unit_rows(E).astype(np.float32)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  2. Language gate & normalization\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "_COMMON_NL_FUN = {\"de\",\"het\",\"een\",\"en\",\"om\",\"te\",\"met\",\"voor\",\"naar\",\"bij\",\"als\",\"dat\",\"die\",\"er\",\"dan\",\"ook\",\"maar\"}\n",
    "_PUNCT_RE = re.compile(r\"[^\\w\\sÀ-ÖØ-öø-ÿ]\", flags=re.UNICODE)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"lowercase + remove punctuation; collapse whitespace; strip.\"\"\"\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = _PUNCT_RE.sub(\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_count(s: str) -> int:\n",
    "    return 0 if not s else len(s.split())\n",
    "\n",
    "def is_probably_dutch(txt: str) -> bool:\n",
    "    if not txt:\n",
    "        return False\n",
    "    s = txt.strip()\n",
    "    # 1) langid when strong\n",
    "    if _HAS_LANGID:\n",
    "        lang, score = langid.classify(s)\n",
    "        if lang == \"nl\" and score >= 0.85:\n",
    "            return True\n",
    "        if lang != \"nl\" and score >= 0.85:\n",
    "            return False\n",
    "    # 2) heuristic: stopword/function rate\n",
    "    toks = re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", s.lower())\n",
    "    if not toks:\n",
    "        return False\n",
    "    hits = sum(1 for t in toks if t in _SPACY_STOPWORDS or t in _COMMON_NL_FUN)\n",
    "    rate = hits / max(1, len(toks))\n",
    "    if rate >= 0.20:\n",
    "        return True\n",
    "    # 3) negative English cue\n",
    "    if any(t in {\"the\",\"and\",\"to\",\"a\",\"for\",\"or\"} for t in toks):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  3. Dedupe helpers\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def char_sim(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, (a or \"\").lower(), (b or \"\").lower()).ratio()\n",
    "\n",
    "def soft_near_dedup(texts: List[str], U: Optional[np.ndarray]=None) -> List[str]:\n",
    "    \"\"\"\n",
    "    High-threshold near-dup removal (surface + embedding). Preserve order.\n",
    "    If U given (unit embeddings matching texts), used for cosine screening.\n",
    "    \"\"\"\n",
    "    keep, keep_vecs = [], []\n",
    "    for i, t in enumerate(texts):\n",
    "        low = t.lower().strip()\n",
    "        # surface\n",
    "        if any(char_sim(low, u.lower().strip()) >= NEAR_DUP_CHAR_RATIO for u in keep):\n",
    "            continue\n",
    "        if U is not None:\n",
    "            v = U[i]\n",
    "            if any(float(np.dot(v, uvec)) >= NEAR_DUP_EMB for uvec in keep_vecs):\n",
    "                continue\n",
    "            keep_vecs.append(v)\n",
    "        keep.append(t)\n",
    "    return keep\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  4. K-center selection: medoid + D²-sampling (k-means++)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def medoid_index(U: np.ndarray) -> int:\n",
    "    \"\"\"Return index of row with highest mean cosine to all others (most 'central').\"\"\"\n",
    "    if len(U) == 0:\n",
    "        return -1\n",
    "    cos_with_all = U @ U.T\n",
    "    means = cos_with_all.mean(axis=1)\n",
    "    return int(np.argmax(means))\n",
    "\n",
    "def kcenter_d2_select(texts: List[str], U: np.ndarray, k: int, rng: random.Random) -> List[int]:\n",
    "    \"\"\"\n",
    "    Return indices of selected items via:\n",
    "      1) medoid start\n",
    "      2) k-1 steps of D²-sampling: pick j with probability ∝ (min_distance_to_S)^2\n",
    "    \"\"\"\n",
    "    n = len(texts)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    if n <= k:\n",
    "        return list(range(n))\n",
    "    start = medoid_index(U)\n",
    "    S = [start]\n",
    "    dmin = 1.0 - (U @ U[start])\n",
    "    dmin[start] = 0.0\n",
    "    while len(S) < k:\n",
    "        w = np.square(np.clip(dmin, 0.0, 1.0))\n",
    "        s = float(np.sum(w))\n",
    "        if s <= 1e-12:\n",
    "            j = int(np.argmax(dmin))\n",
    "        else:\n",
    "            p = (w / s).astype(np.float64)\n",
    "            r = rng.random()\n",
    "            cum = np.cumsum(p)\n",
    "            j = int(np.searchsorted(cum, r))\n",
    "        if j in S:\n",
    "            cand = np.argsort(-dmin)\n",
    "            for jj in cand:\n",
    "                if int(jj) not in S:\n",
    "                    j = int(jj); break\n",
    "        S.append(j)\n",
    "        dnew = 1.0 - (U @ U[j])\n",
    "        dmin = np.minimum(dmin, dnew)\n",
    "    return S\n",
    "\n",
    "def pairwise_diversity(U_sel: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return mean/min pairwise (1 - cosine) among selected rows.\"\"\"\n",
    "    if U_sel.shape[0] <= 1:\n",
    "        return dict(mean_pairwise_dist=0.0, min_pairwise_dist=0.0)\n",
    "    S = U_sel @ U_sel.T\n",
    "    iu = np.triu_indices(S.shape[0], k=1)\n",
    "    d = 1.0 - S[iu]\n",
    "    return dict(mean_pairwise_dist=float(np.mean(d)),\n",
    "                min_pairwise_dist=float(np.min(d)))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  5. OpenAI / Ollama setup + prompts\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "OPENAI_MODEL       = \"gpt-4o-mini\"\n",
    "OPENAI_TEMPERATURE = 0.8\n",
    "OPENAI_SYSTEM_PROMPT = (\n",
    "    \"Je bent een educatieve copywriter. Schrijf korte, kindvriendelijke \"\n",
    "    \"Nederlandse activiteitenzinnen volgens de regels.\"\n",
    ")\n",
    "\n",
    "def have_openai() -> bool:\n",
    "    try:\n",
    "        key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        if openai is None:\n",
    "            return False\n",
    "        openai.api_key = key\n",
    "        return bool(key)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ollama_installed() -> bool:\n",
    "    return shutil.which(\"ollama\") is not None\n",
    "\n",
    "OLLAMA_MODELS = {\n",
    "    \"gemma\":   \"gemma2:9b-instruct-q4_0\",\n",
    "    \"llama3\":  \"llama3:8b-instruct-q4_0\",\n",
    "    \"mistral\": \"mistral:7b-instruct-v0.2-q4_0\",\n",
    "}\n",
    "\n",
    "def _gpt_call(messages, max_tokens=600, temperature=OPENAI_TEMPERATURE, timeout=60, tries=3) -> str:\n",
    "    last = None\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            if hasattr(openai, \"OpenAI\"):\n",
    "                client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "                r = client.chat.completions.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=messages,\n",
    "                    timeout=timeout,\n",
    "                )\n",
    "                return (r.choices[0].message.content or \"\").strip()\n",
    "            # legacy fallback\n",
    "            r = openai.ChatCompletion.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=messages,\n",
    "                request_timeout=timeout,\n",
    "            )\n",
    "            return (r[\"choices\"][0][\"message\"][\"content\"] or \"\").strip()\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            time.sleep(2 ** i)\n",
    "    raise last\n",
    "\n",
    "def call_ollama_model(prompt: str, model: str, max_retries=3, timeout=180) -> Optional[str]:\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            res = subprocess.run(\n",
    "                [\"ollama\", \"run\", model],\n",
    "                input=prompt, text=True,\n",
    "                capture_output=True, timeout=timeout\n",
    "            )\n",
    "            if res.returncode == 0:\n",
    "                return res.stdout.strip()\n",
    "            log.warning(\"Ollama exit code %d: %s\", res.returncode, res.stderr.strip())\n",
    "        except Exception as e:\n",
    "            log.warning(\"Ollama error: %s\", e)\n",
    "        time.sleep(2 ** i)\n",
    "    return None\n",
    "\n",
    "# Prompt builder (fixed + facet-variable)\n",
    "def build_facet_prompt(facet: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact prompt that asks for EXACTLY BLOCK_K numbered lines,\n",
    "    NL only, lowercase, no punctuation, 5–12 tokens, no intro/outro.\n",
    "    \"\"\"\n",
    "    domain      = str(facet.get(\"domain\",\"\")).strip()\n",
    "    subdomain   = str(facet.get(\"subdomain\",\"\")).strip()\n",
    "    facet_name  = str(facet.get(\"facet\",\"\")).strip()\n",
    "    task        = str(facet.get(\"task\",\"\")).strip()\n",
    "    L           = facet.get(\"lexical\", {}) or {}\n",
    "    verbs       = \", \".join(L.get(\"verbs\", [])[:24])\n",
    "    objects     = \", \".join(L.get(\"objects\", [])[:24])\n",
    "    settings    = \", \".join(L.get(\"settings\", [])[:24])\n",
    "    exs         = facet.get(\"examples\", []) or []\n",
    "\n",
    "    examples_block = \"\"\n",
    "    if exs:\n",
    "        exs = [normalize_text(x) for x in exs[:3] if x]\n",
    "        exs = [x for x in exs if x]\n",
    "        if exs:\n",
    "            examples_block = \"VOORBEELDEN:\\n\" + \"\\n\".join(f\"- {x}\" for x in exs) + \"\\n\"\n",
    "\n",
    "    lexical_block = \"\"\n",
    "    if verbs or objects or settings:\n",
    "        lexical_block = \"LEXICALE PALETTEN (inspiratie, niet verplicht):\\n\"\n",
    "        if verbs:   lexical_block += f\"- werkwoorden: {verbs}\\n\"\n",
    "        if objects: lexical_block += f\"- objecten: {objects}\\n\"\n",
    "        if settings:lexical_block += f\"- settings: {settings}\\n\"\n",
    "\n",
    "    fixed = (\n",
    "        \"Je bent een copy-writer voor een educatieve game.\\n\"\n",
    "        \"Schrijf NEDERLANDSTALIGE, KORTE, KINDVRIENDELIJKE zinnen die één concrete activiteit uitdrukken.\\n\"\n",
    "        \"REGELS:\\n\"\n",
    "        \"- uitsluitend Nederlands; geen emoji; geen Engelse woorden\\n\"\n",
    "        f\"- precies {BLOCK_K} regels; elke regel begint met '1.'..'{BLOCK_K}'\\n\"\n",
    "        \"- elke zin 5–12 tokens; alles in lowercase\\n\"\n",
    "        \"- geen leestekens (punt, komma, streepje, dubbele punt, etc.)\\n\"\n",
    "        \"- geen inleiding of afsluiting; geef alleen de zinnen\\n\"\n",
    "        \"- gebruik de locatie alleen als inspiratie; noem de locatie niet letterlijk\\n\"\n",
    "    )\n",
    "    variable = (\n",
    "        f\"ASPECT: {domain} → {subdomain}\\n\"\n",
    "        f\"SUBASPECT/FACET: {facet_name}\\n\"\n",
    "        f\"OPDRACHT: {task}\\n\"\n",
    "    )\n",
    "    fmt = (\n",
    "        \"\\nFORMAT:\\n\"\n",
    "        f\"Geef {BLOCK_K} regels, exact genummerd als:\\n\"\n",
    "        \"1. <zin>\\n2. <zin>\\n...\\n\"\n",
    "        f\"{BLOCK_K}. <zin>\\n\"\n",
    "    )\n",
    "    return f\"{fixed}\\n{variable}\\n{lexical_block}\\n{examples_block}\\n{fmt}\"\n",
    "\n",
    "def parse_numbered_lines(block: str, k: int=BLOCK_K) -> List[str]:\n",
    "    lines = [ln.lstrip(\"-•* \\t\").strip() for ln in (block or \"\").splitlines()]\n",
    "    outs = []\n",
    "    for ln in lines:\n",
    "        if not ln:\n",
    "            continue\n",
    "        m = re.match(r\"^(\\d+)[\\.\\)\\-]\\s+(.*)$\", ln)\n",
    "        txt = (m.group(2) if m else ln).strip(' \"“”')\n",
    "        outs.append(txt)\n",
    "    cleaned = []\n",
    "    for t in outs:\n",
    "        t2 = normalize_text(t)\n",
    "        n = token_count(t2)\n",
    "        if 1 <= n <= MAX_TOKENS_HARD and t2 not in cleaned:\n",
    "            cleaned.append(t2)\n",
    "        if len(cleaned) == k:\n",
    "            break\n",
    "    return cleaned\n",
    "\n",
    "def gen_block_for_facet(facet: dict, method: str) -> List[str]:\n",
    "    prompt = build_facet_prompt(facet)\n",
    "    if method == \"gpt4o\" and have_openai():\n",
    "        try:\n",
    "            raw = _gpt_call(\n",
    "                [{\"role\":\"system\",\"content\":OPENAI_SYSTEM_PROMPT},\n",
    "                 {\"role\":\"user\",\"content\":prompt}],\n",
    "                max_tokens=1200, timeout=60, tries=3\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log.warning(\"OpenAI fail (%s): %s\", facet.get(\"facet_id\",\"?\"), e)\n",
    "            raw = \"\"\n",
    "    elif method in OLLAMA_MODELS and ollama_installed():\n",
    "        raw = call_ollama_model(prompt, model=OLLAMA_MODELS[method]) or \"\"\n",
    "    else:\n",
    "        return []\n",
    "    return parse_numbered_lines(raw, k=BLOCK_K)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6. Loading facet variable blocks\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_facet_entries() -> List[dict]:\n",
    "    entries = []\n",
    "    for p in sorted(PV_BLOCKS.glob(\"vb_*.json\")):\n",
    "        try:\n",
    "            data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(data, list):\n",
    "                for obj in data:\n",
    "                    entries.append({\n",
    "                        \"domain_id\":   str(obj.get(\"domain_id\",\"\")).strip(),\n",
    "                        \"subdomain_id\":str(obj.get(\"subdomain_id\",\"\")).strip(),\n",
    "                        \"facet_id\":    str(obj.get(\"facet_id\",\"\")).strip(),\n",
    "                        \"domain\":      obj.get(\"domain\",\"\"),\n",
    "                        \"subdomain\":   obj.get(\"subdomain\",\"\"),\n",
    "                        \"facet\":       obj.get(\"facet\",\"\"),\n",
    "                        \"task\":        obj.get(\"task\",\"\"),\n",
    "                        \"lexical\":     obj.get(\"lexical\", {}),\n",
    "                        \"examples\":    obj.get(\"examples\", []),\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            log.warning(\"Failed to read %s: %s\", p.name, e)\n",
    "    def id_key(fid: str) -> Tuple:\n",
    "        parts = re.split(r\"[^\\d]+\", fid or \"\")\n",
    "        try:\n",
    "            return tuple(int(x) for x in parts if x != \"\")\n",
    "        except Exception:\n",
    "            return (fid,)\n",
    "    entries = [e for e in entries if e.get(\"facet_id\")]\n",
    "    entries.sort(key=lambda e: (id_key(e[\"subdomain_id\"]), id_key(e[\"facet_id\"])))\n",
    "    return entries\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  7. Staging writer\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def raw_writer(method: str):\n",
    "    path = G_OUT_DIR / f\"g_raw_{method}.jsonl\"\n",
    "    def write(obj: dict):\n",
    "        line = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "        with _RAW_LOCK:\n",
    "            with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(line)\n",
    "    def close():\n",
    "        pass  # no per-thread file handle to close anymore\n",
    "    return write, close\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  8. Facet-pool persistence (immediate checkpointing)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "FSYNC_EVERY_WRITE = False  # set True if you want OS-level durability on every line\n",
    "\n",
    "def _safe_fid(fid: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\-\\.]+\", \"_\", fid or \"unknown\")\n",
    "\n",
    "def pool_path(method: str, facet_id: str) -> Path:\n",
    "    return POOLS_DIR / f\"pool_{method}_{_safe_fid(facet_id)}.txt\"\n",
    "\n",
    "def load_persisted_pool(method: str, facet_id: str) -> List[str]:\n",
    "    p = pool_path(method, facet_id)\n",
    "    if not (p.exists() and p.stat().st_size):\n",
    "        return []\n",
    "    lines = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            t = ln.strip()\n",
    "            if t:\n",
    "                lines.append(t)\n",
    "    # Ensure uniqueness & normalized (idempotent)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for t in lines:\n",
    "        tt = normalize_text(t)\n",
    "        if tt and tt not in seen:\n",
    "            seen.add(tt)\n",
    "            uniq.append(tt)\n",
    "    return uniq\n",
    "\n",
    "def pool_writer(method: str, facet_id: str):\n",
    "    p = pool_path(method, facet_id)\n",
    "    f = open(p, \"a\", encoding=\"utf-8\")\n",
    "    def write_line(text: str):\n",
    "        f.write(text + \"\\n\")\n",
    "        f.flush()\n",
    "        if FSYNC_EVERY_WRITE:\n",
    "            os.fsync(f.fileno())\n",
    "    def close():\n",
    "        try: f.close()\n",
    "        except Exception: pass\n",
    "    return write_line, close\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  9. Pool generation per (facet, method) with validation & facet-level dedupe\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def make_pool_for_facet(facet: dict, method: str) -> Tuple[List[str], Dict[str,int]]:\n",
    "    \"\"\"\n",
    "    Returns (texts_pool, counters) where pool contains normalized, NL-only, ≤20 tokens,\n",
    "    deduped (exact + near-dup) at facet level, up to POOL_MAX_FACET.\n",
    "    Also returns counters: {'gen':..., 'pass':..., 'fail_lang':..., 'fail_len':..., 'warn_len':..., 'resume':...}\n",
    "\n",
    "    NEW:\n",
    "    - Loads any previously accepted lines for this (facet,method) from disk.\n",
    "    - On every new acceptance, appends to the facet pool file immediately.\n",
    "    \"\"\"\n",
    "    write_raw, close_raw = raw_writer(method)\n",
    "    write_pool, close_pool = pool_writer(method, facet.get(\"facet_id\",\"\"))\n",
    "\n",
    "    cnt = defaultdict(int)\n",
    "\n",
    "    # Resume from disk\n",
    "    facet_id = facet.get(\"facet_id\",\"\")\n",
    "    resumed = load_persisted_pool(method, facet_id)\n",
    "    seen_texts: List[str] = list(resumed)\n",
    "    seen_set = set(resumed)\n",
    "    cnt[\"resume\"] = len(resumed)\n",
    "\n",
    "    tries = 0\n",
    "\n",
    "    def accept_and_log(text: str, why: str, passed: bool, warns: Dict[str,bool]):\n",
    "        obj = {\n",
    "            \"domain_id\": facet.get(\"domain_id\",\"\"),\n",
    "            \"subdomain_id\": facet.get(\"subdomain_id\",\"\"),\n",
    "            \"facet_id\": facet_id,\n",
    "            \"method\": method,\n",
    "            \"text\": text,\n",
    "            \"passed\": bool(passed),\n",
    "            \"reason\": \"\" if passed else why,\n",
    "            \"warn_len_gt12\": bool(warns.get(\"warn_len_gt12\", False)),\n",
    "            \"token_len\": token_count(text),\n",
    "            \"ts\": time.time(),\n",
    "        }\n",
    "        write_raw(obj)\n",
    "        # Persist as soon as we accept a new unique sentence\n",
    "        if passed:\n",
    "            write_pool(text)\n",
    "\n",
    "    # If we already have enough, return quickly (still cap by POOL_MAX_FACET)\n",
    "    if len(seen_texts) >= POOL_MIN_FACET:\n",
    "        log.info(\"facet %s: resume=%d ≥ POOL_MIN_FACET (%d) → no new gen\",\n",
    "                 facet_id, len(seen_texts), POOL_MIN_FACET)   \n",
    "        close_raw(); close_pool()\n",
    "        return seen_texts[:POOL_MAX_FACET], {\"gen\":0, **cnt}\n",
    "\n",
    "    while len(seen_texts) < POOL_MIN_FACET and tries < POOL_TRIES_MAX:\n",
    "        tries += 1\n",
    "        cands = gen_block_for_facet(facet, method) or []\n",
    "        cnt[\"gen\"] += len(cands)\n",
    "\n",
    "        for cand in cands:\n",
    "            t = normalize_text(cand)\n",
    "            if not t:\n",
    "                continue\n",
    "\n",
    "            if not is_probably_dutch(t):\n",
    "                cnt[\"fail_lang\"] += 1\n",
    "                accept_and_log(t, \"lang\", False, {})\n",
    "                continue\n",
    "\n",
    "            n_tok = token_count(t)\n",
    "            if n_tok == 0 or n_tok > MAX_TOKENS_HARD:\n",
    "                cnt[\"fail_len\"] += 1\n",
    "                accept_and_log(t, \"len\", False, {})\n",
    "                continue\n",
    "\n",
    "            warns = {}\n",
    "            if n_tok > SOFT_WARN_AT:\n",
    "                warns[\"warn_len_gt12\"] = True\n",
    "                cnt[\"warn_len\"] += 1\n",
    "\n",
    "            # facet-level near-dupe\n",
    "            dup_surface = any(char_sim(t, u) >= NEAR_DUP_CHAR_RATIO for u in seen_texts)\n",
    "            if dup_surface:\n",
    "                accept_and_log(t, \"dup_surface\", False, warns)\n",
    "                continue\n",
    "\n",
    "            if seen_texts:\n",
    "                sample_cmp = seen_texts[-64:]\n",
    "                U = embed_unit(sample_cmp + [t])\n",
    "                v = U[-1]\n",
    "                dup_emb = any(float(np.dot(v, u)) >= NEAR_DUP_EMB for u in U[:-1])\n",
    "                if dup_emb:\n",
    "                    accept_and_log(t, \"dup_emb\", False, warns)\n",
    "                    continue\n",
    "\n",
    "            if t not in seen_set:\n",
    "                seen_texts.append(t)\n",
    "                seen_set.add(t)\n",
    "                cnt[\"pass\"] += 1\n",
    "                accept_and_log(t, \"\", True, warns)\n",
    "\n",
    "            if len(seen_texts) >= POOL_MAX_FACET:\n",
    "                break\n",
    "\n",
    "        if len(seen_texts) < POOL_MIN_FACET:\n",
    "            time.sleep(SLEEP)\n",
    "\n",
    "    close_raw()\n",
    "    close_pool()\n",
    "    return seen_texts[:POOL_MAX_FACET], cnt\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  10. Global near-dup across all facet-pools (per method) with PROTECT_FLOOR\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def global_dedupe_pools(perfacet_pools: Dict[str, List[str]],\n",
    "                        facets_meta: Dict[str, dict],\n",
    "                        protect_floor: int = PROTECT_FLOOR) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Cross-facet near-dup removal with a protect floor.\n",
    "    OPTIMIZED: precompute embeddings once per facet and reuse; never re-embed kept items.\n",
    "    \"\"\"\n",
    "    def id_tuple(x: str) -> Tuple:\n",
    "        parts = re.split(r\"[^\\d]+\", x or \"\")\n",
    "        try: return tuple(int(p) for p in parts if p)\n",
    "        except: return (x,)\n",
    "\n",
    "    facet_ids = sorted(perfacet_pools.keys(),\n",
    "                       key=lambda fid: (id_tuple(facets_meta[fid][\"subdomain_id\"]), id_tuple(fid)))\n",
    "\n",
    "    # Precompute embeddings per facet ONCE\n",
    "    emb_by_facet: Dict[str, np.ndarray] = {}\n",
    "    emb_dim = None\n",
    "    for fid in facet_ids:\n",
    "        texts = perfacet_pools.get(fid, [])\n",
    "        U = embed_unit(texts) if texts else np.empty((0, 768), dtype=np.float32)\n",
    "        emb_by_facet[fid] = U\n",
    "        if emb_dim is None and U.shape[0] > 0:\n",
    "            emb_dim = U.shape[1]\n",
    "    if emb_dim is None:\n",
    "        emb_dim = 768  # default\n",
    "\n",
    "    new_pools: Dict[str, List[str]] = {fid: [] for fid in facet_ids}\n",
    "\n",
    "    kept_texts: List[str] = []\n",
    "    kept_owner: List[str] = []\n",
    "    kept_vecs = np.empty((0, emb_dim), dtype=np.float32)  # rows = kept, cols = dims\n",
    "    order_index = {fid: i for i, fid in enumerate(facet_ids)}\n",
    "\n",
    "    for fid in facet_ids:\n",
    "        texts = perfacet_pools.get(fid, [])\n",
    "        Ufacet = emb_by_facet[fid]\n",
    "        for i, t in enumerate(texts):\n",
    "            dup_idx = None\n",
    "\n",
    "            # 1) surface\n",
    "            for idx, u in enumerate(kept_texts):\n",
    "                if char_sim(t, u) >= NEAR_DUP_CHAR_RATIO:\n",
    "                    dup_idx = idx; break\n",
    "\n",
    "            # 2) embedding\n",
    "            if dup_idx is None and kept_vecs.shape[0] > 0:\n",
    "                v = Ufacet[i]\n",
    "                sims = kept_vecs @ v\n",
    "                j = int(np.argmax(sims))\n",
    "                if float(sims[j]) >= NEAR_DUP_EMB:\n",
    "                    dup_idx = j\n",
    "\n",
    "            if dup_idx is None:\n",
    "                # accept\n",
    "                new_pools[fid].append(t)\n",
    "                kept_texts.append(t)\n",
    "                kept_owner.append(fid)\n",
    "                v = Ufacet[i][None, :]\n",
    "                kept_vecs = np.vstack([kept_vecs, v])\n",
    "                continue\n",
    "\n",
    "            # conflict: decide who drops\n",
    "            prev_fid = kept_owner[dup_idx]\n",
    "            prev_count = len(new_pools[prev_fid])\n",
    "            curr_count = len(new_pools[fid])\n",
    "\n",
    "            prev_can_drop = prev_count > protect_floor\n",
    "            curr_can_drop = curr_count > protect_floor\n",
    "\n",
    "            def remove_from_previous():\n",
    "                target = kept_texts[dup_idx]\n",
    "                lst = new_pools[prev_fid]\n",
    "                rm_i = None\n",
    "                for k_i, s in enumerate(lst):\n",
    "                    if char_sim(s, target) >= NEAR_DUP_CHAR_RATIO:\n",
    "                        rm_i = k_i; break\n",
    "                if rm_i is not None:\n",
    "                    lst.pop(rm_i)\n",
    "                # remove from kept buffers\n",
    "                kept_texts.pop(dup_idx)\n",
    "                kept_owner.pop(dup_idx)\n",
    "                nonlocal kept_vecs\n",
    "                kept_vecs = np.delete(kept_vecs, dup_idx, axis=0)\n",
    "\n",
    "            if prev_count > curr_count and prev_can_drop:\n",
    "                remove_from_previous()\n",
    "                # accept current\n",
    "                new_pools[fid].append(t)\n",
    "                kept_texts.append(t)\n",
    "                kept_owner.append(fid)\n",
    "                v = Ufacet[i][None, :]\n",
    "                kept_vecs = np.vstack([kept_vecs, v])\n",
    "            elif curr_count > prev_count and curr_can_drop:\n",
    "                # drop current\n",
    "                continue\n",
    "            else:\n",
    "                if order_index[prev_fid] <= order_index[fid]:\n",
    "                    continue\n",
    "                else:\n",
    "                    if prev_can_drop:\n",
    "                        remove_from_previous()\n",
    "                        new_pools[fid].append(t)\n",
    "                        kept_texts.append(t)\n",
    "                        kept_owner.append(fid)\n",
    "                        v = Ufacet[i][None, :]\n",
    "                        kept_vecs = np.vstack([kept_vecs, v])\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    return new_pools\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  11. Per-(facet,method) selection K=27 via medoid + D²\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def select_k_for_facet(pool_texts: List[str], k: int=27, method:str=\"\", facet_id:str=\"\") -> Tuple[List[str], Dict[str,float]]:\n",
    "    if not pool_texts:\n",
    "        return [], {\"mean_pairwise_dist\": np.nan, \"min_pairwise_dist\": np.nan}\n",
    "    U = embed_unit(pool_texts)\n",
    "    rng = random.Random(f\"{RAND_SEED}|select|{method}|{facet_id}\")\n",
    "    idx = kcenter_d2_select(pool_texts, U, k, rng)\n",
    "    chosen = [pool_texts[i] for i in idx]\n",
    "    div = pairwise_diversity(U[idx])\n",
    "    return chosen, div\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  12. Build global finals per method & size via deterministic round-robin\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def build_global_finals(selected_map: Dict[str, Dict[str, List[str]]],\n",
    "                        facets_meta: Dict[str, dict]) -> Dict[str, Dict[int, pd.DataFrame]]:\n",
    "    def sort_key(fid: str) -> Tuple:\n",
    "        def idt(x: str) -> Tuple:\n",
    "            parts = re.split(r\"[^\\d]+\", x or \"\")\n",
    "            try: return tuple(int(p) for p in parts if p)\n",
    "            except: return (x,)\n",
    "        meta = facets_meta[fid]\n",
    "        return (idt(meta[\"subdomain_id\"]), idt(fid))\n",
    "\n",
    "    finals = defaultdict(dict)\n",
    "    for method, perfacet in selected_map.items():\n",
    "        facet_ids = sorted(perfacet.keys(), key=sort_key)\n",
    "        streams = {fid: list(perfacet.get(fid, [])) for fid in facet_ids}\n",
    "        max_len = max((len(v) for v in streams.values()), default=0)\n",
    "        rr_stream: List[Tuple[str,str]] = []\n",
    "        for depth in range(max_len):\n",
    "            for fid in facet_ids:\n",
    "                lst = streams[fid]\n",
    "                if depth < len(lst):\n",
    "                    rr_stream.append((fid, lst[depth]))\n",
    "        for N in SIZES:\n",
    "            rows = []\n",
    "            for i, (fid, txt) in enumerate(rr_stream[:N]):\n",
    "                meta = facets_meta[fid]\n",
    "                rows.append({\n",
    "                    \"domain_id\":    meta[\"domain_id\"],\n",
    "                    \"subdomain_id\": meta[\"subdomain_id\"],\n",
    "                    \"facet_id\":     fid,\n",
    "                    \"method\":       method,\n",
    "                    \"rank\":         i+1,\n",
    "                    \"text\":         txt,\n",
    "                    \"passed\":       True,\n",
    "                })\n",
    "            finals[method][N] = pd.DataFrame(rows, columns=[\"domain_id\",\"subdomain_id\",\"facet_id\",\"method\",\"rank\",\"text\",\"passed\"])\n",
    "    return finals\n",
    "\n",
    "def write_finals_and_combo(finals: Dict[str, Dict[int, pd.DataFrame]]):\n",
    "    # Per-method finals: only write when needed\n",
    "    for method, sizes in finals.items():\n",
    "        for N, df in sizes.items():\n",
    "            out = G_OUT_DIR / f\"g_final_n{N}_{method}.csv\"\n",
    "            if out.exists() and _csv_n_rows(out) >= N:\n",
    "                log.info(\"↻ Exists & complete; skip writing %s\", out.name)\n",
    "                continue\n",
    "            df.to_csv(out, index=False)\n",
    "            log.info(\"✔ Wrote %s (%d rows)\", out.name, len(df))\n",
    "\n",
    "    # COMBO finals only when >1 method; skip logic\n",
    "    if len(METHODS) <= 1:\n",
    "        log.info(\"Single-method run; skipping COMBO outputs.\")\n",
    "        return\n",
    "\n",
    "    for N in SIZES:\n",
    "        out = G_OUT_DIR / f\"g_final_n{N}_COMBO.csv\"\n",
    "        # Build round-robin from already constructed finals\n",
    "        rr_rows = []\n",
    "        k = 0\n",
    "        while True:\n",
    "            progress = False\n",
    "            for m in METHODS:\n",
    "                dfm = finals.get(m, {}).get(N, pd.DataFrame())\n",
    "                if k < len(dfm):\n",
    "                    rr_rows.append(dfm.iloc[k].to_dict())\n",
    "                    progress = True\n",
    "            if not progress:\n",
    "                break\n",
    "            k += 1\n",
    "\n",
    "        if not rr_rows:\n",
    "            continue\n",
    "\n",
    "        if out.exists() and _csv_n_rows(out) >= N:\n",
    "            log.info(\"↻ Exists & complete; skip writing %s\", out.name)\n",
    "            continue\n",
    "\n",
    "        cols = [\"domain_id\",\"subdomain_id\",\"facet_id\",\"method\",\"rank\",\"text\",\"passed\"]\n",
    "        pd.DataFrame(rr_rows, columns=cols).to_csv(out, index=False)\n",
    "        log.info(\"✔ Wrote %s (%d rows)\", out.name, len(rr_rows))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  13. Intrinsic metrics writer\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def write_intrinsic_metrics(metrics_rows: List[dict]):\n",
    "    if not metrics_rows:\n",
    "        return\n",
    "    p = G_OUT_DIR / \"g_intrinsic_metrics.csv\"\n",
    "    df = pd.DataFrame(metrics_rows)\n",
    "    if p.exists() and p.stat().st_size:\n",
    "        df.to_csv(p, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(p, index=False)\n",
    "\n",
    "\n",
    "def _csv_n_rows(path: Path) -> int:\n",
    "    if not (path.exists() and path.stat().st_size):\n",
    "        return 0\n",
    "    # header + rows; return data rows only\n",
    "    try:\n",
    "        return max(0, sum(1 for _ in open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  14. Main function\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def run():\n",
    "    log.info(\"=== step_e_1 started ===\")\n",
    "    facet_entries = load_facet_entries()\n",
    "    ids = [e[\"facet_id\"] for e in facet_entries]\n",
    "    log.info(\"Loaded %d unique facets; theoretical max rows = %d\",\n",
    "         len(ids), len(ids)*K_PER_FACET)\n",
    "\n",
    "    for k,v in Counter(ids).items():\n",
    "        if v>1:\n",
    "            log.warning(\"Duplicate facet_id: %s appears %d times\", k, v)\n",
    "    assert facet_entries, \"No facet variable blocks found under config/prompt_variable_blocks/\"\n",
    "    facets_meta = {e[\"facet_id\"]: dict(domain_id=e[\"domain_id\"], subdomain_id=e[\"subdomain_id\"],\n",
    "                                       domain=e[\"domain\"], subdomain=e[\"subdomain\"], facet=e[\"facet\"])\n",
    "                   for e in facet_entries}\n",
    "\n",
    "    selected_map: Dict[str, Dict[str, List[str]]] = defaultdict(dict)\n",
    "    metrics_rows: List[dict] = []\n",
    "\n",
    "    for method in METHODS:\n",
    "        def finals_up_to_size(method: str, N: int) -> bool:\n",
    "            p = G_OUT_DIR / f\"g_final_n{N}_{method}.csv\"\n",
    "            return _csv_n_rows(p) >= N\n",
    "\n",
    "        # Only skip if every target size actually has ≥ N rows\n",
    "        all_satisfied = all(finals_up_to_size(method, N) for N in SIZES)\n",
    "        if all_satisfied:\n",
    "            log.info(\"↻ All finals for '%s' already meet target sizes; skipping generation.\", method)\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # 1) pools per facet (parallel)\n",
    "        pools_by_facet: Dict[str, List[str]] = {}\n",
    "        counters_by_facet: Dict[str, Dict[str,int]] = {}\n",
    "\n",
    "        def _make(e):\n",
    "            return e[\"facet_id\"], *make_pool_for_facet(e, method)\n",
    "\n",
    "        MAX_WORKERS = 4  # tune: 2–4 for Ollama; higher if generation is I/O bound\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = {ex.submit(_make, e): e for e in facet_entries}\n",
    "            for fut in as_completed(futs):\n",
    "                fid, pool_pre, cnt = fut.result()\n",
    "                pools_by_facet[fid] = pool_pre\n",
    "                counters_by_facet[fid] = cnt\n",
    "\n",
    "        # 2) **global dedupe vóór selectie** met PROTECT_FLOOR\n",
    "        pools_post = global_dedupe_pools(pools_by_facet, facets_meta, protect_floor=PROTECT_FLOOR)\n",
    "\n",
    "        # 3) selection per facet (K-center + D²)  — K=27 by default\n",
    "\n",
    "        selected_map[method] = {}\n",
    "\n",
    "        def _select(e):\n",
    "            fid = e[\"facet_id\"]\n",
    "            pool_post = pools_post.get(fid, [])\n",
    "            chosen, div = select_k_for_facet(\n",
    "                pool_texts=pool_post,\n",
    "                k=K_PER_FACET,\n",
    "                method=method,\n",
    "                facet_id=fid\n",
    "            )\n",
    "            return fid, chosen, div\n",
    "\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = {ex.submit(_select, e): e for e in facet_entries}\n",
    "            for fut in as_completed(futs):\n",
    "                fid, chosen, div = fut.result()\n",
    "                selected_map[method][fid] = chosen\n",
    "                e = next(x for x in facet_entries if x[\"facet_id\"] == fid)\n",
    "                cnt = counters_by_facet.get(fid, {})\n",
    "                metrics_rows.append({\n",
    "                    \"method\": method,\n",
    "                    \"facet_id\": fid,\n",
    "                    \"domain_id\": e[\"domain_id\"],\n",
    "                    \"subdomain_id\": e[\"subdomain_id\"],\n",
    "                    \"pool_count_pre\": len(pools_by_facet.get(fid, [])),\n",
    "                    \"pool_count_post\": len(pools_post.get(fid, [])),\n",
    "                    \"selected_count\": len(chosen),\n",
    "                    \"fail_lang\": int(cnt.get(\"fail_lang\",0)),\n",
    "                    \"fail_len\": int(cnt.get(\"fail_len\",0)),\n",
    "                    \"warn_len\": int(cnt.get(\"warn_len\",0)),\n",
    "                    \"mean_pairwise_dist\": float(div.get(\"mean_pairwise_dist\", np.nan)),\n",
    "                    \"min_pairwise_dist\":  float(div.get(\"min_pairwise_dist\", np.nan)),\n",
    "                })\n",
    "\n",
    "    write_intrinsic_metrics(metrics_rows)\n",
    "\n",
    "    # 4) finals + COMBO (writer will skip sizes that already exist; COMBO skipped when single-method)\n",
    "    finals = build_global_finals(selected_map, facets_meta)\n",
    "    write_finals_and_combo(finals)\n",
    "\n",
    "    # run_summary\n",
    "    sum_rows = []\n",
    "    for method in METHODS:\n",
    "        for N in SIZES:\n",
    "            p = G_OUT_DIR / f\"g_final_n{N}_{method}.csv\"\n",
    "            if p.exists() and p.stat().st_size:\n",
    "                n_rows = sum(1 for _ in open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1\n",
    "                sum_rows.append({\"method\": method, \"size\": N, \"n_sent\": max(0, n_rows)})\n",
    "    if sum_rows:\n",
    "        pd.DataFrame(sum_rows).to_csv(RUN_SUMMARY, index=False)\n",
    "    log.info(\"=== step_e_1 completed ===\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  15. Entry-point\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    if all_methods_complete():\n",
    "        print(\"↻ All synthetic datasets already present for all methods and sizes — no work to do.\")\n",
    "        print(\"Run completed (idempotent no-op).\")\n",
    "        return  # avoid SystemExit traceback in notebooks/IPython\n",
    "    run()\n",
    "    print(\"Run completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f2155-305a-420e-85fd-54fdad7a90fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr8cht_embedding_comparison",
   "language": "python",
   "name": "kr8cht_embedding_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
