{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc04f7bc-4663-4856-8570-e0f8d0d3c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kr8cht_review_anonymous/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/kr8cht_review_anonymous/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "2025-10-17 18:29:45 INFO: run_config.json written.\n",
      "[review] Summaries & plots written with explicit embedding/teacher naming.\n",
      "Run completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_3_student_scoring.ipynb\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "LOOCV student scoring with per-fold hyperparameters reused from teacher models\n",
    "(no tuning) and percent-based augmentation sizes. The student embedding is\n",
    "selectable; labels for synthetic examples are read from a configurable teacher\n",
    "label source.\n",
    "\n",
    "Pipeline overview\n",
    "-----------------\n",
    "1) Load seed data and resources\n",
    "   • Read 96 seeds (texts + 14 targets).\n",
    "   • Load or compute seed sentence embeddings for the selected student embedding.\n",
    "   • Discover augmentation methods and their M_max from labeled outputs.\n",
    "   • Preload synthetic text embeddings for the selected student embedding and\n",
    "     per-fold tuned labels from the selected label source.\n",
    "\n",
    "2) Build student regressors from teacher hyperparameters (no tuning)\n",
    "   • Students: chain_ERCcv_lr, local_lasso, local_rf, global_rf, chain_ERCcv_rf.\n",
    "   • Extract per-fold HPs from teacher pickles (or JSON) for the same embedding.\n",
    "\n",
    "3) Evaluate the baseline (seeds-only)\n",
    "   • For each LOOCV fold and student, reconstruct the pipeline from the teacher’s\n",
    "     stored HPs and predict the held-out seed.\n",
    "   • Write per-pct baseline CSVs for naming symmetry.\n",
    "\n",
    "4) Evaluate the augmented “full” variant (seeds + synthetics)\n",
    "   • Percent mode: P ∈ {10, 20, 50, 100, 200, 400}; K = round(P% of 96).\n",
    "   • For each fold, take the first K items from that fold’s M_max labeled set\n",
    "     (labels from the label source), fit the student with the per-fold HPs, and\n",
    "     predict the held-out seed.\n",
    "   • Idempotent: compute only missing folds and preserve already complete CSVs.\n",
    "   • Legacy reuse for chain_ERCcv_lr at {100,200,400} applies only when the\n",
    "     student embedding equals the label-source embedding (e.g., e5_base).\n",
    "\n",
    "5) Produce unified summary tables\n",
    "   • Aggregate per-fold median RRMSE into wide summaries.\n",
    "   • Write comparison tables with ΔRRMSE and relative % change.\n",
    "   • Perform pooled/paired significance tests and bootstrap Δ median CIs.\n",
    "\n",
    "6) Visualize performance\n",
    "   • RRMSE vs %K and ΔRRMSE vs %K per embedding and per teacher.\n",
    "   • Overlay “compare” plots across teachers.\n",
    "\n",
    "Idempotency & disk reuse\n",
    "------------------------\n",
    "• Reuse seed embeddings if shape and max_seq_len match; otherwise rebuild.\n",
    "• Reuse synthetic embeddings (cache); build them if missing.\n",
    "• Skip FULL computations for (student, method, pct) whose per-fold CSV is complete.\n",
    "• Optional: summarization-only review mode to build tables/plots from existing CSVs.\n",
    "\n",
    "Inputs\n",
    "------\n",
    "• data/activity_scores.csv\n",
    "• data/activities.csv\n",
    "• outputs/b_frozen/results/{student_embedding}_vectors.npy\n",
    "• outputs/e_1_synth_augmentation/g_final_n{M}_{method}.csv\n",
    "• outputs/e_2_teacher_labeling/g2f_labels_fold{ii}_n{M}_{method}__{label_embedding}__{label_model}.csv\n",
    "• models/teacher/teacher_fold{ii}_{student_embedding}__{student}.pkl\n",
    "• outputs/e_2_teacher_labeling/cache/synth_embeds/*__{student_embedding}.npy and *__index.csv\n",
    "• (legacy reuse, only if student_embedding==label_embedding==e5_base and student==chain_ERCcv_lr)\n",
    "  outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n96_sps1__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n192_sps2__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n384_sps4__full.csv\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• Compute artifacts (unchanged naming):\n",
    "  outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}[__labels_*]__baseline.csv\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}[__labels_*]__full.csv\n",
    "\n",
    "• HP snapshots:\n",
    "  outputs/e_3_student_scoring/hp_perfold/{regressor}/{method}/n{Mmax}/pct{P}_K{K}/fold{ii}_best.json\n",
    "\n",
    "• Review summaries (explicit naming):\n",
    "  outputs/e_3_student_scoring/tables/\n",
    "    summary_median_rrmse__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    summary_median_rrmse_with_delta__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    bootstrap_delta_ci__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    wilcoxon_vs_baseline__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    wilcoxon_holm_vs_baseline__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    cliffs_delta_paired__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    wilcoxon_pooled_vs_baseline__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    cliffs_delta_pooled_vs_baseline__embedding_{emb}__teachers_{teacherA__teacherB__...}.csv\n",
    "    …and per-teacher splits:\n",
    "    *___embedding_{emb}__teacher_{teacherA}.csv\n",
    "\n",
    "• Review plots (explicit naming):\n",
    "  outputs/e_3_student_scoring/plots/\n",
    "    combined_rrmse_vs_pct__{method}__embedding_{emb}__teacher_{teacherX}.png\n",
    "    combined_delta_vs_pct__{method}__embedding_{emb}__teacher_{teacherX}.png\n",
    "    combined_rrmse_vs_pct__{method}__embedding_{emb}__teachers_{teacherA__teacherB__...}__compare.png\n",
    "\n",
    "Notes on teacher naming in summaries/plots\n",
    "------------------------------------------\n",
    "• Canonical labels in CSV cells and legends:\n",
    "  - default → teacher_e5_base_chainERCcv_lr\n",
    "  - labels_local_lasso → teacher_e5_base_local_lasso\n",
    "• For any tag starting with \"labels_\", the suffix becomes \"teacher_e5_base_<suffix>\".\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Paths\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def project_root(marker: str = \"LICENSE\") -> Path:\n",
    "    \"\"\"Return the repository root by walking up from CWD until a marker file is found.\n",
    "\n",
    "    Args:\n",
    "        marker: File name whose presence identifies the project root.\n",
    "\n",
    "    Returns:\n",
    "        Absolute Path to the project root directory.\n",
    "    \"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    for d in (here, *here.parents):\n",
    "        if (d / marker).is_file():\n",
    "            return d\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "\n",
    "ROOT = project_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "G1_DIR = ROOT / \"outputs\" / \"e_1_synth_augmentation\"\n",
    "G2_DIR = ROOT / \"outputs\" / \"e_2_teacher_labeling\"\n",
    "SEED_VECTORS_DIR = ROOT / \"outputs\" / \"b_frozen\" / \"results\"\n",
    "\n",
    "G3A_DIR = ROOT / \"outputs\" / \"e_3_student_scoring\"\n",
    "RES_DIR = G3A_DIR / \"results\"\n",
    "TABLES_DIR = G3A_DIR / \"tables\"\n",
    "CACHE_DIR = G3A_DIR / \"cache\"\n",
    "HP_PERFOLD = G3A_DIR / \"hp_perfold\"\n",
    "FIGS_DIR = G3A_DIR / \"plots\"\n",
    "\n",
    "for p in (G3A_DIR, RES_DIR, TABLES_DIR, CACHE_DIR, HP_PERFOLD, FIGS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TABLES = TABLES_DIR\n",
    "PLOTS_DIR = FIGS_DIR\n",
    "\n",
    "LOG_FILE = G3A_DIR / \"run.log\"\n",
    "for h in list(logging.root.handlers):\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Logging\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(getattr(sys, \"__stdout__\", sys.stdout)),\n",
    "    ],\n",
    "    force=True,\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "_say_logger = logging.getLogger(\"sayfile\")\n",
    "if not _say_logger.handlers:\n",
    "    _say_logger.setLevel(logging.INFO)\n",
    "    _say_logger.propagate = False\n",
    "    _fh = logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\")\n",
    "    _fh.setFormatter(\n",
    "        logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    _say_logger.addHandler(_fh)\n",
    "\n",
    "RUN_ID: str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Config\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "REVIEW_MODE: bool = True\n",
    "\"\"\"If True, skip compute and only build summaries/plots from existing results.\"\"\"\n",
    "\n",
    "N_TARGETS = 14\n",
    "\n",
    "METHODS: Optional[List[str]] = None\n",
    "\"\"\"Optional whitelist of augmentation methods; None means use all discovered.\"\"\"\n",
    "\n",
    "PCT_LIST: List[int] = [10, 20, 50, 100, 200, 400]\n",
    "PCTS_FOR_PLOTS: List[int] = [10, 20, 50, 100, 200, 400]\n",
    "PRIMARY_PCTS = PCT_LIST\n",
    "\n",
    "SAVE_PRED_PERFOLD: bool = True\n",
    "LOG_EVERY: int = 16\n",
    "MAX_FOLDS: int = 0  # 0 ⇒ all 96\n",
    "\n",
    "# Which student regressors to run\n",
    "STUDENTS: List[str] = [\"chain_ERCcv_lr\", \"local_lasso\", \"local_rf\", \"global_rf\", \"chain_ERCcv_rf\"]\n",
    "\n",
    "TARGET_COLS = [f\"domain{i}\" for i in range(1, 15)]\n",
    "_DOM_PREFIX = \"rrmse_domain\"\n",
    "BOOT_B: int = 5000\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Device, cores, seeding\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "DETERMINISTIC = True\n",
    "N_JOBS: int = min(os.cpu_count() or 6, 6)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", str(N_JOBS))\n",
    "try:\n",
    "    torch.set_num_threads(N_JOBS)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "DEVICE_STR = device.type\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import random\n",
    "\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        torch.manual_seed(SEED)\n",
    "    if DETERMINISTIC:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Embedding registry and selection\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "EMBEDDING_SPECS: Dict[str, str] = {\n",
    "    \"e5_base\": \"embaas/sentence-transformers-multilingual-e5-base\",\n",
    "}\n",
    "\n",
    "STUDENT_EMBEDDINGS: List[str] = [\"e5_base\"]\n",
    "\n",
    "\n",
    "def _normalize_embeddings(x):\n",
    "    \"\"\"Normalize an embedding selection into a deduplicated list of strings.\n",
    "\n",
    "    Args:\n",
    "        x: String with comma-separated names or an iterable of names.\n",
    "\n",
    "    Returns:\n",
    "        List of unique, stripped embedding keys.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return [s.strip() for s in x.split(\",\") if s.strip()]\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [str(s).strip() for s in x if str(s).strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "_seen = set()\n",
    "STUDENT_EMBEDDINGS = [e for e in STUDENT_EMBEDDINGS if not (e in _seen or _seen.add(e))]\n",
    "STUDENT_EMB = STUDENT_EMBEDDINGS[0]\n",
    "\n",
    "# Label source (teacher that produced the labels for synthetics)\n",
    "LABEL_EMB: str = \"e5_base\"\n",
    "LABEL_MODEL: str = \"chain_ERCcv_lr\"  # default label-source teacher for compute\n",
    "if LABEL_EMB not in EMBEDDING_SPECS:\n",
    "    raise ValueError(f\"Unknown LABEL_SOURCE_EMBEDDING='{LABEL_EMB}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "\n",
    "MAX_SEQ_LEN: int = 512\n",
    "\n",
    "# Optional extra pass: also train specific students on alternative label sources\n",
    "EXTRA_STUDENT_LABEL_JOBS = [\n",
    "    {\n",
    "        \"student\": \"chain_ERCcv_lr\",\n",
    "        \"label_emb\": \"e5_base\",\n",
    "        \"label_model\": \"local_lasso\",\n",
    "        \"labels_tag\": \"labels_local_lasso\",\n",
    "    },\n",
    "    {\n",
    "        \"student\": \"local_lasso\",\n",
    "        \"label_emb\": \"e5_base\",\n",
    "        \"label_model\": \"local_lasso\",\n",
    "        \"labels_tag\": \"labels_local_lasso\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Per-embedding tables/plots flags\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def _auto_flag(val: str, default_auto: bool) -> bool:\n",
    "    \"\"\"Parse a boolean flag value with support for 'auto' fallback.\n",
    "\n",
    "    Args:\n",
    "        val: Text value from env/config ('true'/'false'/'auto').\n",
    "        default_auto: Value to return if val is not an explicit boolean.\n",
    "\n",
    "    Returns:\n",
    "        Boolean flag value.\n",
    "    \"\"\"\n",
    "    v = (val or \"\").strip().lower()\n",
    "    if v in {\"1\", \"true\", \"yes\", \"y\"}:\n",
    "        return True\n",
    "    if v in {\"0\", \"false\", \"no\", \"n\"}:\n",
    "        return False\n",
    "    return default_auto\n",
    "\n",
    "\n",
    "MULTI_EMBED = len(STUDENT_EMBEDDINGS) > 1\n",
    "WRITE_PER_EMBED_TABLES: bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "WRITE_PER_EMBED_PLOTS: bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "\n",
    "if not WRITE_PER_EMBED_TABLES:\n",
    "    log.info(\"[guard] Per-embedding tables disabled (combined report will handle tables).\")\n",
    "if not WRITE_PER_EMBED_PLOTS:\n",
    "    log.info(\"[guard] Per-embedding plots disabled (combined report will draw plots).\")\n",
    "\n",
    "\n",
    "def write_run_config():\n",
    "    \"\"\"Write a minimal JSON snapshot of the resolved runtime configuration.\"\"\"\n",
    "    try:\n",
    "        cfg = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"device\": DEVICE_STR,\n",
    "            \"seed\": SEED,\n",
    "            \"n_jobs\": N_JOBS,\n",
    "            \"student_embeddings\": STUDENT_EMBEDDINGS,\n",
    "            \"current_student_embedding\": STUDENT_EMB,\n",
    "            \"students\": STUDENTS,\n",
    "            \"methods_filter_env\": os.getenv(\"METHODS\", \"\").strip() or None,\n",
    "            \"pct_list\": PCT_LIST,\n",
    "            \"pcts_for_plots\": PCTS_FOR_PLOTS,\n",
    "            \"label_source_embedding\": LABEL_EMB,\n",
    "            \"label_source_model\": LABEL_MODEL,\n",
    "            \"max_folds\": MAX_FOLDS,\n",
    "            \"boot_B\": BOOT_B,\n",
    "            \"write_per_embed_tables\": WRITE_PER_EMBED_TABLES,\n",
    "            \"write_per_embed_plots\": WRITE_PER_EMBED_PLOTS,\n",
    "            \"skip_baseline_if_present\": os.getenv(\"SKIP_BASELINE_IF_PRESENT\", \"0\"),\n",
    "            \"do_refits\": os.getenv(\"DO_REFITS\", \"1\"),\n",
    "            \"paths\": {\n",
    "                \"root\": str(ROOT),\n",
    "                \"g1_dir\": str(G1_DIR),\n",
    "                \"g2_dir\": str(G2_DIR),\n",
    "                \"g3a_dir\": str(G3A_DIR),\n",
    "                \"results_dir\": str(RES_DIR),\n",
    "                \"tables_dir\": str(TABLES_DIR),\n",
    "                \"figs_dir\": str(FIGS_DIR),\n",
    "            },\n",
    "        }\n",
    "        (G3A_DIR / \"run_config.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "        log.info(\"run_config.json written.\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Could not write run_config.json: {e}\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Sentence encoders\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "_ENCODERS: Dict[str, SentenceTransformer] = {}\n",
    "\n",
    "\n",
    "def get_encoder(emb_key: str) -> SentenceTransformer:\n",
    "    \"\"\"Load (or reuse) a SentenceTransformer for a given embedding key.\n",
    "\n",
    "    Args:\n",
    "        emb_key: Registered embedding key.\n",
    "\n",
    "    Returns:\n",
    "        Loaded SentenceTransformer instance.\n",
    "    \"\"\"\n",
    "    if emb_key in _ENCODERS:\n",
    "        return _ENCODERS[emb_key]\n",
    "    repo = EMBEDDING_SPECS[emb_key]\n",
    "    log.info(f\"Loading SentenceTransformer [{emb_key}]: {repo} → device={DEVICE_STR}\")\n",
    "    m = SentenceTransformer(repo, device=DEVICE_STR)\n",
    "    m.max_seq_length = MAX_SEQ_LEN\n",
    "    _ENCODERS[emb_key] = m\n",
    "    return m\n",
    "\n",
    "\n",
    "def encode_texts(emb_key: str, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Encode a list of texts into embeddings using the specified encoder.\n",
    "\n",
    "    Args:\n",
    "        emb_key: Registered embedding key.\n",
    "        texts: List of input strings.\n",
    "        batch_size: Batch size for encoding.\n",
    "\n",
    "    Returns:\n",
    "        2D numpy array of shape (n_texts, dim).\n",
    "    \"\"\"\n",
    "    mdl = get_encoder(emb_key)\n",
    "    X = mdl.encode(\n",
    "        texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False\n",
    "    )\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Student models\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class RegressorChainCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Regressor chain with cross-validated out-of-fold features for each link.\n",
    "\n",
    "    For a given base estimator and target order, this model:\n",
    "      1) Produces OOF predictions for each target via K-fold CV and concatenates\n",
    "         them to the input for the next target.\n",
    "      2) Fits a final model per target using the accumulated features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_estimator, order=None, cv_splits=5, random_state=SEED):\n",
    "        \"\"\"Initialize the chain.\n",
    "\n",
    "        Args:\n",
    "            base_estimator: Base regressor used at each link.\n",
    "            order: Optional array of target indices; if None, natural order is used.\n",
    "            cv_splits: Number of folds for OOF generation.\n",
    "            random_state: Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.order = order\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.chain_models_ = []\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit the chain with OOF features derived by K-fold cross-validation.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            Y: Target matrix of shape (n_samples, n_targets).\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples, self.n_targets_ = Y.shape\n",
    "        if self.order is None:\n",
    "            self.order = np.arange(self.n_targets_)\n",
    "        kf = KFold(n_splits=self.cv_splits, shuffle=True, random_state=rng)\n",
    "        X_chain = np.ascontiguousarray(X, dtype=np.float32)\n",
    "\n",
    "        oof_cols = []\n",
    "        for target_idx in self.order:\n",
    "            y = Y[:, target_idx]\n",
    "            oof = np.zeros(n_samples, dtype=np.float32)\n",
    "            for tr, va in kf.split(X_chain):\n",
    "                est = clone(self.base_estimator)\n",
    "                est.fit(X_chain[tr], y[tr])\n",
    "                oof[va] = est.predict(X_chain[va])\n",
    "            oof_cols.append(oof.reshape(-1, 1))\n",
    "            X_chain = np.hstack([X_chain, oof.reshape(-1, 1)])\n",
    "\n",
    "        self.chain_models_ = []\n",
    "        acc = []\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            if i == 0:\n",
    "                X_full = X\n",
    "            else:\n",
    "                acc.append(oof_cols[i - 1])\n",
    "                X_full = np.hstack([X, np.hstack(acc)])\n",
    "            est = clone(self.base_estimator)\n",
    "            est.fit(X_full, Y[:, target_idx])\n",
    "            self.chain_models_.append(est)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict targets sequentially, appending previous predictions as features.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            2D numpy array of predictions with shape (n_samples, n_targets).\n",
    "        \"\"\"\n",
    "        X_ext = np.ascontiguousarray(X, dtype=np.float32)\n",
    "        n = X.shape[0]\n",
    "        preds = np.zeros((n, self.n_targets_), dtype=np.float32)\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            yhat = self.chain_models_[i].predict(X_ext).reshape(-1, 1)\n",
    "            preds[:, target_idx] = yhat[:, 0]\n",
    "            X_ext = np.hstack([X_ext, yhat])\n",
    "        return preds\n",
    "\n",
    "\n",
    "class EnsembleRegressorChainsCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Ensemble of regressor chains over random target orders; averages predictions.\"\"\"\n",
    "\n",
    "    def __init__(self, base_estimator, n_chains=5, cv_splits=5, random_state=SEED):\n",
    "        \"\"\"Initialize the ensemble.\n",
    "\n",
    "        Args:\n",
    "            base_estimator: Base regressor used at each link.\n",
    "            n_chains: Number of random target orders to ensemble.\n",
    "            cv_splits: Number of folds for OOF generation within each chain.\n",
    "            random_state: Random state for reproducibility.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_chains = n_chains\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.ensemble_ = None\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit multiple RegressorChainCV models with random target orders.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            Y: Target matrix of shape (n_samples, n_targets).\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self.n_targets_ = Y.shape[1]\n",
    "        self.ensemble_ = []\n",
    "        for _ in range(self.n_chains):\n",
    "            order = np.arange(self.n_targets_)\n",
    "            rng.shuffle(order)\n",
    "            chain = RegressorChainCV(\n",
    "                base_estimator=self.base_estimator,\n",
    "                order=order,\n",
    "                cv_splits=self.cv_splits,\n",
    "                random_state=rng.randint(0, 1_000_000),\n",
    "            )\n",
    "            chain.fit(X, Y)\n",
    "            self.ensemble_.append((order, chain))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict by averaging the outputs of all chains.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            2D numpy array of predictions with shape (n_samples, n_targets).\n",
    "        \"\"\"\n",
    "        preds = [chain.predict(X) for (_, chain) in self.ensemble_]\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "def build_with_hp(student_key: str, hp: Dict[str, float | int]) -> Pipeline:\n",
    "    \"\"\"Construct a sklearn Pipeline for a student model from stored hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        student_key: Student regressor identifier.\n",
    "        hp: Mapping of hyperparameters extracted from teacher artifacts.\n",
    "\n",
    "    Returns:\n",
    "        Configured sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        return Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "                (\n",
    "                    \"chain\",\n",
    "                    EnsembleRegressorChainsCV(\n",
    "                        base_estimator=LinearRegression(),\n",
    "                        n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                        cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                        random_state=SEED,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    elif student_key == \"local_lasso\":\n",
    "        return Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "                (\n",
    "                    \"reg\",\n",
    "                    MultiOutputRegressor(\n",
    "                        Lasso(alpha=float(hp[\"reg__estimator__alpha\"]), random_state=SEED, max_iter=10_000)\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    elif student_key == \"local_rf\":\n",
    "        return Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "                (\n",
    "                    \"reg\",\n",
    "                    MultiOutputRegressor(\n",
    "                        RandomForestRegressor(\n",
    "                            n_estimators=int(hp[\"reg__estimator__n_estimators\"]),\n",
    "                            max_depth=None\n",
    "                            if (hp.get(\"reg__estimator__max_depth\", None) in [None, \"None\"])\n",
    "                            else int(hp[\"reg__estimator__max_depth\"]),\n",
    "                            min_samples_leaf=int(hp.get(\"reg__estimator__min_samples_leaf\", 1)),\n",
    "                            random_state=SEED,\n",
    "                            n_jobs=1,\n",
    "                        )\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    elif student_key == \"global_rf\":\n",
    "        return Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "                (\n",
    "                    \"reg\",\n",
    "                    RandomForestRegressor(\n",
    "                        n_estimators=int(hp[\"reg__n_estimators\"]),\n",
    "                        max_depth=None\n",
    "                        if (hp.get(\"reg__max_depth\", None) in [None, \"None\"])\n",
    "                        else int(hp[\"reg__max_depth\"]),\n",
    "                        min_samples_leaf=int(hp.get(\"reg__min_samples_leaf\", 1)),\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=1,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        base_rf = RandomForestRegressor(\n",
    "            n_estimators=int(hp.get(\"chain__base_rf__n_estimators\", 100)),\n",
    "            max_depth=None\n",
    "            if (hp.get(\"chain__base_rf__max_depth\", None) in [None, \"None\"])\n",
    "            else int(hp[\"chain__base_rf__max_depth\"]),\n",
    "            min_samples_leaf=int(hp.get(\"chain__base_rf__min_samples_leaf\", 1)),\n",
    "            random_state=SEED,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        return Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "                (\n",
    "                    \"chain\",\n",
    "                    EnsembleRegressorChainsCV(\n",
    "                        base_estimator=base_rf,\n",
    "                        n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                        cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                        random_state=SEED,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown student_key: {student_key}\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Teacher HP loading\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def teacher_pickle_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    \"\"\"Return the path of the fitted teacher pipeline pickle for a fold/student/embedding.\n",
    "\n",
    "    Args:\n",
    "        fold_idx: Fold index in [0, 95].\n",
    "        student_key: Student/teacher identifier.\n",
    "        emb_key: Embedding key.\n",
    "\n",
    "    Returns:\n",
    "        Path to the pickle file.\n",
    "    \"\"\"\n",
    "    return ROOT / \"models\" / \"teacher\" / f\"teacher_fold{fold_idx:02d}_{emb_key}__{student_key}.pkl\"\n",
    "\n",
    "\n",
    "def _hp_json_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    \"\"\"Return the fallback JSON path containing per-fold HPs for a teacher.\n",
    "\n",
    "    Args:\n",
    "        fold_idx: Fold index.\n",
    "        student_key: Student/teacher identifier.\n",
    "        emb_key: Embedding key.\n",
    "\n",
    "    Returns:\n",
    "        Path to the JSON file.\n",
    "    \"\"\"\n",
    "    return G2_DIR / \"teacher\" / f\"hp_fold{fold_idx:02d}_{emb_key}__{student_key}.json\"\n",
    "\n",
    "\n",
    "def hp_from_teacher(est: Pipeline, student_key: str) -> Dict[str, float | int]:\n",
    "    \"\"\"Extract hyperparameters from a fitted teacher pipeline.\n",
    "\n",
    "    Args:\n",
    "        est: Fitted sklearn Pipeline for the teacher.\n",
    "        student_key: Student/teacher identifier.\n",
    "\n",
    "    Returns:\n",
    "        Flat dict of hyperparameters compatible with `build_with_hp`.\n",
    "    \"\"\"\n",
    "    hp: Dict[str, float | int] = {}\n",
    "    if \"pca\" in est.named_steps:\n",
    "        hp[\"pca__n_components\"] = float(getattr(est.named_steps[\"pca\"], \"n_components\", 0.8))\n",
    "\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "    elif student_key == \"local_lasso\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__alpha\"] = float(getattr(reg, \"alpha\", 0.01))\n",
    "    elif student_key == \"local_rf\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__estimator__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__estimator__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "    elif student_key == \"global_rf\":\n",
    "        reg = est.named_steps[\"reg\"]\n",
    "        hp[\"reg__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "        base_rf = getattr(ch, \"base_estimator\", None)\n",
    "        if base_rf is None and hasattr(ch, \"base_estimator_\"):\n",
    "            base_rf = ch.base_estimator_\n",
    "        if isinstance(base_rf, RandomForestRegressor):\n",
    "            hp[\"chain__base_rf__n_estimators\"] = int(getattr(base_rf, \"n_estimators\", 100))\n",
    "            hp[\"chain__base_rf__max_depth\"] = getattr(base_rf, \"max_depth\", None)\n",
    "            hp[\"chain__base_rf__min_samples_leaf\"] = int(getattr(base_rf, \"min_samples_leaf\", 1))\n",
    "    else:\n",
    "        raise ValueError(student_key)\n",
    "    return hp\n",
    "\n",
    "\n",
    "def load_teacher_fold_estimator(\n",
    "    fold_idx: int, student_key: str, emb_key: str, retries: int = 3, sleep_s: float = 1.5\n",
    ") -> Optional[Pipeline]:\n",
    "    \"\"\"Load a fitted teacher pipeline with limited retries.\n",
    "\n",
    "    Args:\n",
    "        fold_idx: Fold index.\n",
    "        student_key: Student/teacher identifier.\n",
    "        emb_key: Embedding key.\n",
    "        retries: Number of attempts to read the file.\n",
    "        sleep_s: Backoff in seconds between attempts.\n",
    "\n",
    "    Returns:\n",
    "        Loaded Pipeline or None if not found/unreadable.\n",
    "    \"\"\"\n",
    "    p = teacher_pickle_path(fold_idx, student_key, emb_key)\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if not p.exists():\n",
    "            break\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            if \"timed out\" in str(e).lower() or isinstance(e, OSError):\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}) [attempt {attempt}/{retries}]: {e}\")\n",
    "                if attempt < retries:\n",
    "                    time.sleep(sleep_s * attempt)\n",
    "                    continue\n",
    "            else:\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}): {e}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "def hp_from_teacher_or_json(fold_idx: int, student_key: str, emb_key: str, est: Optional[Pipeline]) -> Dict[str, float | int]:\n",
    "    \"\"\"Return per-fold hyperparameters from a teacher pipeline or fallback JSON.\n",
    "\n",
    "    Args:\n",
    "        fold_idx: Fold index.\n",
    "        student_key: Student/teacher identifier.\n",
    "        emb_key: Embedding key.\n",
    "        est: Optional fitted teacher pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Hyperparameter dict for `build_with_hp`.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If neither the pickle nor the JSON source is available.\n",
    "    \"\"\"\n",
    "    if est is not None:\n",
    "        return hp_from_teacher(est, student_key)\n",
    "    j = _hp_json_path(fold_idx, student_key, emb_key)\n",
    "    if j.exists():\n",
    "        try:\n",
    "            payload = json.loads(j.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(payload, dict) and \"best_params\" in payload and isinstance(payload[\"best_params\"], dict):\n",
    "                return payload[\"best_params\"]\n",
    "            if isinstance(payload, dict):\n",
    "                return payload\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"HP JSON unreadable: {j.name} | {e}\") from e\n",
    "    raise RuntimeError(\n",
    "        f\"Missing per-fold HPs for fold={fold_idx}, student={student_key}, emb={emb_key}. \"\n",
    "        f\"Expected either teacher pickle ({teacher_pickle_path(fold_idx, student_key, emb_key).name}) \"\n",
    "        f\"or HP JSON ({_hp_json_path(fold_idx, student_key, emb_key).name}).\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Data loading (seeds + synthetics)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def load_seeds() -> pd.DataFrame:\n",
    "    \"\"\"Load the 96 seed items with text and 14 target scores.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: seed_id, text, domain1..domain14.\n",
    "    \"\"\"\n",
    "    scores = pd.read_csv(DATA_DIR / \"activity_scores.csv\")\n",
    "    acts = pd.read_csv(DATA_DIR / \"activities.csv\")\n",
    "    dm = scores.pivot(index=\"activity_id\", columns=\"domain_id\", values=\"score\").reset_index()\n",
    "    dm = dm.rename(columns=lambda x: f\"domain{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "    dm = dm.merge(acts[[\"activity_id\", \"question\"]], on=\"activity_id\", how=\"left\")\n",
    "    dm = dm.rename(columns={\"activity_id\": \"seed_id\", \"question\": \"text\"})\n",
    "    dm = dm.sort_values(\"seed_id\").reset_index(drop=True)\n",
    "    assert len(dm) == 96, f\"Expected 96 seeds, got {len(dm)}\"\n",
    "    return dm[[\"seed_id\", \"text\", *TARGET_COLS]]\n",
    "\n",
    "\n",
    "def ensure_seed_vectors(seeds_df: pd.DataFrame, emb_key: str) -> np.ndarray:\n",
    "    \"\"\"Ensure seed sentence embeddings exist for the given embedding; compute if missing.\n",
    "\n",
    "    Args:\n",
    "        seeds_df: DataFrame from `load_seeds()`.\n",
    "        emb_key: Embedding key.\n",
    "\n",
    "    Returns:\n",
    "        2D numpy array of shape (96, dim) with seed embeddings.\n",
    "    \"\"\"\n",
    "    vec_path = SEED_VECTORS_DIR / f\"{emb_key}_vectors.npy\"\n",
    "    meta_path = vec_path.with_suffix(\".meta.json\")\n",
    "\n",
    "    if vec_path.exists():\n",
    "        try:\n",
    "            X = np.load(vec_path)\n",
    "            ok_rows = X.shape[0] == len(seeds_df)\n",
    "            ok_len = True\n",
    "            if meta_path.exists():\n",
    "                meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "                ok_len = int(meta.get(\"max_seq_len\", MAX_SEQ_LEN)) == int(MAX_SEQ_LEN)\n",
    "            if ok_rows and ok_len:\n",
    "                np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X)\n",
    "                log.info(\"✔ Reusing seed vectors → %s\", vec_path.relative_to(ROOT))\n",
    "                return X.astype(np.float32, copy=False)\n",
    "            else:\n",
    "                log.warning(\n",
    "                    \"[%s] Rebuilding seed vectors (rows_ok=%s, max_seq_len_ok=%s).\", emb_key, ok_rows, ok_len\n",
    "                )\n",
    "        except Exception as e:\n",
    "            log.warning(\"[%s] Existing vectors unusable (%s); rebuilding.\", emb_key, e)\n",
    "\n",
    "    SEED_VECTORS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(\"Computing %s embeddings for seeds …\", emb_key)\n",
    "    X = encode_texts(emb_key, seeds_df[\"text\"].astype(str).tolist(), batch_size=64)\n",
    "    np.save(vec_path, X.astype(np.float32, copy=False))\n",
    "    np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X.astype(np.float32, copy=False))\n",
    "\n",
    "    try:\n",
    "        meta = {\n",
    "            \"embedding\": emb_key,\n",
    "            \"repo\": EMBEDDING_SPECS[emb_key],\n",
    "            \"max_seq_len\": int(MAX_SEQ_LEN),\n",
    "            \"n_rows\": int(X.shape[0]),\n",
    "            \"n_dim\": int(X.shape[1]),\n",
    "        }\n",
    "        meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    log.info(\"✔ Seed vectors saved → %s\", vec_path.relative_to(ROOT))\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def base_from_method_and_M(method: str, M: int) -> str:\n",
    "    \"\"\"Return a short base tag 'n{M}_{method}' for filenames.\n",
    "\n",
    "    Args:\n",
    "        method: Augmentation method name.\n",
    "        M: Number of synthetic examples available.\n",
    "\n",
    "    Returns:\n",
    "        String tag used in G2/G1 artifacts.\n",
    "    \"\"\"\n",
    "    return f\"n{M}_{method}\"\n",
    "\n",
    "\n",
    "def g2_label_file(\n",
    "    fold_idx: int, method: str, M: int, label_emb: str, label_model: str, strict_model: bool = False\n",
    ") -> Path:\n",
    "    \"\"\"Return the expected path to the labeled synthetics CSV for a given fold/method.\n",
    "\n",
    "    Args:\n",
    "        fold_idx: Fold index.\n",
    "        method: Augmentation method name.\n",
    "        M: Maximum available synthetics (M_max).\n",
    "        label_emb: Embedding used by the label source teacher.\n",
    "        label_model: Teacher model identifier.\n",
    "        strict_model: If True, do not fallback to any other teacher model.\n",
    "\n",
    "    Returns:\n",
    "        Path to the labeled synthetics CSV. If strict_model is False, the function\n",
    "        falls back to the first matching file with the same label embedding.\n",
    "    \"\"\"\n",
    "    base = base_from_method_and_M(method, M)\n",
    "    preferred = G2_DIR / f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__{label_model}.csv\"\n",
    "    if preferred.exists():\n",
    "        return preferred\n",
    "    if strict_model:\n",
    "        return preferred\n",
    "    cand = sorted(G2_DIR.glob(f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__*.csv\"))\n",
    "    if cand:\n",
    "        log.warning(\n",
    "            \"Falling back to %s for fold=%02d, method=%s, M=%d (label_model=%s missing).\",\n",
    "            cand[0].name,\n",
    "            fold_idx,\n",
    "            method,\n",
    "            M,\n",
    "            label_model,\n",
    "        )\n",
    "        return cand[0]\n",
    "    return preferred\n",
    "\n",
    "\n",
    "def discover_methods_and_M_from_g2(label_emb: str) -> Dict[str, List[int]]:\n",
    "    \"\"\"Discover available augmentation methods and their M_max from G2 labels.\n",
    "\n",
    "    Args:\n",
    "        label_emb: Label-source embedding key.\n",
    "\n",
    "    Returns:\n",
    "        Mapping {method: sorted list of available M values}.\n",
    "    \"\"\"\n",
    "    pats = sorted(G2_DIR.glob(f\"g2f_labels_fold00_n*_*__{label_emb}__*.csv\"))\n",
    "    rows: Dict[str, set] = {}\n",
    "    for p in pats:\n",
    "        m = re.match(rf\"g2f_labels_fold00_n(\\d+)_([A-Za-z0-9_]+)__{label_emb}__.*\\.csv$\", p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        M = int(m.group(1))\n",
    "        method = m.group(2)\n",
    "        rows.setdefault(method, set()).add(M)\n",
    "    return {k: sorted(v) for k, v in rows.items()}\n",
    "\n",
    "\n",
    "def g1_source_csv(method: str, M: int) -> Path:\n",
    "    \"\"\"Return the Script-A (augmentation) output CSV path for a given method/M.\n",
    "\n",
    "    Args:\n",
    "        method: Augmentation method name.\n",
    "        M: Number of generated synthetics.\n",
    "\n",
    "    Returns:\n",
    "        Path to the G1 source CSV.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the expected CSV is missing.\n",
    "    \"\"\"\n",
    "    p = G1_DIR / f\"g_final_n{M}_{method}.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing Script-A source: {p}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def ensure_synth_cache(method: str, M: int, emb_key: str) -> Tuple[Path, Path]:\n",
    "    \"\"\"Ensure cached synthetic embeddings exist for a method/M and an embedding.\n",
    "\n",
    "    Args:\n",
    "        method: Augmentation method name.\n",
    "        M: Number of generated synthetics (M_max).\n",
    "        emb_key: Student embedding key.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(Path to npy array, Path to index.csv).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input CSV is malformed.\n",
    "    \"\"\"\n",
    "    src_csv = g1_source_csv(method, M)\n",
    "    base = src_csv.stem\n",
    "    m = re.match(r\"g_final_(n\\d+_[A-Za-z0-9_]+)$\", base)\n",
    "    base = m.group(1) if m else base\n",
    "\n",
    "    cache_dir = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "    npy = cache_dir / f\"g_final_{base}__{emb_key}.npy\"\n",
    "    idx = cache_dir / f\"g_final_{base}__index.csv\"\n",
    "    if npy.exists() and idx.exists():\n",
    "        return npy, idx\n",
    "\n",
    "    df = pd.read_csv(src_csv)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(f\"{src_csv.name}: missing 'text' column\")\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "    log.info(f\"[cache] building synth embeds for method={method}, M={M}, emb={emb_key} …\")\n",
    "    Xs = encode_texts(emb_key, texts, batch_size=64).astype(np.float32, copy=False)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(npy, Xs)\n",
    "    pd.DataFrame({\"text\": texts}).to_csv(idx, index=False)\n",
    "    log.info(\"✔ saved → %s ; %s\", npy.relative_to(ROOT), idx.relative_to(ROOT))\n",
    "    return npy, idx\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Metrics & helpers\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def rmse(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute per-target RMSE between two arrays of equal shape.\n",
    "\n",
    "    Args:\n",
    "        a: Array of shape (n_samples, n_targets).\n",
    "        b: Array of same shape.\n",
    "\n",
    "    Returns:\n",
    "        1D array of RMSE per target.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((a - b) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def rrmse_vs_dummy(y_true: np.ndarray, y_pred: np.ndarray, y_dummy: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute relative RMSE vs a dummy predictor (mean of training targets).\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground-truth of shape (n, t).\n",
    "        y_pred: Predictions of shape (n, t).\n",
    "        y_dummy: Dummy baseline predictions of shape (t,) or (n, t).\n",
    "\n",
    "    Returns:\n",
    "        1D array of RRMSE per target.\n",
    "    \"\"\"\n",
    "    rm = rmse(y_true, y_pred)\n",
    "    rd = rmse(y_true, y_dummy)\n",
    "    return rm / np.maximum(rd, 1e-12)\n",
    "\n",
    "\n",
    "def _fmt_dt(sec: float) -> str:\n",
    "    \"\"\"Format a duration in seconds into h/m/s string.\"\"\"\n",
    "    m, s = divmod(sec, 60.0)\n",
    "    h, m = divmod(m, 60.0)\n",
    "    if h >= 1:\n",
    "        return f\"{int(h)}h{int(m):02d}m{s:04.1f}s\"\n",
    "    if m >= 1:\n",
    "        return f\"{int(m)}m{s:04.1f}s\"\n",
    "    return f\"{s:0.2f}s\"\n",
    "\n",
    "\n",
    "def paired_cliffs_delta(full: np.ndarray, base: np.ndarray):\n",
    "    \"\"\"Compute paired Cliff's delta and counts for two flattened arrays.\n",
    "\n",
    "    Args:\n",
    "        full: Array of scores for the 'full' variant.\n",
    "        base: Array of scores for the 'baseline' variant.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(delta, n_pos, n_neg, n_zero, magnitude).\n",
    "    \"\"\"\n",
    "    diff = base - full  # positive when full improves (lower is better)\n",
    "    n_pos = int(np.sum(diff > 0))\n",
    "    n_neg = int(np.sum(diff < 0))\n",
    "    n_zero = int(np.sum(diff == 0))\n",
    "    denom = max(1, (n_pos + n_neg))\n",
    "    delta = (n_pos - n_neg) / denom\n",
    "    mag = (\n",
    "        \"negligible\"\n",
    "        if abs(delta) < 0.147\n",
    "        else \"small\"\n",
    "        if abs(delta) < 0.33\n",
    "        else \"medium\"\n",
    "        if abs(delta) < 0.474\n",
    "        else \"large\"\n",
    "    )\n",
    "    return float(delta), n_pos, n_neg, n_zero, mag\n",
    "\n",
    "\n",
    "def pct_to_K(pct: int, n_seeds: int = 96) -> int:\n",
    "    \"\"\"Convert a percentage of seeds into a number of synthetics to add (K).\"\"\"\n",
    "    return max(1, int(round((pct / 100.0) * n_seeds)))\n",
    "\n",
    "\n",
    "def _dom_cols(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Return the list of domain metric columns present in a result DataFrame.\"\"\"\n",
    "    return [c for c in df.columns if c.startswith(_DOM_PREFIX)]\n",
    "\n",
    "\n",
    "def _global_median_from_file(p: Path) -> float:\n",
    "    \"\"\"Compute the global median across all folds×targets from a results CSV.\n",
    "\n",
    "    Args:\n",
    "        p: Path to a per-fold RRMSE CSV.\n",
    "\n",
    "    Returns:\n",
    "        Global median of all rrmse_domain* values.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(p)\n",
    "    cols = _dom_cols(df)\n",
    "    if not cols:\n",
    "        raise RuntimeError(f\"{p.name}: no '{_DOM_PREFIX}*' columns present.\")\n",
    "    return float(np.median(df[cols].to_numpy(dtype=np.float32).ravel()))\n",
    "\n",
    "\n",
    "def _flat_arrays(p_base: Path, p_full: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load baseline/full CSVs and return flattened arrays of domain scores.\n",
    "\n",
    "    Args:\n",
    "        p_base: Baseline CSV path.\n",
    "        p_full: Full CSV path.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of flattened arrays (baseline, full).\n",
    "    \"\"\"\n",
    "    db = pd.read_csv(p_base)\n",
    "    df = pd.read_csv(p_full)\n",
    "    cols = _dom_cols(db)\n",
    "    if not cols or not set(cols).issubset(df.columns):\n",
    "        missing = sorted(set(cols) - set(df.columns))\n",
    "        raise RuntimeError(f\"{p_full.name}: missing '{_DOM_PREFIX}*' columns: {missing}\")\n",
    "    xb = db[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    xf = df[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    return xb, xf\n",
    "\n",
    "\n",
    "def section(title):\n",
    "    \"\"\"Print a visually separated section header to stdout and run.log.\"\"\"\n",
    "    bar = \"═\" * len(title)\n",
    "    print(f\"\\n{bar}\\n{title}\\n{bar}\")\n",
    "\n",
    "\n",
    "def _save_and_show(fig, path: str):\n",
    "    \"\"\"Save a matplotlib figure to disk and emit a short console message.\"\"\"\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Plot saved → {path}\")\n",
    "\n",
    "\n",
    "def aligned_ranks(mat):\n",
    "    \"\"\"Compute Hodges–Lehmann alignment and return ranks along rows (lower=better).\n",
    "\n",
    "    Args:\n",
    "        mat: 2D array where rows are blocks (e.g., folds) and columns are methods.\n",
    "\n",
    "    Returns:\n",
    "        2D integer array of ranks aligned per row.\n",
    "    \"\"\"\n",
    "    aligned = mat - np.median(mat, axis=1, keepdims=True)\n",
    "    return np.apply_along_axis(lambda r: np.argsort(np.argsort(r)) + 1, 1, aligned)\n",
    "\n",
    "\n",
    "def friedman_aligned(mat):\n",
    "    \"\"\"Compute aligned-Friedman χ² and Iman–Davenport F-statistic.\n",
    "\n",
    "    Args:\n",
    "        mat: 2D array of aligned data or raw values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(chi2_aligned, F_aligned).\n",
    "    \"\"\"\n",
    "    k = mat.shape[1]\n",
    "    chi2, _ = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff = ((mat.shape[0] - 1) * chi2) / (mat.shape[0] * (k - 1) - chi2)\n",
    "    return chi2, Ff\n",
    "\n",
    "\n",
    "def wilcoxon_matrix(mat, labels):\n",
    "    \"\"\"Compute a symmetric matrix of two-sided Wilcoxon p-values for all pairs.\n",
    "\n",
    "    Args:\n",
    "        mat: 2D array (rows = blocks, cols = methods).\n",
    "        labels: Column labels.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of raw p-values.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for i, j in combinations(range(len(labels)), 2):\n",
    "        diff = mat[:, i] - mat[:, j]\n",
    "        p = 1.0 if np.allclose(diff, 0) else wilcoxon(diff, zero_method=\"zsplit\")[1]\n",
    "        df.iat[i, j] = df.iat[j, i] = p\n",
    "    return df.round(4)\n",
    "\n",
    "\n",
    "def holm_correct_and_effects(raw_p, data, labels):\n",
    "    \"\"\"Apply Holm–Bonferroni correction and compute Cliff's delta effect sizes.\n",
    "\n",
    "    Args:\n",
    "        raw_p: DataFrame of raw p-values.\n",
    "        data: 2D array (rows = blocks, cols = methods).\n",
    "        labels: Column labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(adjusted_p_df, cliffs_delta_df).\n",
    "    \"\"\"\n",
    "    idx = list(combinations(range(len(labels)), 2))\n",
    "    pvals = [raw_p.iat[i, j] for i, j in idx]\n",
    "    _, p_adj, _, _ = multipletests(pvals, method=\"holm\")\n",
    "\n",
    "    adj_df = raw_p.copy()\n",
    "    for (i, j), p in zip(idx, p_adj):\n",
    "        adj_df.iat[i, j] = adj_df.iat[j, i] = p\n",
    "    adj_df[np.eye(len(labels), dtype=bool)] = 1.0\n",
    "\n",
    "    def cliffs_delta(x, y):\n",
    "        diffs = np.subtract.outer(x, y)\n",
    "        n = len(x) * len(y)\n",
    "        return (np.sum(diffs > 0) - np.sum(diffs < 0)) / n\n",
    "\n",
    "    delta_df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for (i, j) in idx:\n",
    "        d_ij = cliffs_delta(data[:, i], data[:, j])\n",
    "        delta_df.iat[i, j] = d_ij\n",
    "        delta_df.iat[j, i] = -d_ij\n",
    "\n",
    "    return adj_df.round(4), delta_df.round(3)\n",
    "\n",
    "\n",
    "def conover_posthoc(ranks, labels, fname_tag):\n",
    "    \"\"\"Run Conover–Iman post-hoc tests with Holm correction and write CSV.\n",
    "\n",
    "    Args:\n",
    "        ranks: 2D array of aligned ranks.\n",
    "        labels: Method labels.\n",
    "        fname_tag: Base tag for output files.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of adjusted p-values.\n",
    "    \"\"\"\n",
    "    p_df = sp.posthoc_conover_friedman(ranks, p_adjust=\"holm\")\n",
    "    p_df.index = p_df.columns = labels\n",
    "    out = TABLES_DIR / f\"{fname_tag}_conover_p.csv\"\n",
    "    p_df.to_csv(out)\n",
    "    print(\"\\nConover–Iman post-hoc p-values (Holm-adjusted):\")\n",
    "    print(p_df.round(4).to_string())\n",
    "    print(\"  ↳ saved →\", out)\n",
    "    return p_df\n",
    "\n",
    "\n",
    "def run_friedman(mat, block_name, col_labels, fname_tag):\n",
    "    \"\"\"Report Friedman statistics and post-hoc tests; write multiple CSV outputs.\n",
    "\n",
    "    Args:\n",
    "        mat: 2D array with rows as blocks (e.g., folds) and columns as methods.\n",
    "        block_name: Name for rows (e.g., 'folds').\n",
    "        col_labels: Column labels for methods.\n",
    "        fname_tag: Base tag for saved CSVs.\n",
    "    \"\"\"\n",
    "    k = len(col_labels)\n",
    "    nblocks = mat.shape[0]\n",
    "\n",
    "    col_meds = pd.Series(np.median(mat, axis=0), index=col_labels)\n",
    "    med_path = TABLES_DIR / f\"{fname_tag}_median.csv\"\n",
    "    col_meds.to_csv(med_path, header=[\"median_rrmse\"])\n",
    "    print(\n",
    "        f\"\\nMedian RRMSE per {block_name[:-1] if block_name.endswith('s') else block_name} (sorted low→high):\"\n",
    "    )\n",
    "    print(col_meds.sort_values().round(3).to_string())\n",
    "    print(\"  ↳ saved →\", med_path)\n",
    "\n",
    "    if nblocks == 2:\n",
    "        print(f\"\\nOnly two {block_name} → skipping Friedman/post-hoc.\")\n",
    "        wilc = wilcoxon_matrix(mat, col_labels)\n",
    "        print(\"\\nWilcoxon pairwise p-values:\")\n",
    "        print(wilc.round(4).to_string())\n",
    "        wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "        wilc.to_csv(wilc_path)\n",
    "        print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "        adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "        print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "        print(adj.round(4).to_string())\n",
    "        adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "        adj.to_csv(adj_path)\n",
    "        print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "        print(\"\\nCliff's Δ effect sizes:\")\n",
    "        print(delta.round(3).to_string())\n",
    "        delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "        delta.to_csv(delta_path)\n",
    "        print(\"  ↳ saved →\", delta_path)\n",
    "        return\n",
    "\n",
    "    if k == 2:\n",
    "        p = wilcoxon(mat[:, 0], mat[:, 1], zero_method=\"zsplit\")[1]\n",
    "        print(f\"\\nPaired Wilcoxon ({col_labels[0]} vs {col_labels[1]}): p = {p:.5g}\")\n",
    "        return\n",
    "\n",
    "    ranks = aligned_ranks(mat)\n",
    "    chi2_a, Ff_a = friedman_aligned(ranks)\n",
    "    chi2_o, p_o = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff_o = ((nblocks - 1) * chi2_o) / (nblocks * (k - 1) - chi2_o)\n",
    "\n",
    "    print(f\"\\n*Aligned-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_a:.3f}    F_F = {Ff_a:.3f}\")\n",
    "    print(f\"\\n*Original-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_o:.3f}    p = {p_o:.3g}    F_F = {Ff_o:.3f}\")\n",
    "\n",
    "    if nblocks < 10:\n",
    "        conover_posthoc(ranks, col_labels, fname_tag)\n",
    "    else:\n",
    "        pvals_nem = sp.posthoc_nemenyi_friedman(ranks)\n",
    "        pvals_nem.index = pvals_nem.columns = col_labels\n",
    "        nem_path = TABLES_DIR / f\"{fname_tag}_nemenyi_p.csv\"\n",
    "        pvals_nem.to_csv(nem_path)\n",
    "        print(\"\\nNemenyi p-values (aligned post-hoc):\")\n",
    "        print(pvals_nem.round(4).to_string())\n",
    "        print(\"  ↳ saved →\", nem_path)\n",
    "\n",
    "    wilc = wilcoxon_matrix(mat, col_labels)\n",
    "    print(\"\\nWilcoxon pairwise p-values:\")\n",
    "    print(wilc.round(4).to_string())\n",
    "    wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "    wilc.to_csv(wilc_path)\n",
    "    print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "    adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "    print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "    print(adj.round(4).to_string())\n",
    "    adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "    adj.to_csv(adj_path)\n",
    "    print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "    print(\"\\nCliff's Δ effect sizes:\")\n",
    "    print(delta.round(3).to_string())\n",
    "    delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "    delta.to_csv(delta_path)\n",
    "    print(\"  ↳ saved →\", delta_path)\n",
    "\n",
    "\n",
    "def vector_per_target(rrmse_array):\n",
    "    \"\"\"Reduce (folds × targets) to per-target medians; pass through if already 1D.\"\"\"\n",
    "    return np.median(rrmse_array, axis=0) if getattr(rrmse_array, \"ndim\", 1) == 2 else rrmse_array\n",
    "\n",
    "\n",
    "def matrix_per_target_compare_models(data_dict, model_list, embedding_list):\n",
    "    \"\"\"Construct a per-target summary matrix across models and embeddings.\n",
    "\n",
    "    Args:\n",
    "        data_dict: Nested dict[embedding][model] → arrays of shape (folds, targets).\n",
    "        model_list: Ordered list of models to include as columns.\n",
    "        embedding_list: Embeddings to aggregate over.\n",
    "\n",
    "    Returns:\n",
    "        2D array with rows as targets and columns as models.\n",
    "    \"\"\"\n",
    "    return np.column_stack(\n",
    "        [\n",
    "            np.median(\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        vector_per_target(data_dict[emb][model])\n",
    "                        for emb in embedding_list\n",
    "                        if emb in data_dict\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                ).reshape(-1, N_TARGETS),\n",
    "                axis=0,\n",
    "            )\n",
    "            for model in model_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  CD plots\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def cd_plot(matrix, labels, title, fname):\n",
    "    \"\"\"Draw a critical difference diagram for one condition and save the figure.\n",
    "\n",
    "    Args:\n",
    "        matrix: 2D array (rows = blocks, cols = methods).\n",
    "        labels: List of method labels corresponding to columns.\n",
    "        title: Figure title.\n",
    "        fname: Output filename in PLOTS_DIR.\n",
    "    \"\"\"\n",
    "    if matrix.shape[1] < 2:\n",
    "        print(f\"⚠  Skipping CD-plot '{title}' (need ≥2 methods, got {matrix.shape[1]})\")\n",
    "        return\n",
    "\n",
    "    ranks = aligned_ranks(matrix)\n",
    "    pvals_raw = sp.posthoc_nemenyi_friedman(ranks)\n",
    "    if not isinstance(pvals_raw, pd.DataFrame):\n",
    "        pvals = pd.DataFrame(pvals_raw, index=range(len(labels)), columns=range(len(labels)))\n",
    "    else:\n",
    "        pvals = pvals_raw.copy()\n",
    "\n",
    "    if pvals.shape != (len(labels), len(labels)):\n",
    "        pvals = pvals.iloc[: len(labels), : len(labels)]\n",
    "        if pvals.shape != (len(labels), len(labels)):\n",
    "            pvals = pd.DataFrame(\n",
    "                np.ones((len(labels), len(labels))), index=range(len(labels)), columns=range(len(labels))\n",
    "            )\n",
    "\n",
    "    pvals.index = labels\n",
    "    pvals.columns = labels\n",
    "    pvals = pvals.astype(float).fillna(1.0)\n",
    "    pvals = pd.DataFrame(np.minimum(pvals.values, pvals.values.T), index=labels, columns=labels)\n",
    "    np.fill_diagonal(pvals.values, 1.0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks.mean(0), index=labels), pvals, ax=ax)\n",
    "    ax.set_title(title, pad=10)\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    tag = Path(fname).stem\n",
    "    run_friedman(matrix, block_name=\"folds\", col_labels=labels, fname_tag=tag)\n",
    "\n",
    "\n",
    "def cd_plot_dual(matrix1, labels1, matrix2, labels2, title1, title2, fname):\n",
    "    \"\"\"Draw two critical difference diagrams side-by-side and save the figure.\n",
    "\n",
    "    Args:\n",
    "        matrix1: 2D array for the left panel.\n",
    "        labels1: Labels for left panel.\n",
    "        matrix2: 2D array for the right panel.\n",
    "        labels2: Labels for right panel.\n",
    "        title1: Title for left panel.\n",
    "        title2: Title for right panel.\n",
    "        fname: Output filename in PLOTS_DIR.\n",
    "    \"\"\"\n",
    "    if matrix1.shape[1] < 2 or matrix2.shape[1] < 2:\n",
    "        print(\"⚠  Skipping dual CD-plot (need ≥2 methods for both)\")\n",
    "        return\n",
    "\n",
    "    def _aligned_pvals(M, lbls):\n",
    "        r = aligned_ranks(M)\n",
    "        raw = sp.posthoc_nemenyi_friedman(r)\n",
    "        if not isinstance(raw, pd.DataFrame):\n",
    "            P = pd.DataFrame(raw, index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        else:\n",
    "            P = raw.copy()\n",
    "        if P.shape != (len(lbls), len(lbls)):\n",
    "            P = P.iloc[: len(lbls), : len(lbls)]\n",
    "            if P.shape != (len(lbls), len(lbls)):\n",
    "                P = pd.DataFrame(np.ones((len(lbls), len(lbls))), index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        P.index = lbls\n",
    "        P.columns = lbls\n",
    "        P = P.astype(float).fillna(1.0)\n",
    "        P = pd.DataFrame(np.minimum(P.values, P.values.T), index=lbls, columns=lbls)\n",
    "        np.fill_diagonal(P.values, 1.0)\n",
    "        return r, P\n",
    "\n",
    "    ranks1, pvals1 = _aligned_pvals(matrix1, labels1)\n",
    "    ranks2, pvals2 = _aligned_pvals(matrix2, labels2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks1.mean(0), index=labels1), pvals1, ax=ax1)\n",
    "    ax1.set_title(title1, pad=10)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks2.mean(0), index=labels2), pvals2, ax=ax2)\n",
    "    ax2.set_title(title2, pad=10)\n",
    "    plt.tight_layout()\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    base_tag = Path(fname).stem\n",
    "    section(f\"Full statistics — LEFT panel: {title1}\")\n",
    "    run_friedman(matrix1, block_name=\"folds\", col_labels=labels1, fname_tag=f\"{base_tag}__left\")\n",
    "\n",
    "    section(f\"Full statistics — RIGHT panel: {title2}\")\n",
    "    run_friedman(matrix2, block_name=\"folds\", col_labels=labels2, fname_tag=f\"{base_tag}__right\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Result file naming (compute CSVs)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def result_path(\n",
    "    student_emb: str,\n",
    "    regressor: str,\n",
    "    method: str,\n",
    "    pct: int,\n",
    "    K: int,\n",
    "    Mmax: int,\n",
    "    variant: str,\n",
    "    labels_tag: Optional[str] = None,\n",
    ") -> Path:\n",
    "    \"\"\"Build the output path for per-fold RRMSE CSVs.\n",
    "\n",
    "    Args:\n",
    "        student_emb: Student embedding key.\n",
    "        regressor: Student regressor name.\n",
    "        method: Augmentation method name.\n",
    "        pct: Percentage of synthetics relative to seeds.\n",
    "        K: Number of synthetics used.\n",
    "        Mmax: Maximum number available.\n",
    "        variant: 'baseline' or 'full'.\n",
    "        labels_tag: Optional label-source tag (e.g., 'labels_local_lasso').\n",
    "\n",
    "    Returns:\n",
    "        Path to results CSV.\n",
    "    \"\"\"\n",
    "    tag = f\"__{labels_tag}\" if labels_tag else \"\"\n",
    "    fname = f\"rrmse_perfold_{student_emb}__{regressor}__{method}__pct{pct}_K{K}__Mmax{Mmax}{tag}__{variant}.csv\"\n",
    "    return RES_DIR / fname\n",
    "\n",
    "\n",
    "def ensure_chainlr_from_legacy_pct(\n",
    "    student_emb: str,\n",
    "    method: str,\n",
    "    pct: int,\n",
    "    K: int,\n",
    "    N_SEEDS: int,\n",
    "    Mmax: int,\n",
    "    labels_tag: Optional[str] = None,\n",
    ") -> bool:\n",
    "    \"\"\"Reuse legacy FULL artifacts for chain_ERCcv_lr when conditions permit.\n",
    "\n",
    "    Args:\n",
    "        student_emb: Student embedding key.\n",
    "        method: Augmentation method.\n",
    "        pct: Percentage for which legacy exists (100, 200, 400).\n",
    "        K: Number of synthetics used.\n",
    "        N_SEEDS: Number of seeds (96).\n",
    "        Mmax: Maximum available synthetics.\n",
    "        labels_tag: Label source tag; must be None to reuse.\n",
    "\n",
    "    Returns:\n",
    "        True if a legacy file was copied to the current naming; False otherwise.\n",
    "    \"\"\"\n",
    "    if labels_tag:\n",
    "        return False\n",
    "    if not (student_emb == \"e5_base\" and LABEL_EMB == \"e5_base\"):\n",
    "        return False\n",
    "    mapping = {100: (\"n96_sps1\", 96), 200: (\"n192_sps2\", 192), 400: (\"n384_sps4\", 384)}\n",
    "    if pct not in mapping:\n",
    "        return False\n",
    "    tag, _ = mapping[pct]\n",
    "    legacy_root = RES_DIR\n",
    "    legacy_full = legacy_root / f\"rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__{tag}__full.csv\"\n",
    "    if not legacy_full.exists():\n",
    "        log.info(f\"[reuse/chain_lr] Missing legacy {legacy_full.name} → recompute instead.\")\n",
    "        return False\n",
    "\n",
    "    new_full = result_path(student_emb, \"chain_ERCcv_lr\", method, pct, K, Mmax, \"full\", labels_tag=None)\n",
    "    try:\n",
    "        if new_full.exists():\n",
    "            n_rows = sum(1 for _ in open(new_full, \"r\", encoding=\"utf-8\")) - 1\n",
    "            if n_rows >= N_SEEDS:\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df_old = pd.read_csv(legacy_full).iloc[:N_SEEDS].copy()\n",
    "    keep_cols = [c for c in df_old.columns if c.startswith(\"rrmse_domain\")]\n",
    "    if \"median_rrmse_fold\" in df_old.columns:\n",
    "        keep_cols = [\"median_rrmse_fold\"] + keep_cols\n",
    "\n",
    "    fold_col = df_old[\"fold\"].astype(int).values if \"fold\" in df_old.columns else np.arange(N_SEEDS, dtype=int)\n",
    "    df_new = pd.DataFrame(\n",
    "        {\n",
    "            \"fold\": fold_col,\n",
    "            \"method\": method,\n",
    "            \"pct\": pct,\n",
    "            \"K\": K,\n",
    "            \"M_max\": Mmax,\n",
    "            \"student\": \"S1\",\n",
    "            \"embedding\": student_emb,\n",
    "            \"regressor\": \"chain_ERCcv_lr\",\n",
    "            \"variant\": \"full\",\n",
    "        }\n",
    "    )\n",
    "    for c in keep_cols:\n",
    "        df_new[c] = df_old[c].values\n",
    "\n",
    "    new_full.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new.to_csv(new_full, index=False)\n",
    "    log.info(f\"[reuse/chain_lr] Wrote {new_full.name} from legacy {legacy_full.name}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_for_embedding(emb_key: str):\n",
    "    \"\"\"Run compute pipeline for a single student embedding key.\"\"\"\n",
    "    global STUDENT_EMB\n",
    "    if emb_key not in EMBEDDING_SPECS:\n",
    "        raise ValueError(f\"Unknown embedding '{emb_key}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "    prev = STUDENT_EMB\n",
    "    try:\n",
    "        STUDENT_EMB = emb_key\n",
    "        log.info(f\"=== BEGIN run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "        _run_core(labels_tag=None)\n",
    "        log.info(f\"=== END   run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "    finally:\n",
    "        STUDENT_EMB = prev\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Canonical teacher naming\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "_CANON_LABEL_MAP = {\n",
    "    \"default\": \"teacher_e5_base_chainERCcv_lr\",\n",
    "    \"labels_local_lasso\": \"teacher_e5_base_local_lasso\",\n",
    "}\n",
    "\n",
    "\n",
    "def canon_label(src: str) -> str:\n",
    "    \"\"\"Map a raw label-source tag to a canonical teacher name.\n",
    "\n",
    "    Args:\n",
    "        src: Raw label-source tag ('default', 'labels_*', or already canonical).\n",
    "\n",
    "    Returns:\n",
    "        Canonical teacher name used in CSV cells and plot legends.\n",
    "    \"\"\"\n",
    "    if src in _CANON_LABEL_MAP:\n",
    "        return _CANON_LABEL_MAP[src]\n",
    "    if src.startswith(\"labels_\"):\n",
    "        return f\"teacher_e5_base_{src[len('labels_'):]}\"\n",
    "    if src.startswith(\"teacher_\"):\n",
    "        return src\n",
    "    return src\n",
    "\n",
    "\n",
    "def teachers_token(teachers: List[str]) -> str:\n",
    "    \"\"\"Return a stable token representing a set of teachers for filenames.\n",
    "\n",
    "    Args:\n",
    "        teachers: List of canonical teacher names.\n",
    "\n",
    "    Returns:\n",
    "        String 'teachers_' joined by '__' in sorted order.\n",
    "    \"\"\"\n",
    "    toks = sorted(teachers)\n",
    "    return \"teachers_\" + \"__\".join(toks)\n",
    "\n",
    "\n",
    "def teacher_token(teacher: str) -> str:\n",
    "    \"\"\"Return a normalized teacher token 'teacher_*' for filenames.\n",
    "\n",
    "    Args:\n",
    "        teacher: Canonical teacher name or raw token.\n",
    "\n",
    "    Returns:\n",
    "        String starting with 'teacher_'.\n",
    "    \"\"\"\n",
    "    return f\"teacher_{teacher}\" if not teacher.startswith(\"teacher_\") else teacher\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Combined reports (REVIEW MODE) — explicit naming + canonical labels\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def say(msg: str):\n",
    "    \"\"\"Log a message once to the console and once to the run.log file.\"\"\"\n",
    "    stream = getattr(sys, \"__stdout__\", sys.stdout)\n",
    "    print(msg, file=stream, flush=True)\n",
    "    _say_logger.info(msg)\n",
    "\n",
    "\n",
    "def _bootstrap_delta_ci_from_files(p_base: Path, p_full: Path, B: int = BOOT_B, seed: int = SEED):\n",
    "    \"\"\"Compute hierarchical bootstrap CI for Δ median (baseline − full).\n",
    "\n",
    "    Args:\n",
    "        p_base: Baseline per-fold CSV.\n",
    "        p_full: Full per-fold CSV.\n",
    "        B: Number of bootstrap replicates.\n",
    "        seed: Random seed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(delta_median, ci_lo, ci_hi).\n",
    "    \"\"\"\n",
    "    db = pd.read_csv(p_base)\n",
    "    df = pd.read_csv(p_full)\n",
    "    cols = [c for c in db.columns if c.startswith(_DOM_PREFIX)]\n",
    "    if not cols:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    folds = sorted(set(db[\"fold\"]).intersection(set(df[\"fold\"])))\n",
    "    if not folds:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    xb = db.set_index(\"fold\")[cols].loc[folds].to_numpy(dtype=np.float32)\n",
    "    xf = df.set_index(\"fold\")[cols].loc[folds].to_numpy(dtype=np.float32)\n",
    "    n_f, n_d = xb.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    deltas = np.empty(B, dtype=np.float32)\n",
    "    for b in range(B):\n",
    "        f_idx = rng.integers(0, n_f, size=n_f, endpoint=False)\n",
    "        d_idx = rng.integers(0, n_d, size=n_d, endpoint=False)\n",
    "        xb_s = xb[f_idx][:, d_idx]\n",
    "        xf_s = xf[f_idx][:, d_idx]\n",
    "        deltas[b] = np.median(xb_s) - np.median(xf_s)\n",
    "    lo, hi = np.percentile(deltas, [2.5, 97.5])\n",
    "    return float(np.median(deltas)), float(lo), float(hi)\n",
    "\n",
    "\n",
    "def build_combined_reports_across_embeddings(include_alt_labels: bool = True):\n",
    "    \"\"\"Build explicit-naming summaries and plots across embeddings and teachers.\n",
    "\n",
    "    Args:\n",
    "        include_alt_labels: If True, include non-default label sources when present.\n",
    "    \"\"\"\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    methods = sorted(avail.keys()) if (METHODS is None) else [m for m in sorted(avail.keys()) if m in METHODS]\n",
    "\n",
    "    pat = re.compile(\n",
    "        r\"^rrmse_perfold_(?P<emb>.+?)__(?P<reg>.+?)__(?P<meth>.+?)__pct(?P<pct>\\d+)_K(?P<K>\\d+)__Mmax(?P<M>\\d+)\"\n",
    "        r\"(?:__(?P<labels>labels_[A-Za-z0-9_]+))?__(?P<var>baseline|full)\\.csv$\"\n",
    "    )\n",
    "\n",
    "    summary_rows, wil_rows, cliffs_rows = [], [], []\n",
    "    pooled_pairs = {}\n",
    "    pooled_pairs_by_embed = {}\n",
    "    bootstrap_rows = []\n",
    "\n",
    "    for emb in STUDENT_EMBEDDINGS:\n",
    "        files = sorted([p for p in RES_DIR.glob(f\"rrmse_perfold_{emb}__*__full.csv\") if pat.match(p.name)])\n",
    "        for f_full in files:\n",
    "            m = pat.match(f_full.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            d = m.groupdict()\n",
    "            reg, meth = d[\"reg\"], d[\"meth\"]\n",
    "            pct, K, Mmax = int(d[\"pct\"]), int(d[\"K\"]), int(d[\"M\"])\n",
    "            labels_tag = (d.get(\"labels\") or \"\").strip()\n",
    "            raw_label_src = labels_tag if labels_tag else \"default\"\n",
    "            if (not include_alt_labels) and labels_tag:\n",
    "                continue\n",
    "            if pct not in PCT_LIST or (methods and meth not in methods):\n",
    "                continue\n",
    "\n",
    "            label_src = canon_label(raw_label_src)\n",
    "\n",
    "            p_full = f_full\n",
    "            p_base = result_path(\n",
    "                emb,\n",
    "                reg,\n",
    "                meth,\n",
    "                pct,\n",
    "                K,\n",
    "                Mmax,\n",
    "                \"baseline\",\n",
    "                labels_tag=None if raw_label_src == \"default\" else raw_label_src,\n",
    "            )\n",
    "            if not p_base.exists():\n",
    "                continue\n",
    "\n",
    "            g_base = _global_median_from_file(p_base)\n",
    "            g_full = _global_median_from_file(p_full)\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"student\": \"S1\",\n",
    "                    \"embedding\": emb,\n",
    "                    \"regressor\": reg,\n",
    "                    \"method\": meth,\n",
    "                    \"pct\": pct,\n",
    "                    \"K\": K,\n",
    "                    \"M_max\": Mmax,\n",
    "                    \"baseline\": g_base,\n",
    "                    \"full\": g_full,\n",
    "                    \"labels_source\": label_src,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            xb, xf = _flat_arrays(p_base, p_full)\n",
    "            pooled_pairs.setdefault((meth, pct, label_src), {\"base\": [], \"full\": []})\n",
    "            pooled_pairs[(meth, pct, label_src)][\"base\"].extend(xb.tolist())\n",
    "            pooled_pairs[(meth, pct, label_src)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "            pooled_pairs_by_embed.setdefault((emb, meth, pct, label_src), {\"base\": [], \"full\": []})\n",
    "            pooled_pairs_by_embed[(emb, meth, pct, label_src)][\"base\"].extend(xb.tolist())\n",
    "            pooled_pairs_by_embed[(emb, meth, pct, label_src)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "            except Exception:\n",
    "                p_raw = 1.0\n",
    "            dlt, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "            wil_rows.append(\n",
    "                {\n",
    "                    \"student\": \"S1\",\n",
    "                    \"embedding\": emb,\n",
    "                    \"regressor\": reg,\n",
    "                    \"method\": meth,\n",
    "                    \"pct\": pct,\n",
    "                    \"K\": K,\n",
    "                    \"M_max\": Mmax,\n",
    "                    \"labels_source\": label_src,\n",
    "                    \"p_raw\": float(p_raw),\n",
    "                }\n",
    "            )\n",
    "            cliffs_rows.append(\n",
    "                {\n",
    "                    \"student\": \"S1\",\n",
    "                    \"embedding\": emb,\n",
    "                    \"regressor\": reg,\n",
    "                    \"method\": meth,\n",
    "                    \"pct\": pct,\n",
    "                    \"K\": K,\n",
    "                    \"M_max\": Mmax,\n",
    "                    \"labels_source\": label_src,\n",
    "                    \"cliffs_delta_paired\": float(dlt),\n",
    "                    \"npos\": int(npos),\n",
    "                    \"nneg\": int(nneg),\n",
    "                    \"nzero\": int(nzero),\n",
    "                    \"magnitude\": mag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            d_med, ci_lo, ci_hi = _bootstrap_delta_ci_from_files(p_base, p_full, B=BOOT_B, seed=SEED)\n",
    "            bootstrap_rows.append(\n",
    "                {\n",
    "                    \"embedding\": emb,\n",
    "                    \"regressor\": reg,\n",
    "                    \"method\": meth,\n",
    "                    \"pct\": pct,\n",
    "                    \"K\": K,\n",
    "                    \"M_max\": Mmax,\n",
    "                    \"labels_source\": label_src,\n",
    "                    \"delta_median\": d_med,\n",
    "                    \"ci_lo\": ci_lo,\n",
    "                    \"ci_hi\": ci_hi,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not summary_rows:\n",
    "        say(\"[review] No result pairs found to summarise.\")\n",
    "        return\n",
    "\n",
    "    wide = pd.DataFrame(summary_rows).sort_values(\n",
    "        [\"labels_source\", \"embedding\", \"regressor\", \"method\", \"pct\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    tol = 1e-9\n",
    "\n",
    "    def _cmp(row):\n",
    "        b, f = row[\"baseline\"], row[\"full\"]\n",
    "        if pd.isna(b) or pd.isna(f):\n",
    "            return \"n/a\"\n",
    "        if (b - f) > tol:\n",
    "            return \"better\"\n",
    "        if (f - b) > tol:\n",
    "            return \"worse\"\n",
    "        return \"same\"\n",
    "\n",
    "    wide[\"baseline_vs_full\"] = wide.apply(_cmp, axis=1)\n",
    "\n",
    "    comp = wide.copy()\n",
    "    comp[\"delta\"] = comp[\"baseline\"] - comp[\"full\"]\n",
    "    comp[\"rel_change_pct\"] = 100.0 * comp[\"delta\"] / comp[\"baseline\"].replace(0, np.nan)\n",
    "\n",
    "    if bootstrap_rows:\n",
    "        boot = pd.DataFrame(bootstrap_rows).sort_values(\n",
    "            [\"labels_source\", \"embedding\", \"regressor\", \"method\", \"pct\"]\n",
    "        )\n",
    "    else:\n",
    "        boot = pd.DataFrame(\n",
    "            columns=[\"embedding\", \"regressor\", \"method\", \"pct\", \"K\", \"M_max\", \"labels_source\", \"delta_median\", \"ci_lo\", \"ci_hi\"]\n",
    "        )\n",
    "\n",
    "    EMB_LIST = sorted(wide[\"embedding\"].unique())\n",
    "    for emb in EMB_LIST:\n",
    "        teachers = sorted(wide.loc[wide[\"embedding\"] == emb, \"labels_source\"].unique())\n",
    "        teachers_tok = teachers_token(teachers)\n",
    "\n",
    "        # Summaries combined over present teachers\n",
    "        sub_all = wide[wide[\"embedding\"] == emb].copy()\n",
    "        sub_all.to_csv(OUT_TABLES / f\"summary_median_rrmse__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "\n",
    "        comp_all = comp[comp[\"embedding\"] == emb].copy()\n",
    "        comp_all.to_csv(OUT_TABLES / f\"summary_median_rrmse_with_delta__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "\n",
    "        # Bootstrap combined\n",
    "        boot_all = boot[boot[\"embedding\"] == emb].copy()\n",
    "        boot_all.to_csv(OUT_TABLES / f\"bootstrap_delta_ci__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "\n",
    "        # Per-config Wilcoxon (raw & Holm)\n",
    "        if wil_rows:\n",
    "            wil_df = pd.DataFrame(wil_rows)\n",
    "            wil_df = wil_df[wil_df[\"embedding\"] == emb].sort_values([\"labels_source\", \"regressor\", \"method\", \"pct\"])\n",
    "            wil_df.to_csv(OUT_TABLES / f\"wilcoxon_vs_baseline__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "\n",
    "            holm_chunks = []\n",
    "            for _, grp in wil_df.groupby([\"labels_source\", \"regressor\", \"method\"], dropna=False):\n",
    "                _, p_adj, _, _ = multipletests(grp[\"p_raw\"].values, method=\"holm\")\n",
    "                g2 = grp.copy()\n",
    "                g2[\"p_holm\"] = p_adj\n",
    "                holm_chunks.append(g2)\n",
    "            if holm_chunks:\n",
    "                holm_df = pd.concat(holm_chunks, axis=0).sort_values([\"labels_source\", \"regressor\", \"method\", \"pct\"])\n",
    "                holm_df.to_csv(\n",
    "                    OUT_TABLES / f\"wilcoxon_holm_vs_baseline__embedding_{emb}__{teachers_tok}.csv\", index=False\n",
    "                )\n",
    "                for t in teachers:\n",
    "                    holm_df[holm_df[\"labels_source\"] == t].to_csv(\n",
    "                        OUT_TABLES / f\"wilcoxon_holm_vs_baseline__embedding_{emb}__{teacher_token(t)}.csv\", index=False\n",
    "                    )\n",
    "\n",
    "        # Cliff's delta (paired deltas)\n",
    "        if cliffs_rows:\n",
    "            cliffs_df = pd.DataFrame(cliffs_rows)\n",
    "            cliffs_df = cliffs_df[cliffs_df[\"embedding\"] == emb].sort_values(\n",
    "                [\"labels_source\", \"regressor\", \"method\", \"pct\"]\n",
    "            )\n",
    "            cliffs_df.to_csv(OUT_TABLES / f\"cliffs_delta_paired__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "            for t in teachers:\n",
    "                cliffs_df[cliffs_df[\"labels_source\"] == t].to_csv(\n",
    "                    OUT_TABLES / f\"cliffs_delta_vs_baseline__embedding_{emb}__{teacher_token(t)}.csv\", index=False\n",
    "                )\n",
    "\n",
    "        # Pooled tests combined and per teacher\n",
    "        pooled_rows_df = []\n",
    "        for (meth, pct, lbl), pair in pooled_pairs.items():\n",
    "            if lbl not in teachers:\n",
    "                continue\n",
    "            xb = np.array(pair[\"base\"], dtype=np.float32)\n",
    "            xf = np.array(pair[\"full\"], dtype=np.float32)\n",
    "            if xb.size == 0 or xf.size == 0:\n",
    "                continue\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "            except Exception:\n",
    "                p_raw = 1.0\n",
    "            dlt, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "            pooled_rows_df.append(\n",
    "                {\n",
    "                    \"method\": meth,\n",
    "                    \"pct\": pct,\n",
    "                    \"labels_source\": lbl,\n",
    "                    \"p_raw\": float(p_raw),\n",
    "                    \"cliffs_delta\": float(dlt),\n",
    "                    \"npos\": int(npos),\n",
    "                    \"nneg\": int(nneg),\n",
    "                    \"nzero\": int(nzero),\n",
    "                    \"magnitude\": mag,\n",
    "                }\n",
    "            )\n",
    "        if pooled_rows_df:\n",
    "            pooled_df = pd.DataFrame(pooled_rows_df).sort_values([\"labels_source\", \"method\", \"pct\"])\n",
    "            pooled_df.to_csv(OUT_TABLES / f\"wilcoxon_cliffs_pooled__embedding_{emb}__{teachers_tok}.csv\", index=False)\n",
    "            pooled_df.rename(columns={\"p_raw\": \"p_value\"}).to_csv(\n",
    "                OUT_TABLES / f\"wilcoxon_pooled_vs_baseline__embedding_{emb}__{teachers_tok}.csv\", index=False\n",
    "            )\n",
    "            pooled_df[\n",
    "                [\"method\", \"pct\", \"labels_source\", \"cliffs_delta\", \"magnitude\", \"npos\", \"nneg\", \"nzero\"]\n",
    "            ].to_csv(\n",
    "                OUT_TABLES / f\"cliffs_delta_pooled_vs_baseline__embedding_{emb}__{teachers_tok}.csv\", index=False\n",
    "            )\n",
    "            for t in teachers:\n",
    "                sub_t = pooled_df[pooled_df[\"labels_source\"] == t]\n",
    "                sub_t.rename(columns={\"p_raw\": \"p_value\"}).to_csv(\n",
    "                    OUT_TABLES / f\"wilcoxon_pooled_vs_baseline__embedding_{emb}__{teacher_token(t)}.csv\", index=False\n",
    "                )\n",
    "                sub_t[\n",
    "                    [\"method\", \"pct\", \"labels_source\", \"cliffs_delta\", \"magnitude\", \"npos\", \"nneg\", \"nzero\"]\n",
    "                ].to_csv(\n",
    "                    OUT_TABLES / f\"cliffs_delta_pooled_vs_baseline__embedding_{emb}__{teacher_token(t)}.csv\",\n",
    "                    index=False,\n",
    "                )\n",
    "\n",
    "        # Plots per teacher and overlay compares\n",
    "        for meth in sorted(wide[\"method\"].unique()):\n",
    "            for t in teachers:\n",
    "                sub = wide[(wide[\"embedding\"] == emb) & (wide[\"method\"] == meth) & (wide[\"labels_source\"] == t)]\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                grp = sub.groupby(\"pct\", as_index=False)[[\"baseline\", \"full\"]].median()\n",
    "                # RRMSE curves\n",
    "                fig, ax = plt.subplots(figsize=(6, 4), dpi=120)\n",
    "                ax.plot(grp[\"pct\"], grp[\"baseline\"], marker=\"o\", label=\"baseline\")\n",
    "                ax.plot(grp[\"pct\"], grp[\"full\"], marker=\"o\", label=\"full\")\n",
    "                ax.set_xlabel(\"%K\")\n",
    "                ax.set_ylabel(\"Global median RRMSE\")\n",
    "                ax.set_title(f\"{meth} [{t}]\")\n",
    "                ax.legend()\n",
    "                fig.savefig(\n",
    "                    PLOTS_DIR / f\"combined_rrmse_vs_pct__{meth}__embedding_{emb}__{teacher_token(t)}.png\",\n",
    "                    bbox_inches=\"tight\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                plt.close(fig)\n",
    "                # Δ curves\n",
    "                grp[\"delta\"] = grp[\"baseline\"] - grp[\"full\"]\n",
    "                fig, ax = plt.subplots(figsize=(6, 4), dpi=120)\n",
    "                ax.plot(grp[\"pct\"], grp[\"delta\"], marker=\"o\")\n",
    "                ax.set_xlabel(\"%K\")\n",
    "                ax.set_ylabel(\"Δ median RRMSE (baseline - full)\")\n",
    "                ax.set_title(f\"{meth} [{t}] Δ vs %K\")\n",
    "                fig.savefig(\n",
    "                    PLOTS_DIR / f\"combined_delta_vs_pct__{meth}__embedding_{emb}__{teacher_token(t)}.png\",\n",
    "                    bbox_inches=\"tight\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                plt.close(fig)\n",
    "\n",
    "            piv = (\n",
    "                wide[(wide[\"embedding\"] == emb) & (wide[\"method\"] == meth)]\n",
    "                .groupby([\"labels_source\", \"pct\"], as_index=False)[\"full\"]\n",
    "                .median()\n",
    "            )\n",
    "            if not piv.empty and piv[\"labels_source\"].nunique() >= 2:\n",
    "                fig, ax = plt.subplots(figsize=(6, 4), dpi=120)\n",
    "                teachers_sorted = sorted(piv[\"labels_source\"].unique())\n",
    "                for t in teachers_sorted:\n",
    "                    chunk = piv[piv[\"labels_source\"] == t]\n",
    "                    ax.plot(chunk[\"pct\"], chunk[\"full\"], marker=\"o\", label=f\"{t} (full)\")\n",
    "                ax.set_xlabel(\"%K\")\n",
    "                ax.set_ylabel(\"Global median RRMSE\")\n",
    "                ax.set_title(f\"{meth} — label sources (FULL)\")\n",
    "                ax.legend()\n",
    "                fig.savefig(\n",
    "                    PLOTS_DIR\n",
    "                    / f\"combined_rrmse_vs_pct__{meth}__embedding_{emb}__{teachers_token(teachers_sorted)}__compare.png\",\n",
    "                    bbox_inches=\"tight\",\n",
    "                    dpi=300,\n",
    "                )\n",
    "                plt.close(fig)\n",
    "\n",
    "    say(\"[review] Summaries & plots written with explicit embedding/teacher naming.\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Core run (compute)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def _run_core(labels_tag: Optional[str] = None):\n",
    "    \"\"\"Run the compute pipeline for the current STUDENT_EMB and label source.\n",
    "\n",
    "    Args:\n",
    "        labels_tag: Optional suffix to disambiguate label sources in filenames.\n",
    "    \"\"\"\n",
    "    t0_all = time.time()\n",
    "    write_run_config()\n",
    "\n",
    "    seeds_df = load_seeds()\n",
    "    N_SEEDS = len(seeds_df)\n",
    "    X_seed = ensure_seed_vectors(seeds_df, STUDENT_EMB)\n",
    "    Y_seed = seeds_df[TARGET_COLS].to_numpy(dtype=np.float32)\n",
    "\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    methods = sorted(avail.keys()) if (METHODS is None) else [m for m in sorted(avail.keys()) if m in METHODS]\n",
    "    if not methods:\n",
    "        raise FileNotFoundError(f\"No labeled synthetics found in {G2_DIR} for embedding={LABEL_EMB}\")\n",
    "\n",
    "    folds = list(range(N_SEEDS)) if MAX_FOLDS <= 0 else list(range(min(MAX_FOLDS, N_SEEDS)))\n",
    "    say(f\"[run] STUDENT_EMB={STUDENT_EMB} | LABEL_SRC={LABEL_EMB}+{LABEL_MODEL} | labels_tag={labels_tag or 'default'}\")\n",
    "    say(f\"[run] methods={methods} | folds={len(folds)} | PCT_LIST={PCT_LIST}\")\n",
    "\n",
    "    for method in methods:\n",
    "        Mmax = max(avail[method])\n",
    "        npy_path, _ = ensure_synth_cache(method, Mmax, STUDENT_EMB)\n",
    "        X_synth = np.load(npy_path)\n",
    "        n_synth_rows = X_synth.shape[0]\n",
    "\n",
    "        for pct in PCT_LIST:\n",
    "            K = pct_to_K(pct, N_SEEDS)\n",
    "            if K > n_synth_rows:\n",
    "                log.warning(f\"[{method}] pct={pct} → K={K} exceeds Mmax={n_synth_rows}; clipping to Mmax\")\n",
    "                K = n_synth_rows\n",
    "\n",
    "            for reg in STUDENTS:\n",
    "                p_base = result_path(STUDENT_EMB, reg, method, pct, K, Mmax, \"baseline\", labels_tag=labels_tag)\n",
    "                p_full = result_path(STUDENT_EMB, reg, method, pct, K, Mmax, \"full\", labels_tag=labels_tag)\n",
    "\n",
    "                def _is_complete(p: Path) -> bool:\n",
    "                    \"\"\"Return True if the CSV exists and has ≥ len(folds) rows.\"\"\"\n",
    "                    try:\n",
    "                        if not p.exists():\n",
    "                            return False\n",
    "                        n_rows = sum(1 for _ in open(p, \"r\", encoding=\"utf-8\")) - 1\n",
    "                        return max(0, n_rows) >= len(folds)\n",
    "                    except Exception:\n",
    "                        return False\n",
    "\n",
    "                base_done = _is_complete(p_base)\n",
    "                full_done = _is_complete(p_full)\n",
    "\n",
    "                if (reg == \"chain_ERCcv_lr\") and (pct in {100, 200, 400}) and not full_done:\n",
    "                    reused = ensure_chainlr_from_legacy_pct(\n",
    "                        STUDENT_EMB, method, pct, K, N_SEEDS, Mmax, labels_tag=labels_tag\n",
    "                    )\n",
    "                    if reused:\n",
    "                        full_done = True\n",
    "\n",
    "                if base_done and full_done:\n",
    "                    log.info(f\"[skip] {p_base.name} & {p_full.name} complete.\")\n",
    "                    continue\n",
    "\n",
    "                rows_base, rows_full = [], []\n",
    "                t_start = time.time()\n",
    "\n",
    "                for i, fold_idx in enumerate(folds):\n",
    "                    if (i % LOG_EVERY) == 0:\n",
    "                        log.info(f\"[{method} | {reg} | pct={pct}] fold {i+1}/{len(folds)} …\")\n",
    "\n",
    "                    tr_idx = np.array([j for j in range(N_SEEDS) if j != fold_idx], dtype=int)\n",
    "                    y_dummy = np.mean(Y_seed[tr_idx], axis=0, dtype=np.float32)\n",
    "\n",
    "                    teacher_est = load_teacher_fold_estimator(fold_idx, reg, STUDENT_EMB)\n",
    "                    hp = hp_from_teacher_or_json(fold_idx, reg, STUDENT_EMB, teacher_est)\n",
    "                    hp_out = HP_PERFOLD / reg / method / f\"n{Mmax}\" / f\"pct{pct}_K{K}\"\n",
    "                    hp_out.mkdir(parents=True, exist_ok=True)\n",
    "                    (hp_out / f\"fold{fold_idx:02d}_best.json\").write_text(json.dumps(hp, indent=2))\n",
    "\n",
    "                    model_base = build_with_hp(reg, hp)\n",
    "\n",
    "                    if not base_done:\n",
    "                        model_base.fit(X_seed[tr_idx], Y_seed[tr_idx])\n",
    "                        y_true = Y_seed[fold_idx : fold_idx + 1]\n",
    "                        y_pred = model_base.predict(X_seed[fold_idx : fold_idx + 1])\n",
    "                        rr = rrmse_vs_dummy(y_true, y_pred, y_dummy.reshape(1, -1)).flatten()\n",
    "                        row = {\n",
    "                            \"fold\": int(fold_idx),\n",
    "                            \"method\": method,\n",
    "                            \"pct\": int(pct),\n",
    "                            \"K\": int(K),\n",
    "                            \"M_max\": int(Mmax),\n",
    "                            \"student\": \"S1\",\n",
    "                            \"embedding\": STUDENT_EMB,\n",
    "                            \"regressor\": reg,\n",
    "                            \"variant\": \"baseline\",\n",
    "                            \"median_rrmse_fold\": float(np.median(rr)),\n",
    "                        }\n",
    "                        for t_idx in range(1, 15):\n",
    "                            row[f\"{_DOM_PREFIX}{t_idx}\"] = float(rr[t_idx - 1])\n",
    "                        rows_base.append(row)\n",
    "\n",
    "                    if not full_done:\n",
    "                        lbl_csv = g2_label_file(fold_idx, method, Mmax, LABEL_EMB, LABEL_MODEL, strict_model=True)\n",
    "                        if not lbl_csv.exists():\n",
    "                            raise FileNotFoundError(f\"Missing labels: {lbl_csv}\")\n",
    "                        df_lbl = pd.read_csv(lbl_csv)\n",
    "                        if any(c not in df_lbl.columns for c in TARGET_COLS):\n",
    "                            raise RuntimeError(f\"{lbl_csv.name}: missing target cols {TARGET_COLS}\")\n",
    "                        Y_syn = df_lbl[TARGET_COLS].to_numpy(dtype=np.float32)[:K, :]\n",
    "                        X_syn = X_synth[:K, :]\n",
    "\n",
    "                        model_full = build_with_hp(reg, hp)\n",
    "                        X_tr_full = np.vstack([X_seed[tr_idx], X_syn])\n",
    "                        Y_tr_full = np.vstack([Y_seed[tr_idx], Y_syn])\n",
    "                        model_full.fit(X_tr_full, Y_tr_full)\n",
    "\n",
    "                        y_true = Y_seed[fold_idx : fold_idx + 1]\n",
    "                        y_pred = model_full.predict(X_seed[fold_idx : fold_idx + 1])\n",
    "                        rr = rrmse_vs_dummy(y_true, y_pred, y_dummy.reshape(1, -1)).flatten()\n",
    "                        row = {\n",
    "                            \"fold\": int(fold_idx),\n",
    "                            \"method\": method,\n",
    "                            \"pct\": int(pct),\n",
    "                            \"K\": int(K),\n",
    "                            \"M_max\": int(Mmax),\n",
    "                            \"student\": \"S1\",\n",
    "                            \"embedding\": STUDENT_EMB,\n",
    "                            \"regressor\": reg,\n",
    "                            \"variant\": \"full\",\n",
    "                            \"median_rrmse_fold\": float(np.median(rr)),\n",
    "                        }\n",
    "                        for t_idx in range(1, 15):\n",
    "                            row[f\"{_DOM_PREFIX}{t_idx}\"] = float(rr[t_idx - 1])\n",
    "                        rows_full.append(row)\n",
    "\n",
    "                if rows_base and (not base_done):\n",
    "                    pd.DataFrame(rows_base).sort_values(\"fold\").to_csv(p_base, index=False)\n",
    "                    log.info(f\"[write] {p_base.name} ({len(rows_base)} rows, { _fmt_dt(time.time()-t_start) })\")\n",
    "                if rows_full and (not full_done):\n",
    "                    pd.DataFrame(rows_full).sort_values(\"fold\").to_csv(p_full, index=False)\n",
    "                    log.info(f\"[write] {p_full.name} ({len(rows_full)} rows, { _fmt_dt(time.time()-t_start) })\")\n",
    "\n",
    "    say(f\"[run] completed in { _fmt_dt(time.time()-t0_all) }\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Pipelines: compute & review\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def _run_students_for_label_source(\n",
    "    label_emb: str, label_model: str, labels_tag: Optional[str], restrict_students: Optional[List[str]] = None\n",
    "):\n",
    "    \"\"\"Run the compute pipeline for a specific label source and restricted students.\n",
    "\n",
    "    Args:\n",
    "        label_emb: Label-source embedding key.\n",
    "        label_model: Label-source teacher model.\n",
    "        labels_tag: Tag to append to results filenames for this label source.\n",
    "        restrict_students: Optional list of student keys to run.\n",
    "    \"\"\"\n",
    "    global LABEL_EMB, LABEL_MODEL, STUDENTS\n",
    "\n",
    "    _orig_label_emb, _orig_label_model = LABEL_EMB, LABEL_MODEL\n",
    "    students_backup = list(STUDENTS)\n",
    "    try:\n",
    "        LABEL_EMB, LABEL_MODEL = label_emb, label_model\n",
    "\n",
    "        if restrict_students is not None:\n",
    "            STUDENTS = [s for s in students_backup if s in restrict_students]\n",
    "            if not STUDENTS:\n",
    "                log.warning(\"No matching students in restrict_students=%s; skipping.\", restrict_students)\n",
    "                return\n",
    "\n",
    "        log.info(\"[labels] Running students %s on labels %s+%s (tag=%s)\", STUDENTS, LABEL_EMB, LABEL_MODEL, labels_tag)\n",
    "        _run_core(labels_tag=labels_tag)\n",
    "    finally:\n",
    "        LABEL_EMB, LABEL_MODEL = _orig_label_emb, _orig_label_model\n",
    "        STUDENTS = students_backup\n",
    "\n",
    "\n",
    "def run_pipeline_compute():\n",
    "    \"\"\"Run compute passes for default label source, then any extra label-source jobs.\"\"\"\n",
    "    for emb_key in STUDENT_EMBEDDINGS:\n",
    "        run_for_embedding(emb_key)\n",
    "    for job in EXTRA_STUDENT_LABEL_JOBS:\n",
    "        _run_students_for_label_source(\n",
    "            label_emb=job[\"label_emb\"],\n",
    "            label_model=job[\"label_model\"],\n",
    "            labels_tag=job.get(\"labels_tag\"),\n",
    "            restrict_students=[job[\"student\"]],\n",
    "        )\n",
    "\n",
    "\n",
    "def run_pipeline_review():\n",
    "    \"\"\"Build explicit-naming summaries and plots from existing compute artifacts.\"\"\"\n",
    "    build_combined_reports_across_embeddings(include_alt_labels=True)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "#  Entry point\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        write_run_config()\n",
    "        if REVIEW_MODE:\n",
    "            run_pipeline_review()\n",
    "        else:\n",
    "            run_pipeline_compute()\n",
    "        print(\"Run completed.\")\n",
    "    except Exception as e:\n",
    "        log.exception(\"Fatal error in step e_3: %s\", e)\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcd452-ece2-41a8-bdbf-60404a98b6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr8cht_review_anonymous",
   "language": "python",
   "name": "kr8cht_review_anonymous"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
