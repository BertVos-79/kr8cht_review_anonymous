{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af744b6e-79f3-42f0-8e27-d6e55d20b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "2025-10-16 00:35:34 INFO: run_config.json written.\n",
      "[combined] summary_median_rrmse.csv written (unified, all pcts).\n",
      "[combined] summary_median_rrmse_with_delta.csv written (unified).\n",
      "[combined] wilcoxon_holm_vs_baseline.csv written.\n",
      "[combined] cliffs_delta_vs_baseline.csv written.\n",
      "[combined] pooled tests written.\n",
      "[combined] bootstrap_delta_ci.csv written.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_3_student_scoring.ipynb\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "LOOCV student scoring with per-fold hyperparameters reused from teacher models\n",
    "(no tuning) and percent-based augmentation sizes. Student embedding is selectable;\n",
    "labels are read from a configurable teacher label source.\n",
    "\n",
    "This script:\n",
    "1) Loads seed data and resources\n",
    "   - Reads 96 seeds (texts + 14 targets).\n",
    "   - Loads or computes seed sentence embeddings for the selected student embedding.\n",
    "   - Discovers augmentation methods and their M_max from tuned teacher outputs.\n",
    "   - Preloads synthetic text embeddings for the selected student embedding and\n",
    "     per-fold tuned labels from the label source (e.g., e5_base + chain_ERCcv_lr).\n",
    "\n",
    "2) Builds student regressors from teacher hyperparameters (no tuning)\n",
    "   - Students: chain_ERCcv_lr, local_lasso, local_rf, global_rf, chain_ERCcv_rf.\n",
    "   - Extracts per-fold HPs from teacher pickles (step_g_2a) for the same\n",
    "     student embedding; raises an error if missing (no silent defaults).\n",
    "\n",
    "3) Evaluates the baseline (seeds-only)\n",
    "   - For each LOOCV fold and student, loads that fold’s teacher estimator\n",
    "     (if present) and predicts the held-out seed; otherwise rebuilds the\n",
    "     same pipeline from the teacher’s stored HPs and predicts.\n",
    "   - Writes per-pct baseline files for naming symmetry.\n",
    "\n",
    "4) Evaluates the augmented “full” variant (seeds + synthetics)\n",
    "   - Percent mode: P ∈ {10, 20, 50, 100, 200, 400}; K = round(P% of 96).\n",
    "   - For each fold, takes the first K items from that fold’s M_max labeled set\n",
    "     (labels from the label source), fits the student with the per-fold HPs,\n",
    "     and predicts the held-out seed.\n",
    "   - Idempotent: computes only missing folds and preserves already complete CSVs.\n",
    "   - Legacy reuse for chain_ERCcv_lr at {100,200,400} applies only when the\n",
    "     student embedding equals the label-source embedding (e.g., e5_base).\n",
    "\n",
    "5) Produces unified summary tables (no PRIMARY/APPENDIX split)\n",
    "   - Aggregates per-fold median RRMSE into a wide summary with columns:\n",
    "     baseline, full, baseline_vs_full.\n",
    "   - Writes a comparison table with ΔRRMSE and relative % change.\n",
    "\n",
    "6) Performs statistical analyses\n",
    "   - Per-config Wilcoxon signed-rank tests (one-sided, alternative=\"less\",\n",
    "     zero_method=\"pratt\") with Holm correction grouped by (regressor, method).\n",
    "   - Paired Cliff’s delta per configuration.\n",
    "   - Pooled Wilcoxon and Cliff’s delta across students per (method, pct).\n",
    "   - Hierarchical bootstrap (folds × domains micro-bootstrap) for Δ median with 95% CI.\n",
    "\n",
    "7) Visualizes performance\n",
    "   - RRMSE vs %K and ΔRRMSE vs %K for the configured PCTS_FOR_PLOTS.\n",
    "\n",
    "Idempotency & disk reuse:\n",
    "- Reuses seed embeddings if shape and max_seq_len match; otherwise rebuilds.\n",
    "- Reuses synthetic embeddings (cache); builds them if missing.\n",
    "- Skips FULL computations for (student, method, pct) whose per-fold CSV is complete.\n",
    "- Optional: skip recomputing baseline if all baseline CSVs are already complete.\n",
    "- Optional: summarization-only mode to build tables/plots from existing CSVs.\n",
    "\n",
    "Inputs:\n",
    "- data/activity_scores.csv\n",
    "- data/activities.csv\n",
    "- outputs/results/{student_embedding}_vectors.npy\n",
    "- outputs/e_1_synth_augmentation/g_final_n{M}_{method}.csv\n",
    "- outputs/e_2_teacher_labeling/g2f_labels_fold{ii}_n{M}_{method}__{label_embedding}__{label_model}.csv\n",
    "- models/teacher/teacher_fold{ii}_{student_embedding}__{student}.pkl\n",
    "- outputs/e_2_teacher_labeling/cache/synth_embeds/*__{student_embedding}.npy and *__index.csv\n",
    "- (legacy reuse, only if student_embedding==label_embedding==e5_base and student==chain_ERCcv_lr)\n",
    "  outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n96_sps1__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n192_sps2__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n384_sps4__full.csv\n",
    "\n",
    "Outputs:\n",
    "- outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}__baseline.csv\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}__full.csv\n",
    "- outputs/e_3_student_scoring/hp_perfold/\n",
    "    {regressor}/{method}/n{Mmax}/pct{P}_K{K}/fold{ii}_best.json\n",
    "- outputs/e_3_student_scoring/tables/\n",
    "    summary_median_rrmse.csv\n",
    "    summary_median_rrmse_with_delta.csv\n",
    "    wilcoxon_holm_vs_baseline.csv\n",
    "    cliffs_delta_vs_baseline.csv\n",
    "    wilcoxon_pooled_vs_baseline.csv\n",
    "    cliffs_delta_pooled_vs_baseline.csv\n",
    "    bootstrap_delta_ci.csv\n",
    "- outputs/e_3_student_scoring/plots/\n",
    "    combined_rrmse_vs_pct__{method}.png\n",
    "    combined_delta_vs_pct__{method}.png\n",
    "    combined_rrmse_vs_pct__{method}__{embedding|all}.png\n",
    "    combined_delta_vs_pct__{method}__{embedding|all}.png\n",
    "- outputs/e_3_student_scoring/\n",
    "    cache/, run.log, run_config.json\n",
    "\"\"\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Imports\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from itertools import combinations, cycle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Paths \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def project_root(marker: str = \"LICENSE\") -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for d in (here, *here.parents):\n",
    "        if (d / marker).is_file():\n",
    "            return d\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "ROOT = project_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "G1_DIR  = ROOT / \"outputs\" / \"e_1_synth_augmentation\"\n",
    "G2_DIR  = ROOT / \"outputs\" / \"e_2_teacher_labeling\"\n",
    "G3A_DIR = ROOT / \"outputs\" / \"e_3_student_scoring\"\n",
    "\n",
    "RES_DIR     = G3A_DIR / \"results\"\n",
    "TABLES_DIR  = G3A_DIR / \"tables\"\n",
    "CACHE_DIR   = G3A_DIR / \"cache\"\n",
    "HP_PERFOLD  = G3A_DIR / \"hp_perfold\"\n",
    "FIGS_DIR    = G3A_DIR / \"plots\"\n",
    "\n",
    "for p in (G3A_DIR, RES_DIR, TABLES_DIR, CACHE_DIR, HP_PERFOLD, FIGS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TABLES = TABLES_DIR\n",
    "PLOTS_DIR = FIGS_DIR\n",
    "OUT_PLOTS  = FIGS_DIR\n",
    "\n",
    "LOG_FILE = G3A_DIR / \"run.log\"\n",
    "for h in list(logging.root.handlers):\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Logging \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\"),\n",
    "        # use the uncaptured stdout to bypass notebook/pytest capturing\n",
    "        logging.StreamHandler(getattr(sys, \"__stdout__\", sys.stdout)),\n",
    "    ],\n",
    "    force=True,\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "_say_logger = logging.getLogger(\"sayfile\")\n",
    "if not _say_logger.handlers:\n",
    "    _say_logger.setLevel(logging.INFO)\n",
    "    _say_logger.propagate = False  # do NOT bubble to root (prevents console echo)\n",
    "    _fh = logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\")\n",
    "    _fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "                                       datefmt=\"%Y-%m-%d %H:%M:%S\"))\n",
    "    _say_logger.addHandler(_fh)\n",
    "    \n",
    "RUN_ID: str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Config\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "REVIEW_MODE: bool = True\n",
    "\n",
    "N_TARGETS = 14\n",
    "\n",
    "METHODS: Optional[List[str]] = None\n",
    "\n",
    "PCT_LIST: List[int]       = [10, 20, 50, 100, 200, 400]\n",
    "PCTS_FOR_PLOTS: List[int] = [10, 20, 50, 100, 200, 400]\n",
    "PRIMARY_PCTS = PCT_LIST\n",
    "\n",
    "SAVE_PRED_PERFOLD: bool = True\n",
    "LOG_EVERY: int          = 16\n",
    "MAX_FOLDS: int          = 0  # 0 ⇒ all 96\n",
    "\n",
    "# Which student regressors to run\n",
    "STUDENTS: List[str] = [\"chain_ERCcv_lr\", \"local_lasso\", \"local_rf\", \"global_rf\", \"chain_ERCcv_rf\"]\n",
    "\n",
    "TARGET_COLS = [f\"domain{i}\" for i in range(1, 15)]\n",
    "_DOM_PREFIX = \"rrmse_domain\"\n",
    "BOOT_B: int = 5000\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Device, cores, seeding\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "DETERMINISTIC = True\n",
    "N_JOBS: int = min(os.cpu_count() or 6, 6)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\",        str(N_JOBS))\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\",   str(N_JOBS))\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\",        str(N_JOBS))\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\",    str(N_JOBS))\n",
    "try:\n",
    "    torch.set_num_threads(N_JOBS)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "DEVICE_STR = device.type\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import random\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        torch.manual_seed(SEED)\n",
    "    if DETERMINISTIC:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Embedding registry and selection\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "EMBEDDING_SPECS: Dict[str, str] = {\n",
    "    \"e5_base\":          \"embaas/sentence-transformers-multilingual-e5-base\",\n",
    "    #\"e5_large\":         \"embaas/sentence-transformers-multilingual-e5-large\",\n",
    "    #\"simcse_xlmr_base\": \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "    #\"sbert_bert\":       \"jegormeister/bert-base-dutch-cased-snli\",\n",
    "}\n",
    "\n",
    "STUDENT_EMBEDDINGS: List[str] = [\"e5_base\"]\n",
    "def _normalize_embeddings(x):\n",
    "    if isinstance(x, str):\n",
    "        return [s.strip() for s in x.split(\",\") if s.strip()]\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [str(s).strip() for s in x if str(s).strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Normalize & dedupe\n",
    "_seen = set()\n",
    "STUDENT_EMBEDDINGS = [e for e in STUDENT_EMBEDDINGS if not (e in _seen or _seen.add(e))]\n",
    "# After deduping STUDENT_EMBEDDINGS\n",
    "STUDENT_EMB = STUDENT_EMBEDDINGS[0]\n",
    "\n",
    "# Label source (teacher that produced the labels for synthetics)\n",
    "LABEL_EMB: str   = \"e5_base\"\n",
    "LABEL_MODEL: str = \"chain_ERCcv_lr\"\n",
    "if LABEL_EMB not in EMBEDDING_SPECS:\n",
    "    raise ValueError(f\"Unknown LABEL_SOURCE_EMBEDDING='{LABEL_EMB}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "\n",
    "MAX_SEQ_LEN: int = 512\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Per-embedding tables/plots\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def _auto_flag(val: str, default_auto: bool) -> bool:\n",
    "    v = (val or \"\").strip().lower()\n",
    "    if v in {\"1\",\"true\",\"yes\",\"y\"}:  return True\n",
    "    if v in {\"0\",\"false\",\"no\",\"n\"}:  return False\n",
    "    return default_auto\n",
    "\n",
    "MULTI_EMBED = len(STUDENT_EMBEDDINGS) > 1\n",
    "WRITE_PER_EMBED_TABLES: bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "WRITE_PER_EMBED_PLOTS:  bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "\n",
    "if not WRITE_PER_EMBED_TABLES:\n",
    "    log.info(\"[guard] Per-embedding tables disabled (combined report will handle tables).\")\n",
    "if not WRITE_PER_EMBED_PLOTS:\n",
    "    log.info(\"[guard] Per-embedding plots disabled (combined report will draw plots).\")\n",
    "\n",
    "def write_run_config():\n",
    "    \"\"\"Dump a lightweight snapshot of env + resolved settings for reproducibility.\"\"\"\n",
    "    try:\n",
    "        cfg = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"device\": DEVICE_STR,\n",
    "            \"seed\": SEED,\n",
    "            \"n_jobs\": N_JOBS,\n",
    "            \"student_embeddings\": STUDENT_EMBEDDINGS,\n",
    "            \"current_student_embedding\": STUDENT_EMB,\n",
    "            \"students\": STUDENTS,\n",
    "            \"methods_filter_env\": os.getenv(\"METHODS\", \"\").strip() or None,\n",
    "            \"pct_list\": PCT_LIST,\n",
    "            \"pcts_for_plots\": PCTS_FOR_PLOTS,\n",
    "            \"label_source_embedding\": LABEL_EMB,\n",
    "            \"label_source_model\": LABEL_MODEL,\n",
    "            \"max_folds\": MAX_FOLDS,\n",
    "            \"boot_B\": BOOT_B,\n",
    "            \"write_per_embed_tables\": WRITE_PER_EMBED_TABLES,\n",
    "            \"write_per_embed_plots\": WRITE_PER_EMBED_PLOTS,\n",
    "            \"skip_baseline_if_present\": os.getenv(\"SKIP_BASELINE_IF_PRESENT\", \"0\"),\n",
    "            \"do_refits\": os.getenv(\"DO_REFITS\", \"1\"),\n",
    "            \"paths\": {\n",
    "                \"root\": str(ROOT),\n",
    "                \"g1_dir\": str(G1_DIR),\n",
    "                \"g2_dir\": str(G2_DIR),\n",
    "                \"g3a_dir\": str(G3A_DIR),\n",
    "                \"results_dir\": str(RES_DIR),\n",
    "                \"tables_dir\": str(TABLES_DIR),\n",
    "                \"figs_dir\": str(FIGS_DIR),\n",
    "            },\n",
    "        }\n",
    "        (G3A_DIR / \"run_config.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "        log.info(\"run_config.json written.\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Could not write run_config.json: {e}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Sentence encoders\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "_ENCODERS: Dict[str, SentenceTransformer] = {}\n",
    "\n",
    "def get_encoder(emb_key: str) -> SentenceTransformer:\n",
    "    if emb_key in _ENCODERS:\n",
    "        return _ENCODERS[emb_key]\n",
    "    repo = EMBEDDING_SPECS[emb_key]\n",
    "    log.info(f\"Loading SentenceTransformer [{emb_key}]: {repo} → device={DEVICE_STR}\")\n",
    "    m = SentenceTransformer(repo, device=DEVICE_STR)\n",
    "    m.max_seq_length = MAX_SEQ_LEN\n",
    "    _ENCODERS[emb_key] = m\n",
    "    return m\n",
    "\n",
    "def encode_texts(emb_key: str, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    mdl = get_encoder(emb_key)\n",
    "    X = mdl.encode(texts, batch_size=batch_size, convert_to_numpy=True,\n",
    "                   show_progress_bar=False, normalize_embeddings=False)\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Student models\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "class RegressorChainCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Cross-validated out-of-fold chaining with randomized target order support.\"\"\"\n",
    "    def __init__(self, base_estimator, order=None, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.order = order\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.chain_models_ = []\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples, self.n_targets_ = Y.shape\n",
    "        if self.order is None:\n",
    "            self.order = np.arange(self.n_targets_)\n",
    "        kf = KFold(n_splits=self.cv_splits, shuffle=True, random_state=rng)\n",
    "        X_chain = np.ascontiguousarray(X, dtype=np.float32)\n",
    "\n",
    "        oof_cols = []\n",
    "        for target_idx in self.order:\n",
    "            y = Y[:, target_idx]\n",
    "            oof = np.zeros(n_samples, dtype=np.float32)\n",
    "            for tr, va in kf.split(X_chain):\n",
    "                est = clone(self.base_estimator)\n",
    "                est.fit(X_chain[tr], y[tr])\n",
    "                oof[va] = est.predict(X_chain[va])\n",
    "            oof_cols.append(oof.reshape(-1, 1))\n",
    "            X_chain = np.hstack([X_chain, oof.reshape(-1,1)])\n",
    "\n",
    "        self.chain_models_ = []\n",
    "        acc = []\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            if i == 0:\n",
    "                X_full = X\n",
    "            else:\n",
    "                acc.append(oof_cols[i-1])\n",
    "                X_full = np.hstack([X, np.hstack(acc)])\n",
    "            est = clone(self.base_estimator)\n",
    "            est.fit(X_full, Y[:, target_idx])\n",
    "            self.chain_models_.append(est)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ext = np.ascontiguousarray(X, dtype=np.float32)\n",
    "        n = X.shape[0]\n",
    "        preds = np.zeros((n, self.n_targets_), dtype=np.float32)\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            yhat = self.chain_models_[i].predict(X_ext).reshape(-1, 1)\n",
    "            preds[:, target_idx] = yhat[:, 0]\n",
    "            X_ext = np.hstack([X_ext, yhat])\n",
    "        return preds\n",
    "\n",
    "class EnsembleRegressorChainsCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Ensemble over random target orders; average predictions.\"\"\"\n",
    "    def __init__(self, base_estimator, n_chains=5, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_chains = n_chains\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.ensemble_ = None\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self.n_targets_ = Y.shape[1]\n",
    "        self.ensemble_ = []\n",
    "        for _ in range(self.n_chains):\n",
    "            order = np.arange(self.n_targets_)\n",
    "            rng.shuffle(order)\n",
    "            chain = RegressorChainCV(\n",
    "                base_estimator=self.base_estimator,\n",
    "                order=order,\n",
    "                cv_splits=self.cv_splits,\n",
    "                random_state=rng.randint(0, 1_000_000),\n",
    "            )\n",
    "            chain.fit(X, Y)\n",
    "            self.ensemble_.append((order, chain))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [chain.predict(X) for (_, chain) in self.ensemble_]\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "def build_with_hp(student_key: str, hp: Dict[str, float | int]) -> Pipeline:\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(\n",
    "                base_estimator=LinearRegression(),\n",
    "                n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                random_state=SEED\n",
    "            )),\n",
    "        ])\n",
    "    elif student_key == \"local_lasso\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", MultiOutputRegressor(Lasso(alpha=float(hp[\"reg__estimator__alpha\"]),\n",
    "                                               random_state=SEED, max_iter=10_000))),\n",
    "        ])\n",
    "    elif student_key == \"local_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", MultiOutputRegressor(RandomForestRegressor(\n",
    "                n_estimators=int(hp[\"reg__estimator__n_estimators\"]),\n",
    "                max_depth=None if (hp.get(\"reg__estimator__max_depth\", None) in [None, \"None\"]) else int(hp[\"reg__estimator__max_depth\"]),\n",
    "                min_samples_leaf=int(hp.get(\"reg__estimator__min_samples_leaf\", 1)),\n",
    "                random_state=SEED, n_jobs=1\n",
    "            ))),\n",
    "        ])\n",
    "    elif student_key == \"global_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", RandomForestRegressor(\n",
    "                n_estimators=int(hp[\"reg__n_estimators\"]),\n",
    "                max_depth=None if (hp.get(\"reg__max_depth\", None) in [None, \"None\"]) else int(hp[\"reg__max_depth\"]),\n",
    "                min_samples_leaf=int(hp.get(\"reg__min_samples_leaf\", 1)),\n",
    "                random_state=SEED, n_jobs=1\n",
    "            )),\n",
    "        ])\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        base_rf = RandomForestRegressor(\n",
    "            n_estimators=int(hp.get(\"chain__base_rf__n_estimators\", 100)),\n",
    "            max_depth=None if (hp.get(\"chain__base_rf__max_depth\", None) in [None, \"None\"]) else int(hp[\"chain__base_rf__max_depth\"]),\n",
    "            min_samples_leaf=int(hp.get(\"chain__base_rf__min_samples_leaf\", 1)),\n",
    "            random_state=SEED, n_jobs=1\n",
    "        )\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(\n",
    "                base_estimator=base_rf,\n",
    "                n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                random_state=SEED\n",
    "            )),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown student_key: {student_key}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Load per-fold teacher pickle & extract HPs\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def teacher_pickle_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    # output/g_synth_augmented/g_2a_teacher_labeling_loocv_tuned/teacher/teacher_fold{ii}_{emb}__{student}.pkl\n",
    "    return ROOT / \"models\" / \"teacher\" / f\"teacher_fold{fold_idx:02d}_{emb_key}__{student_key}.pkl\"\n",
    "\n",
    "def _hp_json_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    return G2_DIR / \"teacher\" / f\"hp_fold{fold_idx:02d}_{emb_key}__{student_key}.json\"\n",
    "\n",
    "def hp_from_teacher(est: Pipeline, student_key: str) -> Dict[str, float | int]:\n",
    "    hp: Dict[str, float | int] = {}\n",
    "\n",
    "    if \"pca\" in est.named_steps:\n",
    "        hp[\"pca__n_components\"] = float(getattr(est.named_steps[\"pca\"], \"n_components\", 0.8))\n",
    "\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "\n",
    "    elif student_key == \"local_lasso\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__alpha\"] = float(getattr(reg, \"alpha\", 0.01))\n",
    "\n",
    "    elif student_key == \"local_rf\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__estimator__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__estimator__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "\n",
    "    elif student_key == \"global_rf\":\n",
    "        reg = est.named_steps[\"reg\"]\n",
    "        hp[\"reg__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "\n",
    "        base_rf = getattr(ch, \"base_estimator\", None)\n",
    "        if base_rf is None and hasattr(ch, \"base_estimator_\"):\n",
    "            base_rf = ch.base_estimator_\n",
    "\n",
    "        if isinstance(base_rf, RandomForestRegressor):\n",
    "            hp[\"chain__base_rf__n_estimators\"] = int(getattr(base_rf, \"n_estimators\", 100))\n",
    "            hp[\"chain__base_rf__max_depth\"] = getattr(base_rf, \"max_depth\", None)\n",
    "            hp[\"chain__base_rf__min_samples_leaf\"] = int(getattr(base_rf, \"min_samples_leaf\", 1))\n",
    "    else:\n",
    "        raise ValueError(student_key)\n",
    "\n",
    "    return hp\n",
    "\n",
    "def load_teacher_fold_estimator(fold_idx: int, student_key: str, emb_key: str, retries: int = 3, sleep_s: float = 1.5) -> Optional[Pipeline]:\n",
    "    \"\"\"Load fitted teacher pipeline with a few retries for flaky network/cloud files.\"\"\"\n",
    "    p = teacher_pickle_path(fold_idx, student_key, emb_key)\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if not p.exists():\n",
    "            break\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            if \"timed out\" in str(e).lower() or isinstance(e, OSError):\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}) [attempt {attempt}/{retries}]: {e}\")\n",
    "                if attempt < retries:\n",
    "                    time.sleep(sleep_s * attempt)\n",
    "                    continue\n",
    "            else:\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}): {e}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def hp_from_teacher_or_json(fold_idx: int, student_key: str, emb_key: str, est: Optional[Pipeline]) -> Dict[str, float | int]:\n",
    "    \"\"\"\n",
    "    Extract HPs from teacher estimator if provided; else from the per-fold hp_*.json.\n",
    "    If neither is available, RAISE an error (no hardcoded defaults).\n",
    "    \"\"\"\n",
    "    # 1) Teacher pickle provided\n",
    "    if est is not None:\n",
    "        return hp_from_teacher(est, student_key)\n",
    "\n",
    "    # 2) JSON fallback\n",
    "    j = _hp_json_path(fold_idx, student_key, emb_key)\n",
    "    if j.exists():\n",
    "        try:\n",
    "            payload = json.loads(j.read_text(encoding=\"utf-8\"))\n",
    "            # Some older JSONs may store the whole payload; accept either {\"best_params\": {...}} or the flat dict\n",
    "            if isinstance(payload, dict) and \"best_params\" in payload and isinstance(payload[\"best_params\"], dict):\n",
    "                return payload[\"best_params\"]\n",
    "            if isinstance(payload, dict):\n",
    "                return payload  # assume it already is the HP dict\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"HP JSON unreadable: {j.name} | {e}\") from e\n",
    "\n",
    "    # 3) Hard stop\n",
    "    raise RuntimeError(\n",
    "        f\"Missing per-fold HPs for fold={fold_idx}, student={student_key}, emb={emb_key}. \"\n",
    "        f\"Expected either teacher pickle ({teacher_pickle_path(fold_idx, student_key, emb_key).name}) \"\n",
    "        f\"or HP JSON ({_hp_json_path(fold_idx, student_key, emb_key).name}).\"\n",
    "    )\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Data loading (seeds + synthetics)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_seeds() -> pd.DataFrame:\n",
    "    scores = pd.read_csv(DATA_DIR / \"activity_scores.csv\")\n",
    "    acts   = pd.read_csv(DATA_DIR / \"activities.csv\")\n",
    "    dm = scores.pivot(index=\"activity_id\", columns=\"domain_id\", values=\"score\").reset_index()\n",
    "    dm = dm.rename(columns=lambda x: f\"domain{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "    dm = dm.merge(acts[[\"activity_id\", \"question\"]], on=\"activity_id\", how=\"left\")\n",
    "    dm = dm.rename(columns={\"activity_id\":\"seed_id\", \"question\":\"text\"})\n",
    "    dm = dm.sort_values(\"seed_id\").reset_index(drop=True)\n",
    "    assert len(dm) == 96, f\"Expected 96 seeds, got {len(dm)}\"\n",
    "    return dm[[\"seed_id\",\"text\", *TARGET_COLS]]\n",
    "\n",
    "def ensure_seed_vectors(seeds_df: pd.DataFrame, emb_key: str) -> np.ndarray:\n",
    "    vec_path = ROOT / \"outputs\" / \"results\" / f\"{emb_key}_vectors.npy\"\n",
    "    meta_path = vec_path.with_suffix(\".meta.json\")\n",
    "\n",
    "    def _write_meta(X: np.ndarray):\n",
    "        try:\n",
    "            meta = {\n",
    "                \"embedding\": emb_key,\n",
    "                \"repo\": EMBEDDING_SPECS[emb_key],\n",
    "                \"max_seq_len\": int(MAX_SEQ_LEN),\n",
    "                \"n_rows\": int(X.shape[0]),\n",
    "                \"n_dim\": int(X.shape[1]),\n",
    "            }\n",
    "            meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Prefer reuse only if rows AND max_seq_len match\n",
    "    if vec_path.exists():\n",
    "        try:\n",
    "            X = np.load(vec_path)\n",
    "            ok_rows = (X.shape[0] == len(seeds_df))\n",
    "            ok_len = False\n",
    "            if meta_path.exists():\n",
    "                meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "                ok_len = int(meta.get(\"max_seq_len\", MAX_SEQ_LEN)) == int(MAX_SEQ_LEN)\n",
    "            # If no meta, force a one-time rebuild to stamp the file with the correct max_seq_len\n",
    "            if ok_rows and ok_len:\n",
    "                np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X)\n",
    "                return X\n",
    "            else:\n",
    "                log.warning(\"[%s] Rebuilding seed vectors (rows_ok=%s, max_seq_len_ok=%s).\",\n",
    "                            emb_key, ok_rows, ok_len)\n",
    "        except Exception as e:\n",
    "            log.warning(\"[%s] Existing vectors unusable (%s); rebuilding.\", emb_key, e)\n",
    "\n",
    "    log.info(\"Computing %s embeddings for seeds …\", emb_key)\n",
    "    X = encode_texts(emb_key, seeds_df[\"text\"].astype(str).tolist(), batch_size=64)\n",
    "    np.save(vec_path, X)\n",
    "    np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X)\n",
    "    _write_meta(X)\n",
    "    log.info(\"✔ Seed vectors saved → %s\", vec_path.relative_to(ROOT))\n",
    "    return X\n",
    "\n",
    "def base_from_method_and_M(method: str, M: int) -> str:\n",
    "    return f\"n{M}_{method}\"\n",
    "\n",
    "def g2_label_file(fold_idx: int, method: str, M: int, label_emb: str, label_model: str) -> Path:\n",
    "    base = base_from_method_and_M(method, M)\n",
    "    preferred = G2_DIR / f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__{label_model}.csv\"\n",
    "    if preferred.exists():\n",
    "        return preferred\n",
    "    # fallback: any model with the label embedding (keeps label source embedding fixed)\n",
    "    cand = list(G2_DIR.glob(f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__*.csv\"))\n",
    "    if cand:\n",
    "        return cand[0]\n",
    "    return preferred  # will raise later if missing\n",
    "\n",
    "def discover_methods_and_M_from_g2(label_emb: str) -> Dict[str, List[int]]:\n",
    "    pats = sorted(G2_DIR.glob(f\"g2f_labels_fold00_n*_*__{label_emb}__*.csv\"))\n",
    "    rows: Dict[str, set] = {}\n",
    "    for p in pats:\n",
    "        m = re.match(rf\"g2f_labels_fold00_n(\\d+)_([A-Za-z0-9_]+)__{label_emb}__.*\\.csv$\", p.name)\n",
    "        if not m: \n",
    "            continue\n",
    "        M = int(m.group(1)); method = m.group(2)\n",
    "        rows.setdefault(method, set()).add(M)\n",
    "    return {k: sorted(v) for k, v in rows.items()}\n",
    "\n",
    "def synth_cache_paths_from_g2c(g1_path: Path, emb_key: str) -> Tuple[Path, Path]:\n",
    "    base = g1_path.stem\n",
    "    m = re.match(r\"g_final_(n\\d+_[A-Za-z0-9_]+)$\", base)\n",
    "    if m:\n",
    "        base = m.group(1)\n",
    "    cache_dir = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "    candidates = [\n",
    "        (cache_dir / f\"g_final_{base}__{emb_key}.npy\",  cache_dir / f\"g_final_{base}__index.csv\"),\n",
    "        (cache_dir / f\"{base}__{emb_key}.npy\",          cache_dir / f\"{base}__index.csv\"),\n",
    "    ]\n",
    "    for npy, idx in candidates:\n",
    "        if npy.exists() and idx.exists():\n",
    "            return npy, idx\n",
    "    nearby = sorted(p.name for p in cache_dir.glob(f\"*{base}*\"))\n",
    "    tried  = \" ; \".join([npy.name for npy, _ in candidates] + [idx.name for _, idx in candidates])\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing synth cache for base='{base}' and embedding='{emb_key}'. Tried: {tried}. \"\n",
    "        f\"Found near-matches in cache: {nearby}\"\n",
    "    )\n",
    "\n",
    "def g1_source_csv(method: str, M: int) -> Path:\n",
    "    p = G1_DIR / f\"g_final_n{M}_{method}.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing Script-A source: {p}\")\n",
    "    return p\n",
    "\n",
    "def ensure_synth_cache(method: str, M: int, emb_key: str) -> Tuple[Path, Path]:\n",
    "    \"\"\"Ensure (npy, index.csv) exist for Script-A synthetics under the given embedding.\"\"\"\n",
    "    src_csv = g1_source_csv(method, M)\n",
    "    base = src_csv.stem\n",
    "    m = re.match(r\"g_final_(n\\d+_[A-Za-z0-9_]+)$\", base)\n",
    "    base = m.group(1) if m else base\n",
    "\n",
    "    cache_dir = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "    npy  = cache_dir / f\"g_final_{base}__{emb_key}.npy\"\n",
    "    idx  = cache_dir / f\"g_final_{base}__index.csv\"\n",
    "    if npy.exists() and idx.exists():\n",
    "        return npy, idx\n",
    "\n",
    "    df = pd.read_csv(src_csv)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(f\"{src_csv.name}: missing 'text' column\")\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "    log.info(f\"[cache] building synth embeds for method={method}, M={M}, emb={emb_key} …\")\n",
    "    Xs = encode_texts(emb_key, texts, batch_size=64).astype(np.float32, copy=False)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(npy, Xs)\n",
    "    pd.DataFrame({\"text\": texts}).to_csv(idx, index=False)\n",
    "    log.info(f\"[cache] ✔ saved → {npy.relative_to(ROOT)} ; {idx.relative_to(ROOT)}\")\n",
    "    return npy, idx\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Metrics \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def rmse(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(np.mean((a - b) ** 2, axis=0))\n",
    "\n",
    "def rrmse_vs_dummy(y_true: np.ndarray, y_pred: np.ndarray, y_dummy: np.ndarray) -> np.ndarray:\n",
    "    rm = rmse(y_true, y_pred)\n",
    "    rd = rmse(y_true, y_dummy)\n",
    "    return rm / np.maximum(rd, 1e-12)\n",
    "\n",
    "def _fmt_dt(sec: float) -> str:\n",
    "    m, s = divmod(sec, 60.0)\n",
    "    h, m = divmod(m, 60.0)\n",
    "    if h >= 1: return f\"{int(h)}h{int(m):02d}m{s:04.1f}s\"\n",
    "    if m >= 1: return f\"{int(m)}m{s:04.1f}s\"\n",
    "    return f\"{s:0.2f}s\"\n",
    "\n",
    "def paired_cliffs_delta(full: np.ndarray, base: np.ndarray):\n",
    "    diff = base - full  # positive when full improves (lower is better)\n",
    "    n_pos = int(np.sum(diff > 0))\n",
    "    n_neg = int(np.sum(diff < 0))\n",
    "    n_zero = int(np.sum(diff == 0))\n",
    "    denom = max(1, (n_pos + n_neg))\n",
    "    delta = (n_pos - n_neg) / denom\n",
    "    mag = (\"negligible\" if abs(delta) < 0.147 else\n",
    "           \"small\"       if abs(delta) < 0.33  else\n",
    "           \"medium\"      if abs(delta) < 0.474 else\n",
    "           \"large\")\n",
    "    return float(delta), n_pos, n_neg, n_zero, mag\n",
    "\n",
    "def pct_to_K(pct: int, n_seeds: int = 96) -> int:\n",
    "    return max(1, int(round((pct / 100.0) * n_seeds)))\n",
    "\n",
    "def _dom_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return [c for c in df.columns if c.startswith(_DOM_PREFIX)]\n",
    "\n",
    "def _global_median_from_file(p: Path) -> float:\n",
    "    df = pd.read_csv(p)\n",
    "    cols = _dom_cols(df)\n",
    "    if not cols:\n",
    "        raise RuntimeError(f\"{p.name}: no '{_DOM_PREFIX}*' columns present.\")\n",
    "    return float(np.median(df[cols].to_numpy(dtype=np.float32).ravel()))\n",
    "\n",
    "def _flat_arrays(p_base: Path, p_full: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    db = pd.read_csv(p_base)\n",
    "    df = pd.read_csv(p_full)\n",
    "    cols = _dom_cols(db)\n",
    "    if not cols or not set(cols).issubset(df.columns):\n",
    "        missing = sorted(set(cols) - set(df.columns))\n",
    "        raise RuntimeError(f\"{p_full.name}: missing '{_DOM_PREFIX}*' columns: {missing}\")\n",
    "    xb = db[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    xf = df[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    return xb, xf\n",
    "\n",
    "def section(title):\n",
    "    \"\"\"Print section header\"\"\"\n",
    "    bar = \"═\" * len(title)\n",
    "    print(f\"\\n{bar}\\n{title}\\n{bar}\")\n",
    "\n",
    "def _save_and_show(fig, path: str):\n",
    "    \"\"\"Save and display figure\"\"\"\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Plot saved → {path}\")\n",
    "\n",
    "def aligned_ranks(mat):\n",
    "    \"\"\"Hodges–Lehmann alignment + ranking along rows (lower is better)\"\"\"\n",
    "    aligned = mat - np.median(mat, axis=1, keepdims=True)\n",
    "    return np.apply_along_axis(lambda r: np.argsort(np.argsort(r)) + 1, 1, aligned)\n",
    "\n",
    "def friedman_aligned(mat):\n",
    "    \"\"\"Aligned-Friedman χ² and Iman–Davenport F-statistic (expects ranks or aligned data)\"\"\"\n",
    "    k = mat.shape[1]\n",
    "    chi2, _ = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff = ((mat.shape[0] - 1) * chi2) / (mat.shape[0] * (k - 1) - chi2)\n",
    "    return chi2, Ff\n",
    "\n",
    "def wilcoxon_matrix(mat, labels):\n",
    "    \"\"\"Pairwise two-sided Wilcoxon (zero-method = zsplit)\"\"\"\n",
    "    df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for i, j in combinations(range(len(labels)), 2):\n",
    "        diff = mat[:, i] - mat[:, j]\n",
    "        p = 1.0 if np.allclose(diff, 0) else wilcoxon(diff, zero_method=\"zsplit\")[1]\n",
    "        df.iat[i, j] = df.iat[j, i] = p\n",
    "    return df.round(4)\n",
    "\n",
    "def holm_correct_and_effects(raw_p, data, labels):\n",
    "    \"\"\"Holm–Bonferroni correction and Cliff's Δ effect sizes\"\"\"\n",
    "    idx = list(combinations(range(len(labels)), 2))\n",
    "    pvals = [raw_p.iat[i, j] for i, j in idx]\n",
    "    _, p_adj, _, _ = multipletests(pvals, method=\"holm\")\n",
    "\n",
    "    adj_df = raw_p.copy()\n",
    "    for (i, j), p in zip(idx, p_adj):\n",
    "        adj_df.iat[i, j] = adj_df.iat[j, i] = p\n",
    "    adj_df[np.eye(len(labels), dtype=bool)] = 1.0\n",
    "\n",
    "    def cliffs_delta(x, y):\n",
    "        diffs = np.subtract.outer(x, y)\n",
    "        n = len(x) * len(y)\n",
    "        return (np.sum(diffs > 0) - np.sum(diffs < 0)) / n\n",
    "\n",
    "    delta_df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for (i, j) in idx:\n",
    "        d_ij = cliffs_delta(data[:, i], data[:, j])\n",
    "        delta_df.iat[i, j] = d_ij\n",
    "        delta_df.iat[j, i] = -d_ij\n",
    "\n",
    "    return adj_df.round(4), delta_df.round(3)\n",
    "\n",
    "def conover_posthoc(ranks, labels, fname_tag):\n",
    "    \"\"\"Conover–Iman test with Holm correction\"\"\"\n",
    "    p_df = sp.posthoc_conover_friedman(ranks, p_adjust=\"holm\")\n",
    "    p_df.index = p_df.columns = labels\n",
    "    out = TABLES_DIR / f\"{fname_tag}_conover_p.csv\"\n",
    "    p_df.to_csv(out)\n",
    "    print(\"\\nConover–Iman post-hoc p-values (Holm-adjusted):\")\n",
    "    print(p_df.round(4).to_string())\n",
    "    print(\"  ↳ saved →\", out)\n",
    "    return p_df\n",
    "\n",
    "def run_friedman(mat, block_name, col_labels, fname_tag):\n",
    "    \"\"\"Generic routine for Friedman analysis with post-hoc tests\"\"\"\n",
    "    k = len(col_labels)\n",
    "    nblocks = mat.shape[0]\n",
    "\n",
    "    # Save & print medians (PRINT SORTED low→high; CSV keeps original order)\n",
    "    col_meds = pd.Series(np.median(mat, axis=0), index=col_labels)\n",
    "    med_path = TABLES_DIR / f\"{fname_tag}_median.csv\"\n",
    "    col_meds.to_csv(med_path, header=[\"median_rrmse\"])\n",
    "    print(f\"\\nMedian RRMSE per {block_name[:-1] if block_name.endswith('s') else block_name} (sorted low→high):\")\n",
    "    print(col_meds.sort_values().round(3).to_string())\n",
    "    print(\"  ↳ saved →\", med_path)\n",
    "\n",
    "    if nblocks == 2:\n",
    "        print(f\"\\nOnly two {block_name} → skipping Friedman/post-hoc.\")\n",
    "        wilc = wilcoxon_matrix(mat, col_labels)\n",
    "        print(\"\\nWilcoxon pairwise p-values:\")\n",
    "        print(wilc.round(4).to_string())\n",
    "        wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "        wilc.to_csv(wilc_path)\n",
    "        print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "        adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "        print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "        print(adj.round(4).to_string())\n",
    "        adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "        adj.to_csv(adj_path)\n",
    "        print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "        print(\"\\nCliff's Δ effect sizes:\")\n",
    "        print(delta.round(3).to_string())\n",
    "        delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "        delta.to_csv(delta_path)\n",
    "        print(\"  ↳ saved →\", delta_path)\n",
    "        return\n",
    "\n",
    "    if k == 2:\n",
    "        p = wilcoxon(mat[:, 0], mat[:, 1], zero_method=\"zsplit\")[1]\n",
    "        print(f\"\\nPaired Wilcoxon ({col_labels[0]} vs {col_labels[1]}): p = {p:.5g}\")\n",
    "        return\n",
    "\n",
    "    # Friedman statistics\n",
    "    ranks = aligned_ranks(mat)\n",
    "    chi2_a, Ff_a = friedman_aligned(ranks)\n",
    "    chi2_o, p_o = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff_o = ((nblocks - 1) * chi2_o) / (nblocks * (k - 1) - chi2_o)\n",
    "\n",
    "    print(f\"\\n*Aligned-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_a:.3f}    F_F = {Ff_a:.3f}\")\n",
    "    print(f\"\\n*Original-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_o:.3f}    p = {p_o:.3g}    F_F = {Ff_o:.3f}\")\n",
    "\n",
    "    # Post-hoc tests\n",
    "    if nblocks < 10:\n",
    "        conover_posthoc(ranks, col_labels, fname_tag)\n",
    "    else:\n",
    "        pvals_nem = sp.posthoc_nemenyi_friedman(ranks)\n",
    "        pvals_nem.index = pvals_nem.columns = col_labels\n",
    "        nem_path = TABLES_DIR / f\"{fname_tag}_nemenyi_p.csv\"\n",
    "        pvals_nem.to_csv(nem_path)\n",
    "        print(\"\\nNemenyi p-values (aligned post-hoc):\")\n",
    "        print(pvals_nem.round(4).to_string())\n",
    "        print(\"  ↳ saved →\", nem_path)\n",
    "\n",
    "    # 1) Pair-wise Wilcoxon\n",
    "    wilc = wilcoxon_matrix(mat, col_labels)\n",
    "    print(\"\\nWilcoxon pairwise p-values:\")\n",
    "    print(wilc.round(4).to_string())\n",
    "    wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "    wilc.to_csv(wilc_path)\n",
    "    print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "    # 2) Holm–Bonferroni adjustment + Cliff’s Δ\n",
    "    adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "\n",
    "    print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "    print(adj.round(4).to_string())\n",
    "    adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "    adj.to_csv(adj_path)\n",
    "    print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "    print(\"\\nCliff's Δ effect sizes:\")\n",
    "    print(delta.round(3).to_string())\n",
    "    delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "    delta.to_csv(delta_path)\n",
    "    print(\"  ↳ saved →\", delta_path)\n",
    "\n",
    "def vector_per_target(rrmse_array):\n",
    "    \"\"\"Collapse (folds × targets) → (targets,) by median across folds\"\"\"\n",
    "    return np.median(rrmse_array, axis=0) if getattr(rrmse_array, \"ndim\", 1) == 2 else rrmse_array\n",
    "\n",
    "def matrix_per_target_compare_models(data_dict, model_list, embedding_list):\n",
    "    \"\"\"Build matrix: rows = targets, cols = models, aggregated across embeddings\"\"\"\n",
    "    return np.column_stack([\n",
    "        np.median(\n",
    "            np.concatenate([\n",
    "                vector_per_target(data_dict[emb][model])\n",
    "                for emb in embedding_list if emb in data_dict\n",
    "            ], axis=0).reshape(-1, N_TARGETS),\n",
    "            axis=0\n",
    "        )\n",
    "        for model in model_list\n",
    "    ])\n",
    "\n",
    "# --- PATCH: show full statistics right under each CD diagram -----------------\n",
    "\n",
    "def cd_plot(matrix, labels, title, fname):\n",
    "    \"\"\"Critical-distance diagram with robust p-value alignment to labels.\"\"\"\n",
    "    if matrix.shape[1] < 2:\n",
    "        print(f\"⚠  Skipping CD-plot '{title}' (need ≥2 methods, got {matrix.shape[1]})\")\n",
    "        return\n",
    "\n",
    "    ranks = aligned_ranks(matrix)\n",
    "\n",
    "    # Compute post-hoc p-values and FORCE index/columns to match `labels`\n",
    "    pvals_raw = sp.posthoc_nemenyi_friedman(ranks)\n",
    "    if not isinstance(pvals_raw, pd.DataFrame):\n",
    "        pvals = pd.DataFrame(pvals_raw, index=range(len(labels)), columns=range(len(labels)))\n",
    "    else:\n",
    "        pvals = pvals_raw.copy()\n",
    "\n",
    "    # Defensive shape fix (trim/pad unlikely; trim covers rare inconsistencies)\n",
    "    if pvals.shape != (len(labels), len(labels)):\n",
    "        pvals = pvals.iloc[:len(labels), :len(labels)]\n",
    "        if pvals.shape != (len(labels), len(labels)):\n",
    "            # Last resort: identity p-values (no significant lines)\n",
    "            pvals = pd.DataFrame(np.ones((len(labels), len(labels))), index=range(len(labels)), columns=range(len(labels)))\n",
    "\n",
    "    # Align names to your model labels, sanitize & symmetrize\n",
    "    pvals.index = labels\n",
    "    pvals.columns = labels\n",
    "    pvals = pvals.astype(float).fillna(1.0)\n",
    "    pvals = pd.DataFrame(np.minimum(pvals.values, pvals.values.T), index=labels, columns=labels)\n",
    "    np.fill_diagonal(pvals.values, 1.0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks.mean(0), index=labels), pvals, ax=ax)\n",
    "    ax.set_title(title, pad=10)\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    # full report printed under the plot\n",
    "    tag = Path(fname).stem\n",
    "    run_friedman(matrix, block_name=\"folds\", col_labels=labels, fname_tag=tag)\n",
    "\n",
    "\n",
    "def cd_plot_dual(matrix1, labels1, matrix2, labels2, title1, title2, fname):\n",
    "    \"\"\"Two CD-diagrams side-by-side with robust p-value alignment.\"\"\"\n",
    "    if matrix1.shape[1] < 2 or matrix2.shape[1] < 2:\n",
    "        print(\"⚠  Skipping dual CD-plot (need ≥2 methods for both)\")\n",
    "        return\n",
    "\n",
    "    def _aligned_pvals(M, lbls):\n",
    "        r = aligned_ranks(M)\n",
    "        raw = sp.posthoc_nemenyi_friedman(r)\n",
    "        if not isinstance(raw, pd.DataFrame):\n",
    "            P = pd.DataFrame(raw, index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        else:\n",
    "            P = raw.copy()\n",
    "        if P.shape != (len(lbls), len(lbls)):\n",
    "            P = P.iloc[:len(lbls), :len(lbls)]\n",
    "            if P.shape != (len(lbls), len(lbls)):\n",
    "                P = pd.DataFrame(np.ones((len(lbls), len(lbls))), index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        P.index = lbls\n",
    "        P.columns = lbls\n",
    "        P = P.astype(float).fillna(1.0)\n",
    "        P = pd.DataFrame(np.minimum(P.values, P.values.T), index=lbls, columns=lbls)\n",
    "        np.fill_diagonal(P.values, 1.0)\n",
    "        return r, P\n",
    "\n",
    "    ranks1, pvals1 = _aligned_pvals(matrix1, labels1)\n",
    "    ranks2, pvals2 = _aligned_pvals(matrix2, labels2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks1.mean(0), index=labels1), pvals1, ax=ax1)\n",
    "    ax1.set_title(title1, pad=10)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks2.mean(0), index=labels2), pvals2, ax=ax2)\n",
    "    ax2.set_title(title2, pad=10)\n",
    "    plt.tight_layout()\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    base_tag = Path(fname).stem\n",
    "    section(f\"Full statistics — LEFT panel: {title1}\")\n",
    "    run_friedman(matrix1, block_name=\"folds\", col_labels=labels1, fname_tag=f\"{base_tag}__left\")\n",
    "\n",
    "    section(f\"Full statistics — RIGHT panel: {title2}\")\n",
    "    run_friedman(matrix2, block_name=\"folds\", col_labels=labels2, fname_tag=f\"{base_tag}__right\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Reuse for chain_ERCcv_lr — only for e5_base student == label source\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def chainlr_pct_full_path(student_emb: str, method: str, pct: int, K: int, Mmax: int) -> Path:\n",
    "    return RES_DIR / f\"rrmse_perfold_{student_emb}__chain_ERCcv_lr__{method}__pct{pct}_K{K}__Mmax{Mmax}__full.csv\"\n",
    "\n",
    "def ensure_chainlr_from_legacy_pct(student_emb: str, method: str, pct: int, K: int, N_SEEDS: int, Mmax: int) -> bool:\n",
    "    if not (student_emb == \"e5_base\" and LABEL_EMB == \"e5_base\"):\n",
    "        return False\n",
    "    mapping = {100: (\"n96_sps1\", 96), 200: (\"n192_sps2\", 192), 400: (\"n384_sps4\", 384)}\n",
    "    if pct not in mapping:\n",
    "        return False\n",
    "    tag, _ = mapping[pct]\n",
    "    legacy_root = RES_DIR\n",
    "    legacy_full = legacy_root / f\"rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__{tag}__full.csv\"\n",
    "    if not legacy_full.exists():\n",
    "        log.info(f\"[reuse/chain_lr] Missing legacy {legacy_full.name} → recompute instead.\")\n",
    "        return False\n",
    "\n",
    "    new_full = chainlr_pct_full_path(student_emb, method, pct, K, Mmax)\n",
    "    try:\n",
    "        if new_full.exists():\n",
    "            n_rows = sum(1 for _ in open(new_full, \"r\", encoding=\"utf-8\")) - 1\n",
    "            if n_rows >= N_SEEDS:\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df_old = pd.read_csv(legacy_full).iloc[:N_SEEDS].copy()\n",
    "    keep_cols = [c for c in df_old.columns if c.startswith(\"rrmse_domain\")]\n",
    "    if \"median_rrmse_fold\" in df_old.columns:\n",
    "        keep_cols = [\"median_rrmse_fold\"] + keep_cols\n",
    "\n",
    "    fold_col = df_old[\"fold\"].astype(int).values if \"fold\" in df_old.columns else np.arange(N_SEEDS, dtype=int)\n",
    "    df_new = pd.DataFrame({\n",
    "        \"fold\": fold_col,\n",
    "        \"method\": method,\n",
    "        \"pct\": pct,\n",
    "        \"K\": K,\n",
    "        \"M_max\": Mmax,\n",
    "        \"student\": \"S1\",\n",
    "        \"embedding\": student_emb,\n",
    "        \"regressor\": \"chain_ERCcv_lr\",\n",
    "        \"variant\": \"full\",\n",
    "    })\n",
    "    for c in keep_cols:\n",
    "        df_new[c] = df_old[c].values\n",
    "\n",
    "    new_full.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new.to_csv(new_full, index=False)\n",
    "    log.info(f\"[reuse/chain_lr] Wrote {new_full.name} from legacy {legacy_full.name}\")\n",
    "    return True\n",
    "\n",
    "def run_for_embedding(emb_key: str):\n",
    "    \"\"\"Run the whole pipeline for a single student embedding key.\"\"\"\n",
    "    global STUDENT_EMB\n",
    "    if emb_key not in EMBEDDING_SPECS:\n",
    "        raise ValueError(f\"Unknown embedding '{emb_key}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "    prev = STUDENT_EMB\n",
    "    try:\n",
    "        STUDENT_EMB = emb_key\n",
    "        log.info(f\"=== BEGIN run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "        run()\n",
    "        log.info(f\"=== END   run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "    finally:\n",
    "        STUDENT_EMB = prev\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Combined reports \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _matrix_for_method_pct(emb: str, method: str, pct: int, Mmax: int, regressors: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build mat of shape (n_folds, n_regressors) using FULL variant,\n",
    "    each cell = median over domain-wise RRMSE for that fold.\n",
    "    Only keeps folds present for ALL regressors (inner join on 'fold').\n",
    "    \"\"\"\n",
    "    K = pct_to_K(pct, 96)\n",
    "    fold_sets = []\n",
    "    per_reg = {}\n",
    "\n",
    "    for reg in regressors:\n",
    "        p_full = RES_DIR / f\"rrmse_perfold_{emb}__{reg}__{method}__pct{pct}_K{K}__Mmax{Mmax}__full.csv\"\n",
    "        if not p_full.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(p_full)\n",
    "        if \"median_rrmse_fold\" in df.columns:\n",
    "            z = df[[\"fold\", \"median_rrmse_fold\"]].rename(columns={\"median_rrmse_fold\":\"med\"})\n",
    "        else:\n",
    "            cols = [c for c in df.columns if c.startswith(\"rrmse_domain\")]\n",
    "            if not cols:\n",
    "                continue\n",
    "            z = df[[\"fold\", *cols]].copy()\n",
    "            z[\"med\"] = z[cols].median(axis=1)\n",
    "            z = z[[\"fold\", \"med\"]]\n",
    "        per_reg[reg] = z\n",
    "        fold_sets.append(set(z[\"fold\"].astype(int).tolist()))\n",
    "\n",
    "    regs = sorted(per_reg.keys())\n",
    "    if len(regs) < 2:\n",
    "        return np.empty((0, 0)), []\n",
    "\n",
    "    common_folds = sorted(set.intersection(*fold_sets)) if fold_sets else []\n",
    "    if len(common_folds) < 2:\n",
    "        return np.empty((0, 0)), []\n",
    "\n",
    "    mats = []\n",
    "    for reg in regs:\n",
    "        z = per_reg[reg]\n",
    "        z = z[z[\"fold\"].isin(common_folds)].sort_values(\"fold\")\n",
    "        mats.append(z[\"med\"].to_numpy(dtype=np.float32))\n",
    "\n",
    "    mat = np.vstack(mats).T  # rows = folds, cols = regs\n",
    "    return mat, regs\n",
    "\n",
    "def say(msg: str):\n",
    "    \"\"\"Print once to screen and write once to run.log (no console duplicates).\"\"\"\n",
    "    stream = getattr(sys, \"__stdout__\", sys.stdout)\n",
    "    print(msg, file=stream, flush=True)  \n",
    "    _say_logger.info(msg)          \n",
    "\n",
    "def build_combined_reports_across_embeddings():\n",
    "    \"\"\"\n",
    "    Aggregate results already computed on disk into a single set of\n",
    "    tables/plots using the GLOBAL MEDIAN RRMSE (median over all folds×domains).\n",
    "    No PRIMARY/Appendix split — one unified set of CSVs for all %K in PCT_LIST.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Discover methods with optional filter\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    methods = sorted(avail.keys()) if (METHODS is None) else [m for m in sorted(avail.keys()) if m in METHODS]\n",
    "\n",
    "    # Pattern for per-fold CSVs\n",
    "    pat = re.compile(\n",
    "        r\"^rrmse_perfold_(?P<emb>.+?)__(?P<reg>.+?)__(?P<meth>.+?)__pct(?P<pct>\\d+)_K(?P<K>\\d+)__Mmax(?P<M>\\d+)__(?P<var>baseline|full)\\.csv$\"\n",
    "    )\n",
    "\n",
    "    summary_rows, wil_rows, cliffs_rows = [], [], []\n",
    "    pooled_pairs: Dict[Tuple[str,int], Dict[str, List[float]]] = {}\n",
    "\n",
    "    # Scan all embeddings/regressors/methods/%K present on disk\n",
    "    for emb in STUDENT_EMBEDDINGS:\n",
    "        full_files = sorted([p for p in RES_DIR.glob(f\"rrmse_perfold_{emb}__*__full.csv\") if pat.match(p.name)])\n",
    "        for f_full in full_files:\n",
    "            m = pat.match(f_full.name)\n",
    "            reg = m[\"reg\"]; meth = m[\"meth\"]\n",
    "            pct = int(m[\"pct\"]); K = int(m[\"K\"]); Mmax = int(m[\"M\"])\n",
    "            if pct not in PCT_LIST:\n",
    "                continue\n",
    "            f_base = RES_DIR / f\"rrmse_perfold_{emb}__{reg}__{meth}__pct{pct}_K{K}__Mmax{Mmax}__baseline.csv\"\n",
    "            if not f_base.exists():\n",
    "                continue\n",
    "\n",
    "            g_base = _global_median_from_file(f_base)\n",
    "            g_full = _global_median_from_file(f_full)\n",
    "            summary_rows.append({\n",
    "                \"student\": \"S1\", \"embedding\": emb, \"regressor\": reg, \"method\": meth,\n",
    "                \"pct\": pct, \"K\": K, \"M_max\": Mmax,\n",
    "                \"baseline\": g_base, \"full\": g_full\n",
    "            })\n",
    "\n",
    "            # Paired tests on flattened (fold×domain) arrays\n",
    "            xb, xf = _flat_arrays(f_base, f_full)\n",
    "            pooled_pairs.setdefault((meth, pct), {\"base\": [], \"full\": []})\n",
    "            pooled_pairs[(meth, pct)][\"base\"].extend(xb.tolist())\n",
    "            pooled_pairs[(meth, pct)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "            except Exception:\n",
    "                p_raw = 1.0\n",
    "            d, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "            wil_rows.append({\"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "                             \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"p_raw\":float(p_raw)})\n",
    "            cliffs_rows.append({\"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "                                \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"cliffs_delta_paired\":float(d),\n",
    "                                \"npos\":int(npos),\"nneg\":int(nneg),\"nzero\":int(nzero),\"magnitude\":mag})\n",
    "\n",
    "    # ── Unified wide summary (all pcts)\n",
    "    if not summary_rows:\n",
    "        say(\"[combined] No result pairs found to summarise.\")\n",
    "        return\n",
    "\n",
    "    wide = pd.DataFrame(summary_rows).sort_values([\"embedding\",\"regressor\",\"method\",\"pct\"]).reset_index(drop=True)\n",
    "\n",
    "    tol = 1e-9\n",
    "    def _cmp(row):\n",
    "        b, f = row[\"baseline\"], row[\"full\"]\n",
    "        if pd.isna(b) or pd.isna(f): return \"n/a\"\n",
    "        if (b - f) > tol:  return \"better\"\n",
    "        if (f - b) > tol:  return \"worse\"\n",
    "        return \"same\"\n",
    "    wide[\"baseline_vs_full\"] = wide.apply(_cmp, axis=1)\n",
    "\n",
    "    # Save unified tables (no appendix split) — ALWAYS write combined outputs\n",
    "    out_sum = TABLES_DIR / \"summary_median_rrmse.csv\"\n",
    "    wide.to_csv(out_sum, index=False)\n",
    "    say(\"[combined] summary_median_rrmse.csv written (unified, all pcts).\")\n",
    "\n",
    "    comp = wide.copy()\n",
    "    comp[\"delta\"] = comp[\"baseline\"] - comp[\"full\"]\n",
    "    comp[\"rel_change_pct\"] = 100.0 * comp[\"delta\"] / comp[\"baseline\"].replace(0, np.nan)\n",
    "    comp.to_csv(TABLES_DIR / \"summary_median_rrmse_with_delta.csv\", index=False)\n",
    "    say(\"[combined] summary_median_rrmse_with_delta.csv written (unified).\")\n",
    "\n",
    "    # Wilcoxon + Holm — single file for the combined set\n",
    "    if wil_rows:\n",
    "        df_w = pd.DataFrame(wil_rows)\n",
    "        holm_rows = []\n",
    "        for (embedding, regressor, method), grp in df_w.groupby([\"embedding\",\"regressor\",\"method\"]):\n",
    "            pvals = grp[\"p_raw\"].to_numpy()\n",
    "            _, p_holm, _, _ = multipletests(pvals, method=\"holm\")\n",
    "            for i, (_, row) in enumerate(grp.reset_index(drop=True).iterrows()):\n",
    "                holm_rows.append({\n",
    "                    \"student\":\"S1\",\"embedding\":embedding,\"regressor\":regressor,\"method\":method,\n",
    "                    \"pct\": int(row[\"pct\"]), \"K\": int(row[\"K\"]), \"M_max\": int(row[\"M_max\"]),\n",
    "                    \"p_raw\": float(row[\"p_raw\"]), \"p_holm\": float(p_holm[i]),\n",
    "                })\n",
    "        pd.DataFrame(holm_rows).sort_values([\"embedding\",\"regressor\",\"method\",\"pct\"]).to_csv(\n",
    "            TABLES_DIR / \"wilcoxon_holm_vs_baseline.csv\", index=False\n",
    "        )\n",
    "        say(\"[combined] wilcoxon_holm_vs_baseline.csv written.\")\n",
    "\n",
    "    # Cliff’s Δ — single file for the combined set\n",
    "    if cliffs_rows:\n",
    "        pd.DataFrame(cliffs_rows).sort_values([\"embedding\",\"regressor\",\"method\",\"pct\"]).to_csv(\n",
    "            TABLES_DIR / \"cliffs_delta_vs_baseline.csv\", index=False\n",
    "        )\n",
    "        say(\"[combined] cliffs_delta_vs_baseline.csv written.\")\n",
    "\n",
    "    # Pooled tests across (method, pct) — single files\n",
    "    pooled_wil, pooled_cliffs = [], []\n",
    "    for (meth, pct), bins in pooled_pairs.items():\n",
    "        xb = np.asarray(bins[\"base\"], dtype=np.float32)\n",
    "        xf = np.asarray(bins[\"full\"], dtype=np.float32)\n",
    "        if xb.size and xb.shape == xf.shape:\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "            except Exception:\n",
    "                p_raw = 1.0\n",
    "            d, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "            pooled_wil.append({\"method\": meth, \"pct\": pct, \"n_pairs\": int(xb.size), \"p_value\": float(p_raw)})\n",
    "            pooled_cliffs.append({\"method\": meth, \"pct\": pct, \"n_pairs\": int(xb.size),\n",
    "                                  \"cliffs_delta_paired\": float(d), \"npos\": int(npos),\n",
    "                                  \"nneg\": int(nneg), \"nzero\": int(nzero), \"magnitude\": mag})\n",
    "    if pooled_wil:\n",
    "        pd.DataFrame(pooled_wil).sort_values([\"method\",\"pct\"]).to_csv(\n",
    "            TABLES_DIR / \"wilcoxon_pooled_vs_baseline.csv\", index=False\n",
    "        )\n",
    "        pd.DataFrame(pooled_cliffs).sort_values([\"method\",\"pct\"]).to_csv(\n",
    "            TABLES_DIR / \"cliffs_delta_pooled_vs_baseline.csv\", index=False\n",
    "        )\n",
    "        say(\"[combined] pooled tests written.\")\n",
    "\n",
    "    # Bootstrap Δ median (global micro-bootstrap over folds×domains) for ALL rows\n",
    "    boots = []\n",
    "    for _, row in wide.iterrows():\n",
    "        emb, reg, meth = row[\"embedding\"], row[\"regressor\"], row[\"method\"]\n",
    "        pct, K, Mmax   = int(row[\"pct\"]), int(row[\"K\"]), int(row[\"M_max\"])\n",
    "        p_b = RES_DIR / f\"rrmse_perfold_{emb}__{reg}__{meth}__pct{pct}_K{K}__Mmax{Mmax}__baseline.csv\"\n",
    "        p_f = RES_DIR / f\"rrmse_perfold_{emb}__{reg}__{meth}__pct{pct}_K{K}__Mmax{Mmax}__full.csv\"\n",
    "        if not (p_b.exists() and p_f.exists()):\n",
    "            continue\n",
    "        db, df = pd.read_csv(p_b), pd.read_csv(p_f)\n",
    "        cols = _dom_cols(db)\n",
    "        if not cols or not set(cols).issubset(df.columns):\n",
    "            continue\n",
    "        base_mat = db[cols].to_numpy(dtype=np.float32)\n",
    "        full_mat = df[cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        n_folds, n_dom = base_mat.shape\n",
    "        draws = np.empty(BOOT_B, dtype=np.float32)\n",
    "        for b in range(BOOT_B):\n",
    "            idx_f = rng.integers(0, n_folds, size=n_folds)\n",
    "            idx_d = rng.integers(0, n_dom,   size=n_dom)\n",
    "            b_flat = base_mat[idx_f][:, idx_d].ravel()\n",
    "            f_flat = full_mat[idx_f][:, idx_d].ravel()\n",
    "            draws[b] = np.median(b_flat) - np.median(f_flat)\n",
    "        ci_lo, ci_hi = np.percentile(draws, [2.5, 97.5])\n",
    "        boots.append({\n",
    "            \"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "            \"pct\":pct,\"K\":K,\"M_max\":Mmax,\n",
    "            \"delta_hat\": float(np.median(draws)),\n",
    "            \"ci_lo\": float(ci_lo), \"ci_hi\": float(ci_hi),\n",
    "            \"p_win\": float((draws > 0).mean()), \"B\": int(BOOT_B)\n",
    "        })\n",
    "    if boots:\n",
    "        pd.DataFrame(boots).sort_values([\"embedding\",\"regressor\",\"method\",\"pct\"]).to_csv(\n",
    "            TABLES_DIR / \"bootstrap_delta_ci.csv\", index=False\n",
    "        )\n",
    "        say(\"[combined] bootstrap_delta_ci.csv written.\")\n",
    "\n",
    "    # ── Combined plots (full range, baseline dotted, full solid; same color per (emb,reg))\n",
    "    def _color_map_for_pairs(pairs):\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])\n",
    "        if not colors: colors = [f\"C{i}\" for i in range(10)]\n",
    "        cyc = iter(colors * ((len(pairs) // max(1,len(colors))) + 2))\n",
    "        return {pair: next(cyc) for pair in pairs}\n",
    "\n",
    "    def plot_rrmse_vs_pct(summary_csv_path, methods_filter=None):\n",
    "        df = pd.read_csv(summary_csv_path)\n",
    "        need = {\"embedding\",\"regressor\",\"method\",\"pct\",\"baseline\",\"full\"}\n",
    "        if not need.issubset(df.columns):\n",
    "            log.warning(\"[combined] summary file missing required columns; skipping plots.\")\n",
    "            return\n",
    "        if methods_filter:\n",
    "            df = df[df[\"method\"].isin(methods_filter)].copy()\n",
    "\n",
    "        for method in sorted(df[\"method\"].unique()):\n",
    "            sub = df[df[\"method\"] == method].copy()\n",
    "            agg = sub.groupby([\"embedding\",\"regressor\",\"pct\"], as_index=False)[[\"baseline\",\"full\"]].median()\n",
    "            pairs = sorted(agg[[\"embedding\",\"regressor\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            cmap = _color_map_for_pairs(pairs)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(16, 4.8))\n",
    "            xticks = sorted(set(PCTS_FOR_PLOTS) | set(agg[\"pct\"].unique()))\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xlim(min(PCTS_FOR_PLOTS) - 5, max(PCTS_FOR_PLOTS) + 20)\n",
    "\n",
    "            for emb, reg in pairs:\n",
    "                cur = agg[(agg[\"embedding\"] == emb) & (agg[\"regressor\"] == reg)].sort_values(\"pct\")\n",
    "                color = cmap[(emb, reg)]\n",
    "                ax.plot(cur[\"pct\"], cur[\"full\"],     marker=\"o\", linestyle=\"-\",  label=f\"{emb} | {reg} (full)\",  color=color)\n",
    "                ax.plot(cur[\"pct\"], cur[\"baseline\"], marker=\"o\", linestyle=\"--\", label=f\"{emb} | {reg} (baseline)\", color=color)\n",
    "\n",
    "            ax.set_xlabel(\"%K\"); ax.set_ylabel(\"RRMSE\")\n",
    "            ax.set_title(f\"(Global median) RRMSE vs %K — {method}\")\n",
    "            ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False, ncol=1)\n",
    "            fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "            out = FIGS_DIR / f\"combined_rrmse_vs_pct__{method}.png\"\n",
    "            fig.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "    def plot_delta_vs_pct(summary_csv_path, methods_filter=None):\n",
    "        df = pd.read_csv(summary_csv_path)\n",
    "        need = {\"embedding\",\"regressor\",\"method\",\"pct\",\"baseline\",\"full\"}\n",
    "        if not need.issubset(df.columns):\n",
    "            log.warning(\"[combined] summary file missing required columns; skipping Δ plot.\")\n",
    "            return\n",
    "        if methods_filter:\n",
    "            df = df[df[\"method\"].isin(methods_filter)].copy()\n",
    "        df[\"delta\"] = df[\"baseline\"] - df[\"full\"]\n",
    "\n",
    "        for method in sorted(df[\"method\"].unique()):\n",
    "            sub = df[df[\"method\"] == method].copy()\n",
    "            agg = sub.groupby([\"embedding\",\"regressor\",\"pct\"], as_index=False)[\"delta\"].median()\n",
    "            pairs = sorted(agg[[\"embedding\",\"regressor\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            cmap = _color_map_for_pairs(pairs)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(16, 4.8))\n",
    "            xticks = sorted(set(PCTS_FOR_PLOTS) | set(agg[\"pct\"].unique()))\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xlim(min(PCTS_FOR_PLOTS) - 5, max(PCTS_FOR_PLOTS) + 20)\n",
    "\n",
    "            for emb, reg in pairs:\n",
    "                cur = agg[(agg[\"embedding\"] == emb) & (agg[\"regressor\"] == reg)].sort_values(\"pct\")\n",
    "                color = cmap[(emb, reg)]\n",
    "                ax.plot(cur[\"pct\"], cur[\"delta\"], marker=\"o\", linestyle=\"-\", label=f\"{emb} | {reg}\", color=color)\n",
    "\n",
    "            ax.axhline(0.0, linestyle=\"--\")\n",
    "            ax.set_xlabel(\"%K\"); ax.set_ylabel(\"ΔRRMSE (baseline - full)\")\n",
    "            ax.set_title(f\"(Global median) ΔRRMSE vs %K — {method}\")\n",
    "            ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False, ncol=1)\n",
    "            fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "            out = FIGS_DIR / f\"combined_delta_vs_pct__{method}.png\"\n",
    "            fig.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Draw full-range combined plots \n",
    "    plot_rrmse_vs_pct(TABLES_DIR / \"summary_median_rrmse.csv\", methods_filter=methods)\n",
    "    plot_delta_vs_pct(TABLES_DIR / \"summary_median_rrmse.csv\", methods_filter=methods)\n",
    "\n",
    "    # --- CD diagrams + full printouts per (embedding, method, pct) ---\n",
    "    for emb in STUDENT_EMBEDDINGS:\n",
    "        for method in methods:\n",
    "            Mmax = max(avail[method])  # same M_max you used elsewhere\n",
    "            for pct in PCT_LIST:\n",
    "                mat, labels = _matrix_for_method_pct(\n",
    "                    emb, method, pct, Mmax,\n",
    "                    regressors=sorted(df[\"regressor\"].unique()) if 'df' in locals() else STUDENTS\n",
    "                )\n",
    "                if mat.size == 0 or len(labels) < 2:\n",
    "                    continue\n",
    "                tag = f\"{method}__pct{pct}__{emb}__full\"\n",
    "                title = f\"CD (global median per fold) — {method} @ {pct}%K • {emb} (full)\"\n",
    "                cd_plot(mat, labels, title, f\"cd_{tag}.png\")\n",
    "                \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Main functions\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def _all_baselines_exist(students, methods, pct_list, N_SEEDS, student_emb, M_MAX_BY_METHOD):\n",
    "    \"\"\"Return True if for every (student, method, pct) the baseline CSV exists with >= N_SEEDS rows.\"\"\"\n",
    "    for method in methods:\n",
    "        Mmax = M_MAX_BY_METHOD[method]\n",
    "        for pct in pct_list:\n",
    "            K = pct_to_K(pct, N_SEEDS)\n",
    "            for student in students:\n",
    "                out_base = RES_DIR / f\"rrmse_perfold_{student_emb}__{student}__{method}__pct{pct}_K{K}__Mmax{Mmax}__baseline.csv\"\n",
    "                if not out_base.exists():\n",
    "                    return False\n",
    "                try:\n",
    "                    n_rows = sum(1 for _ in open(out_base, \"r\", encoding=\"utf-8\")) - 1\n",
    "                    if n_rows < N_SEEDS:\n",
    "                        return False\n",
    "                except Exception:\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def run():\n",
    "    log.info(f\"=== step e_3 (LOOCV, percent; HPs from teacher, no tuning) | run_id={RUN_ID} ===\")\n",
    "\n",
    "    # ── Seeds\n",
    "    seeds_df = load_seeds()\n",
    "    X_seed = ensure_seed_vectors(seeds_df, STUDENT_EMB)\n",
    "    Y_seed = seeds_df[TARGET_COLS].to_numpy(dtype=np.float32)\n",
    "    N_SEEDS = len(seeds_df)\n",
    "    folds = list(range(N_SEEDS))\n",
    "    if MAX_FOLDS and MAX_FOLDS < N_SEEDS:\n",
    "        folds = folds[:MAX_FOLDS]\n",
    "        log.info(f\"Using first {len(folds)} LOOCV folds (MAX_FOLDS={MAX_FOLDS})\")\n",
    "    else:\n",
    "        log.info(f\"Folds: {len(folds)} (LOOCV)\")\n",
    "\n",
    "    # ── Methods & M_max\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    if METHODS:\n",
    "        avail = {m: avail[m] for m in avail if m in METHODS}\n",
    "    if not avail:\n",
    "        raise RuntimeError(\"No methods discovered in tuned g_2a_teacher_labeling (fold00 files).\")\n",
    "    methods = sorted(avail.keys())\n",
    "    M_MAX_BY_METHOD = {m: max(avail[m]) for m in methods}\n",
    "    log.info(f\"Methods: {methods}\")\n",
    "    log.info(f\"M_max per method: {M_MAX_BY_METHOD}\")\n",
    "\n",
    "    # ── Preload synth embeddings & index mappings for M_max\n",
    "    SYN: Dict[Tuple[str,int], Dict[str, object]] = {}\n",
    "    for method in methods:\n",
    "        Mmax = M_MAX_BY_METHOD[method]\n",
    "        src_csv = g1_source_csv(method, Mmax)\n",
    "        try:\n",
    "            npy, idx = synth_cache_paths_from_g2c(src_csv, STUDENT_EMB)\n",
    "        except FileNotFoundError:\n",
    "            npy, idx = ensure_synth_cache(method, Mmax, STUDENT_EMB)\n",
    "        Xs = np.load(npy).astype(np.float32, copy=False)\n",
    "        df_idx = pd.read_csv(idx)\n",
    "        if \"text\" not in df_idx.columns:\n",
    "            raise ValueError(f\"{idx.name}: missing 'text' column\")\n",
    "        texts = df_idx[\"text\"].astype(str).tolist()\n",
    "        text2pos = {t: i for i, t in enumerate(texts)}\n",
    "        SYN[(method, Mmax)] = {\"X\": Xs, \"text2pos\": text2pos}\n",
    "        log.info(f\"[{method}] M_max={Mmax}: loaded synth embeds ({Xs.shape[0]} rows)\")\n",
    "\n",
    "    # ── Preload per-fold tuned labels (teacher outputs) for M_max\n",
    "    LABELS: Dict[Tuple[str,int], List[pd.DataFrame]] = {}\n",
    "    label_cols = [\"text\", *TARGET_COLS]\n",
    "    for method in methods:\n",
    "        Mmax = M_MAX_BY_METHOD[method]\n",
    "        per_fold = []\n",
    "        for fi in folds:\n",
    "            path = g2_label_file(fi, method, Mmax, label_emb=LABEL_EMB, label_model=LABEL_MODEL)\n",
    "            if not path.exists():\n",
    "                raise FileNotFoundError(f\"Missing tuned labels: {path.name}\")\n",
    "            df = pd.read_csv(path, usecols=lambda c: (c == \"text\") or (c in TARGET_COLS))\n",
    "            if set(label_cols).issubset(df.columns):\n",
    "                df = df[label_cols].reset_index(drop=True)\n",
    "            else:\n",
    "                missing = [c for c in label_cols if c not in df.columns]\n",
    "                raise ValueError(f\"{path.name}: missing columns {missing}\")\n",
    "            per_fold.append(df)\n",
    "        LABELS[(method, Mmax)] = per_fold\n",
    "        log.info(f\"[{method}] M_max={Mmax}: loaded tuned labels for {len(per_fold)} folds\")\n",
    "\n",
    "    # ── Baseline: seeds-only prediction (reuse teacher HPs where possible)\n",
    "    SKIP_BASELINE_IF_PRESENT = os.getenv(\"SKIP_BASELINE_IF_PRESENT\", \"0\").lower() in {\"1\",\"true\",\"yes\"}\n",
    "\n",
    "    if SKIP_BASELINE_IF_PRESENT and _all_baselines_exist(STUDENTS, methods, PCT_LIST, N_SEEDS, STUDENT_EMB, M_MAX_BY_METHOD):\n",
    "        log.info(\"[baseline] All baseline CSVs already present → skipping baseline computation.\")\n",
    "        baseline_core_by_student: Dict[str, List[Dict]] = {stu: [] for stu in STUDENTS}  # ensure writer no-ops\n",
    "    else:\n",
    "        baseline_core_by_student: Dict[str, List[Dict]] = {stu: [] for stu in STUDENTS}\n",
    "        for fi in folds:\n",
    "            test_mask = (np.arange(N_SEEDS) == fi)\n",
    "            train_mask = ~test_mask\n",
    "            X_tr_seed, Y_tr_seed = X_seed[train_mask], Y_seed[train_mask]\n",
    "            X_te_seed, Y_te_seed = X_seed[test_mask],  Y_seed[test_mask]\n",
    "            dummy = Y_tr_seed.mean(axis=0, keepdims=True)\n",
    "\n",
    "            for stu in STUDENTS:\n",
    "                est = load_teacher_fold_estimator(fi, stu, STUDENT_EMB)\n",
    "                if est is not None:\n",
    "                    Y_hat = est.predict(X_te_seed)\n",
    "                else:\n",
    "                    hp = hp_from_teacher_or_json(fi, stu, STUDENT_EMB, est=None)\n",
    "                    model = build_with_hp(stu, hp)\n",
    "                    model.fit(X_tr_seed, Y_tr_seed)\n",
    "                    Y_hat = model.predict(X_te_seed)\n",
    "\n",
    "                r = rrmse_vs_dummy(Y_te_seed, Y_hat, dummy)\n",
    "                baseline_core_by_student[stu].append({\n",
    "                    \"fold\": fi,\n",
    "                    \"median_rrmse_fold\": float(np.median(r)),  # traceability\n",
    "                    **{f\"rrmse_{c}\": float(r[i]) for i, c in enumerate(TARGET_COLS)}\n",
    "                })\n",
    "\n",
    "            if (fi + 1) % LOG_EVERY == 0:\n",
    "                log.info(f\"[baseline] folds completed: {fi+1}/{len(folds)}\")\n",
    "\n",
    "\n",
    "    # Helper: write/ensure per-pct baseline CSV once for naming symmetry\n",
    "    def ensure_baseline_csv(student: str, method: str, pct: int, K: int, Mmax: int, out_base: Path):\n",
    "        # If we didn’t compute baseline in this run, we can’t write fresh rows\n",
    "        cores = baseline_core_by_student.get(student, [])\n",
    "        if not cores:\n",
    "            return\n",
    "\n",
    "        rows = []\n",
    "        for core in cores:\n",
    "            rows.append({\n",
    "                \"fold\": core[\"fold\"],\n",
    "                \"method\": method, \"pct\": pct, \"K\": K, \"M_max\": Mmax,\n",
    "                \"student\": \"S1\", \"embedding\": STUDENT_EMB, \"regressor\": student, \"variant\": \"baseline\",\n",
    "                \"median_rrmse_fold\": core[\"median_rrmse_fold\"],\n",
    "                **{k: core[k] for k in core if k.startswith(\"rrmse_domain\")}\n",
    "            })\n",
    "        df_new = pd.DataFrame(rows)\n",
    "\n",
    "        if out_base.exists():\n",
    "            try:\n",
    "                df_old = pd.read_csv(out_base)\n",
    "                df_new = (pd.concat([df_old, df_new], axis=0, ignore_index=True)\n",
    "                            .sort_values(\"fold\")\n",
    "                            .drop_duplicates(subset=[\"fold\"], keep=\"last\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        out_base.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_new.to_csv(out_base, index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # ── Full (augmented): per-fold HP reuse per student (no tuning)\n",
    "    log.info(\"── Full (augmented): per-fold HP reuse (no tuning) for each student×method×pct\")\n",
    "    PCT_TO_K = {p: pct_to_K(p, N_SEEDS) for p in PCT_LIST}\n",
    "\n",
    "    for method in methods:\n",
    "        Mmax = M_MAX_BY_METHOD[method]\n",
    "        for pct in PCT_LIST:\n",
    "            K = PCT_TO_K[pct]\n",
    "            for student in STUDENTS:\n",
    "                out_full = RES_DIR / f\"rrmse_perfold_{STUDENT_EMB}__{student}__{method}__pct{pct}_K{K}__Mmax{Mmax}__full.csv\"\n",
    "                out_base = RES_DIR / f\"rrmse_perfold_{STUDENT_EMB}__{student}__{method}__pct{pct}_K{K}__Mmax{Mmax}__baseline.csv\"\n",
    "\n",
    "                # If FULL already complete, just ensure baseline and continue\n",
    "                full_complete = False\n",
    "                if out_full.exists():\n",
    "                    try:\n",
    "                        n_rows = sum(1 for _ in open(out_full, \"r\", encoding=\"utf-8\")) - 1\n",
    "                        full_complete = (n_rows >= len(folds))\n",
    "                    except Exception:\n",
    "                        full_complete = False\n",
    "                if full_complete:\n",
    "                    ensure_baseline_csv(student, method, pct, K, Mmax, out_base)\n",
    "                    log.info(f\"[skip] {out_full.name} already complete.\")\n",
    "                    continue\n",
    "\n",
    "                # Legacy reuse for chain_ERCcv_lr only at {100,200,400}\n",
    "                if student == \"chain_ERCcv_lr\" and pct in {100, 200, 400}:\n",
    "                    if ensure_chainlr_from_legacy_pct(STUDENT_EMB, method, pct, K, N_SEEDS, Mmax):\n",
    "                        ensure_baseline_csv(student, method, pct, K, Mmax, out_base)\n",
    "                        continue\n",
    "\n",
    "                # Compute missing folds\n",
    "                pf_rows = []\n",
    "                done_folds = set()\n",
    "                if out_full.exists():\n",
    "                    try:\n",
    "                        done_folds = set(pd.read_csv(out_full, usecols=[\"fold\"])[\"fold\"].astype(int).tolist())\n",
    "                    except Exception:\n",
    "                        done_folds = set()\n",
    "\n",
    "                for fi in folds:\n",
    "                    if fi in done_folds:\n",
    "                        continue\n",
    "                    test_mask = (np.arange(N_SEEDS) == fi)\n",
    "                    train_mask = ~test_mask\n",
    "                    X_tr_seed, Y_tr_seed = X_seed[train_mask], Y_seed[train_mask]\n",
    "                    X_te_seed, Y_te_seed = X_seed[test_mask],  Y_seed[test_mask]\n",
    "                    dummy = Y_tr_seed.mean(axis=0, keepdims=True)\n",
    "\n",
    "                    # fold-i labels on Mmax, slice first K deterministically\n",
    "                    df_lab = LABELS[(method, Mmax)][fi].iloc[:K].reset_index(drop=True)\n",
    "                    text_list = df_lab[\"text\"].astype(str).tolist()\n",
    "                    mapper = SYN[(method, Mmax)][\"text2pos\"]\n",
    "                    pos = [mapper.get(t, -1) for t in text_list]\n",
    "                    if any(i < 0 for i in pos):\n",
    "                        miss = [t for t, i in zip(text_list, pos) if i < 0][:3]\n",
    "                        raise RuntimeError(f\"[{method}|pct={pct}|K={K}|fold={fi}] {len(miss)} label texts not in synth index. e.g., {miss}\")\n",
    "                    X_syn = SYN[(method, Mmax)][\"X\"][pos, :]\n",
    "                    Y_syn = df_lab[TARGET_COLS].to_numpy(dtype=np.float32)\n",
    "\n",
    "                    X_tr_full = np.ascontiguousarray(np.vstack([X_tr_seed, X_syn]), dtype=np.float32)\n",
    "                    Y_tr_full = np.ascontiguousarray(np.vstack([Y_tr_seed, Y_syn]), dtype=np.float32)\n",
    "\n",
    "                    # Load teacher HPs for THIS student & fold\n",
    "                    est = load_teacher_fold_estimator(fi, student, STUDENT_EMB)\n",
    "                    hp = hp_from_teacher_or_json(fi, student, STUDENT_EMB, est)\n",
    "\n",
    "                    # Persist HPs for traceability\n",
    "                    hp_file = HP_PERFOLD / student / method / f\"n{Mmax}\" / f\"pct{pct}_K{K}\" / f\"fold{fi:02d}_best.json\"\n",
    "                    hp_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    try:\n",
    "                        hp_file.write_text(json.dumps(hp, indent=2), encoding=\"utf-8\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    # Build model, fit on augmented train, predict held-out seed\n",
    "                    model_full = build_with_hp(student, hp)\n",
    "                    t0 = time.perf_counter()\n",
    "                    model_full.fit(X_tr_full, Y_tr_full)\n",
    "                    Y_hat_full = model_full.predict(X_te_seed)\n",
    "                    dt_full = time.perf_counter() - t0\n",
    "\n",
    "                    r_full = rrmse_vs_dummy(Y_te_seed, Y_hat_full, dummy)\n",
    "                    pf_rows.append({\n",
    "                        \"fold\": fi,\n",
    "                        \"method\": method, \"pct\": pct, \"K\": K, \"M_max\": Mmax,\n",
    "                        \"student\": \"S1\", \"embedding\": STUDENT_EMB, \"regressor\": student, \"variant\": \"full\",\n",
    "                        \"median_rrmse_fold\": float(np.median(r_full)),  # trace only\n",
    "                        **{f\"rrmse_{c}\": float(r_full[i]) for i, c in enumerate(TARGET_COLS)}\n",
    "                    })\n",
    "\n",
    "                    if (fi + 1) % LOG_EVERY == 0:\n",
    "                        log.info(f\"[{student}|{method}|pct={pct}|K={K}] folds done: {fi+1}/{len(folds)} (last full fit+pred {_fmt_dt(dt_full)})\")\n",
    "\n",
    "                # Write/append FULL; ensure BASELINE exists\n",
    "                if pf_rows:\n",
    "                    if out_full.exists():\n",
    "                        df_old = pd.read_csv(out_full)\n",
    "                        df_new = pd.concat([df_old, pd.DataFrame(pf_rows)], axis=0, ignore_index=True)\n",
    "                    else:\n",
    "                        df_new = pd.DataFrame(pf_rows)\n",
    "                    df_new = df_new.sort_values(\"fold\").drop_duplicates(subset=[\"fold\"], keep=\"last\")\n",
    "                    out_full.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    df_new.to_csv(out_full, index=False)\n",
    "\n",
    "                ensure_baseline_csv(student, method, pct, K, Mmax, out_base)\n",
    "\n",
    "    # ── Unified (global-median) tables/stats/plots \n",
    "\n",
    "    pat = re.compile(\n",
    "        r\"^rrmse_perfold_(?P<emb>.+?)__(?P<reg>.+?)__(?P<meth>.+?)__pct(?P<pct>\\d+)_K(?P<K>\\d+)__Mmax(?P<M>\\d+)__(?P<var>baseline|full)\\.csv$\"\n",
    "    )\n",
    "    summary_rows, wil_rows, cliffs_rows = [], [], []\n",
    "    pooled_pairs: Dict[Tuple[str,int], Dict[str, List[float]]] = {}\n",
    "\n",
    "    full_files = sorted([p for p in RES_DIR.glob(f\"rrmse_perfold_{STUDENT_EMB}__*__full.csv\") if pat.match(p.name)])\n",
    "    for f_full in full_files:\n",
    "        m = pat.match(f_full.name)\n",
    "        reg = m[\"reg\"]; meth = m[\"meth\"]; pct = int(m[\"pct\"]); K = int(m[\"K\"]); Mmax = int(m[\"M\"])\n",
    "        if pct not in PCT_LIST:\n",
    "            continue\n",
    "        f_base = RES_DIR / f\"rrmse_perfold_{STUDENT_EMB}__{reg}__{meth}__pct{pct}_K{K}__Mmax{Mmax}__baseline.csv\"\n",
    "        if not f_base.exists():\n",
    "            continue\n",
    "\n",
    "        g_base = _global_median_from_file(f_base)\n",
    "        g_full = _global_median_from_file(f_full)\n",
    "        summary_rows.append({\n",
    "            \"student\":\"S1\",\"embedding\":STUDENT_EMB,\"regressor\":reg,\"method\":meth,\n",
    "            \"pct\":pct,\"K\":K,\"M_max\":Mmax, \"baseline\": g_base, \"full\": g_full\n",
    "        })\n",
    "\n",
    "        xb, xf = _flat_arrays(f_base, f_full)\n",
    "        pooled_pairs.setdefault((meth, pct), {\"base\": [], \"full\": []})\n",
    "        pooled_pairs[(meth, pct)][\"base\"].extend(xb.tolist())\n",
    "        pooled_pairs[(meth, pct)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "        try:\n",
    "            _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "        except Exception:\n",
    "            p_raw = 1.0\n",
    "        d, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "        wil_rows.append({\"student\":\"S1\",\"regressor\":reg,\"method\":meth,\n",
    "                         \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"p_raw\":float(p_raw)})\n",
    "        cliffs_rows.append({\"student\":\"S1\",\"regressor\":reg,\"method\":meth,\n",
    "                            \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"cliffs_delta_paired\":float(d),\n",
    "                            \"npos\":int(npos),\"nneg\":int(nneg),\"nzero\":int(nzero),\"magnitude\":mag})\n",
    "\n",
    "    if summary_rows:\n",
    "        wide = pd.DataFrame(summary_rows).sort_values([\"regressor\",\"method\",\"pct\"]).reset_index(drop=True)\n",
    "        tol = 1e-9\n",
    "        def _cmp(row):\n",
    "            b, f = row[\"baseline\"], row[\"full\"]\n",
    "            if pd.isna(b) or pd.isna(f): return \"n/a\"\n",
    "            if (b - f) > tol:  return \"better\"\n",
    "            if (f - b) > tol:  return \"worse\"\n",
    "            return \"same\"\n",
    "        wide[\"baseline_vs_full\"] = wide.apply(_cmp, axis=1)\n",
    "\n",
    "        # Per-embedding tables (optional) — use embedding-suffixed filenames\n",
    "        if WRITE_PER_EMBED_TABLES:\n",
    "            out_sum_embed = TABLES_DIR / f\"summary_median_rrmse__{STUDENT_EMB}.csv\"\n",
    "            wide.to_csv(out_sum_embed, index=False)\n",
    "\n",
    "            comp = wide.copy()\n",
    "            comp[\"delta\"] = comp[\"baseline\"] - comp[\"full\"]\n",
    "            comp[\"rel_change_pct\"] = 100.0 * comp[\"delta\"] / comp[\"baseline\"].replace(0, np.nan)\n",
    "            comp.to_csv(TABLES_DIR / f\"summary_median_rrmse_with_delta__{STUDENT_EMB}.csv\", index=False)\n",
    "\n",
    "            log.info(f\"summary_median_rrmse__{STUDENT_EMB}.csv + _with_delta__{STUDENT_EMB}.csv written (per-embedding).\")\n",
    "\n",
    "\n",
    "    # ── Per-embedding stats: write only when per-embed outputs are desired, and suffix filenames\n",
    "    if WRITE_PER_EMBED_TABLES:\n",
    "        if wil_rows:\n",
    "            df_w = pd.DataFrame(wil_rows)\n",
    "            holm_rows = []\n",
    "            for (regressor, method), grp in df_w.groupby([\"regressor\",\"method\"]):\n",
    "                pvals = grp[\"p_raw\"].to_numpy()\n",
    "                _, p_holm, _, _ = multipletests(pvals, method=\"holm\")\n",
    "                for i, (_, row) in enumerate(grp.reset_index(drop=True).iterrows()):\n",
    "                    holm_rows.append({\n",
    "                        \"student\":\"S1\",\"regressor\": regressor,\"method\": method,\n",
    "                        \"pct\": int(row[\"pct\"]), \"K\": int(row[\"K\"]), \"M_max\": int(row[\"M_max\"]),\n",
    "                        \"p_raw\": float(row[\"p_raw\"]), \"p_holm\": float(p_holm[i]),\n",
    "                    })\n",
    "            pd.DataFrame(holm_rows).sort_values([\"regressor\",\"method\",\"pct\"]).to_csv(\n",
    "                TABLES_DIR / f\"wilcoxon_holm_vs_baseline__{STUDENT_EMB}.csv\", index=False\n",
    "            )\n",
    "            log.info(f\"wilcoxon_holm_vs_baseline__{STUDENT_EMB}.csv written (per-embedding).\")\n",
    "\n",
    "        if cliffs_rows:\n",
    "            pd.DataFrame(cliffs_rows).sort_values([\"regressor\",\"method\",\"pct\"]).to_csv(\n",
    "                TABLES_DIR / f\"cliffs_delta_vs_baseline__{STUDENT_EMB}.csv\", index=False\n",
    "            )\n",
    "            log.info(f\"cliffs_delta_vs_baseline__{STUDENT_EMB}.csv written (per-embedding).\")\n",
    "\n",
    "        # Pooled tests across (method, pct) — per-embedding files\n",
    "        pooled_wil, pooled_cliffs = [], []\n",
    "        for (meth, pct), bins in pooled_pairs.items():\n",
    "            xb = np.asarray(bins[\"base\"], dtype=np.float32)\n",
    "            xf = np.asarray(bins[\"full\"], dtype=np.float32)\n",
    "            if xb.size and xb.shape == xf.shape:\n",
    "                try:\n",
    "                    _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "                except Exception:\n",
    "                    p_raw = 1.0\n",
    "                d, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "                pooled_wil.append({\"method\": meth, \"pct\": pct, \"n_pairs\": int(xb.size), \"p_value\": float(p_raw)})\n",
    "                pooled_cliffs.append({\"method\": meth, \"pct\": pct, \"n_pairs\": int(xb.size),\n",
    "                                      \"cliffs_delta_paired\": float(d), \"npos\": int(npos),\n",
    "                                      \"nneg\": int(nneg), \"nzero\": int(nzero), \"magnitude\": mag})\n",
    "        if pooled_wil:\n",
    "            pd.DataFrame(pooled_wil).sort_values([\"method\",\"pct\"]).to_csv(\n",
    "                TABLES_DIR / f\"wilcoxon_pooled_vs_baseline__{STUDENT_EMB}.csv\", index=False\n",
    "            )\n",
    "            pd.DataFrame(pooled_cliffs).sort_values([\"method\",\"pct\"]).to_csv(\n",
    "                TABLES_DIR / f\"cliffs_delta_pooled_vs_baseline__{STUDENT_EMB}.csv\", index=False\n",
    "            )\n",
    "            log.info(f\"pooled tests written (per-embedding) for {STUDENT_EMB}.\")\n",
    "\n",
    "    # ---------------------------------- PLOTS ------------------------------------\n",
    "    def _color_map_for_pairs(pairs):\n",
    "        colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "        cyc = cycle(colors)\n",
    "        mapping = {}\n",
    "        for pr in pairs:\n",
    "            mapping[pr] = next(cyc)\n",
    "        return mapping\n",
    "\n",
    "    def plot_rrmse_vs_pct(summary_csv_path, pcts=None, emb_only=None):\n",
    "        df = pd.read_csv(summary_csv_path)\n",
    "        need = {\"embedding\",\"regressor\",\"method\",\"pct\",\"baseline\",\"full\"}\n",
    "        if not need.issubset(df.columns):\n",
    "            return\n",
    "        if emb_only:\n",
    "            df = df[df[\"embedding\"] == emb_only].copy()\n",
    "        if pcts:\n",
    "            df = df[df[\"pct\"].isin(list(pcts))].copy()\n",
    "\n",
    "        for method in sorted(df[\"method\"].unique()):\n",
    "            sub = df[df[\"method\"] == method].copy()\n",
    "            agg = sub.groupby([\"embedding\",\"regressor\",\"pct\"], as_index=False)[[\"baseline\",\"full\"]].median()\n",
    "            pairs = sorted(agg[[\"embedding\",\"regressor\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            cmap = _color_map_for_pairs(pairs)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(16, 4.8))\n",
    "            xticks = sorted(set(PCTS_FOR_PLOTS) | set(agg[\"pct\"].unique()))\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xlim(min(PCTS_FOR_PLOTS) - 5, max(PCTS_FOR_PLOTS) + 20)\n",
    "\n",
    "            for emb, reg in pairs:\n",
    "                cur = agg[(agg[\"embedding\"] == emb) & (agg[\"regressor\"] == reg)].sort_values(\"pct\")\n",
    "                color = cmap[(emb, reg)]\n",
    "                ax.plot(cur[\"pct\"], cur[\"full\"],     marker=\"o\", linestyle=\"-\",  color=color, label=f\"{emb} | {reg} (full)\")\n",
    "                ax.plot(cur[\"pct\"], cur[\"baseline\"], marker=\"o\", linestyle=\"--\", color=color, label=f\"{emb} | {reg} (baseline)\")\n",
    "\n",
    "            ax.set_xlabel(\"%K\")\n",
    "            ax.set_ylabel(\"RRMSE\")\n",
    "            title_suffix = f\", {emb_only}\" if emb_only else \"\"\n",
    "            ax.set_title(f\"(Global median{title_suffix}) RRMSE vs %K — {method}\")\n",
    "            ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "            fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "            fig.savefig(OUT_PLOTS / f\"combined_rrmse_vs_pct__{method}__{emb_only or 'all'}.png\", dpi=200, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "    def plot_delta_vs_pct(summary_csv_path, pcts=None, emb_only=None):\n",
    "        df = pd.read_csv(summary_csv_path)\n",
    "        need = {\"embedding\",\"regressor\",\"method\",\"pct\",\"baseline\",\"full\"}\n",
    "        if not need.issubset(df.columns):\n",
    "            return\n",
    "        if emb_only:\n",
    "            df = df[df[\"embedding\"] == emb_only].copy()\n",
    "        if pcts:\n",
    "            df = df[df[\"pct\"].isin(list(pcts))].copy()\n",
    "        df[\"delta\"] = df[\"baseline\"] - df[\"full\"]\n",
    "\n",
    "        for method in sorted(df[\"method\"].unique()):\n",
    "            sub = df[df[\"method\"] == method].copy()\n",
    "            agg = sub.groupby([\"embedding\",\"regressor\",\"pct\"], as_index=False)[\"delta\"].median()\n",
    "            pairs = sorted(agg[[\"embedding\",\"regressor\"]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            cmap = _color_map_for_pairs(pairs)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(16, 4.8))\n",
    "            xticks = sorted(set(PCTS_FOR_PLOTS) | set(agg[\"pct\"].unique()))\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_xlim(min(PCTS_FOR_PLOTS) - 5, max(PCTS_FOR_PLOTS) + 20)\n",
    "\n",
    "            for emb, reg in pairs:\n",
    "                cur = agg[(agg[\"embedding\"] == emb) & (agg[\"regressor\"] == reg)].sort_values(\"pct\")\n",
    "                color = cmap[(emb, reg)]\n",
    "                ax.plot(cur[\"pct\"], cur[\"delta\"], marker=\"o\", linestyle=\"-\", color=color, label=f\"{emb} | {reg}\")\n",
    "\n",
    "            ax.axhline(0.0, linestyle=\"--\")\n",
    "            ax.set_xlabel(\"%K\")\n",
    "            ax.set_ylabel(\"ΔRRMSE (baseline - full)\")\n",
    "            title_suffix = f\", {emb_only}\" if emb_only else \"\"\n",
    "            ax.set_title(f\"(Global median{title_suffix}) ΔRRMSE vs %K — {method}\")\n",
    "            ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "            fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "            fig.savefig(OUT_PLOTS / f\"combined_delta_vs_pct__{method}__{emb_only or 'all'}.png\", dpi=200, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Render per-embedding plots from the per-embedding summary (if written)\n",
    "    sum_path = OUT_TABLES / f\"summary_median_rrmse__{STUDENT_EMB}.csv\"\n",
    "    if WRITE_PER_EMBED_PLOTS and WRITE_PER_EMBED_TABLES and sum_path.exists():\n",
    "        plot_rrmse_vs_pct(sum_path, pcts=PCTS_FOR_PLOTS, emb_only=STUDENT_EMB)\n",
    "        plot_delta_vs_pct(sum_path, pcts=PCTS_FOR_PLOTS, emb_only=STUDENT_EMB)\n",
    "\n",
    "\n",
    "    print(f\"Done.\\nTables → {OUT_TABLES}\\nPlots  → {OUT_PLOTS}\")\n",
    "\n",
    "\n",
    "def run_pipeline_compute():\n",
    "    DO_REFITS = os.getenv(\"DO_REFITS\", \"1\").lower() in {\"1\",\"true\",\"yes\"}\n",
    "    write_run_config()\n",
    "    if DO_REFITS:\n",
    "        for emb in STUDENT_EMBEDDINGS:\n",
    "            run_for_embedding(emb)\n",
    "    build_combined_reports_across_embeddings()\n",
    "\n",
    "def run_pipeline_review():\n",
    "    \"\"\"\n",
    "    Artifact-only: do not encode, fit, or refit anything.\n",
    "    Rebuild combined tables/plots from the per-fold CSVs already on disk.\n",
    "    \"\"\"\n",
    "    write_run_config()\n",
    "    build_combined_reports_across_embeddings()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Entry-point\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        if REVIEW_MODE:\n",
    "            run_pipeline_review()\n",
    "        else:\n",
    "            run_pipeline_compute()\n",
    "    except Exception as e:\n",
    "        logging.getLogger(__name__).exception(\"Fatal error in e_3_student_scoring: %s\", e)\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dec630-0926-4f8a-896d-add9f4f36f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr8cht_embedding_comparison",
   "language": "python",
   "name": "kr8cht_embedding_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
