{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca65977-1dae-4611-9a54-65e24be9e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/kr8cht_embedding_comparison/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "[e_2 (per-fold tuned)] Using device: mps | N_CORES=6\n",
      "=== step e_2 REVIEW_MODE: artifact-only scan started ===\n",
      "Using synth cache dir: outputs/e_2_teacher_labeling/cache/synth_embeds\n",
      "✔ Wrote run_summary.csv with 2887 rows\n",
      "=== step e_2 REVIEW_MODE completed ===\n",
      "Review complete. Summary in: /Users/bertvos/Documents/Notebooks/kr8cht_review_anonymous/outputs/e_2_teacher_labeling/run_summary.csv\n",
      "Run completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_2_teacher_labeling.ipynb\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "Per-fold, leak-free teacher labeling of synthetic Dutch activity sentences for\n",
    "multiple sentence-embedding backbones. The script tunes one teacher per\n",
    "(embedding, model, fold) on the 95 training seeds in LOOCV, then uses the\n",
    "fitted teacher to label all synthetics from Script e_1. All artifacts are cached\n",
    "per embedding to enable full reuse across runs.\n",
    "\n",
    "This script:\n",
    "1) Loads seed data and selects embeddings to use\n",
    "   - Reads 96 seed questions with 14 target scores (domain1..domain14).\n",
    "   - Resolves the selected embedding aliases via EMBEDDING_SPECS and SELECTED_EMBEDDINGS.\n",
    "\n",
    "2) Caches seed and synthetic embeddings per embedding\n",
    "   - STATIC seed vectors (word2vec, fasttext) are loaded from:\n",
    "       outputs/a_static/results/baseline_{word2vec|fasttext}_vectors.npy\n",
    "     (produced by a_static in compute mode).\n",
    "   - FROZEN seed vectors (all other HF/SentenceTransformer encoders) are loaded from, or\n",
    "     computed and saved to:\n",
    "       outputs/b_frozen/results/{embedding}_vectors.npy\n",
    "   - For each Script e_1 source, synthetic vectors are cached at:\n",
    "       outputs/e_2_teacher_labeling/cache/synth_embeds/{stem}__{embedding}.npy\n",
    "     with the paired index CSV at:\n",
    "       outputs/e_2_teacher_labeling/cache/synth_embeds/{stem}__index.csv\n",
    "\n",
    "3) Tunes teachers per (embedding, model, fold) leak-free\n",
    "   - Supported models: local_lasso, local_rf, chain_ERCcv_rf, chain_ERCcv_lr, global_rf.\n",
    "   - GridSearchCV (R²) on 95 training seeds with small/default grids.\n",
    "   - Caches best HPs as:\n",
    "       outputs/e_2_teacher_labeling/teacher/hp_fold{ii}_{embedding}__{model}.json\n",
    "   - Saves fitted fold-i teacher as:\n",
    "       models/teacher/teacher_fold{ii}_{embedding}__{model}.pkl\n",
    "\n",
    "4) Labels synthetics per (embedding, model, fold)\n",
    "   - Applies the fold-i teacher to every row of each Script e_1 CSV.\n",
    "   - Writes labels to:\n",
    "       outputs/e_2_teacher_labeling/g2f_labels_fold{ii}_{stem}__{embedding}__{model}.csv\n",
    "     and optionally a JSONL alongside it.\n",
    "   - Idempotent: skips re-labeling if a complete CSV already exists.\n",
    "\n",
    "5) Records summary and manifest\n",
    "   - Appends or writes:\n",
    "       outputs/e_2_teacher_labeling/run_summary.csv\n",
    "     with (embedding, model, source, fold, status, timing).\n",
    "   - Writes:\n",
    "       outputs/e_2_teacher_labeling/run_config.json\n",
    "     with configuration, cache locations, and results roots.\n",
    "\n",
    "REVIEW mode (REVIEW_MODE = True) — artifact-only:\n",
    "- Scans existing label CSVs and the cache/synth_embeds directory;\n",
    "  rebuilds run_summary.csv and a light run_config.json;\n",
    "- No encoding/tuning/fitting/labeling.\n",
    "\n",
    "Inputs:\n",
    "- data/activity_scores.csv\n",
    "- data/activities.csv\n",
    "- outputs/e_1_synth_augmentation/g_final_n{N}_{method}.csv\n",
    "  (columns include: domain_id, subdomain_id, facet_id, method, rank, text, passed)\n",
    "\n",
    "Outputs:\n",
    "- under outputs/e_2_teacher_labeling/:\n",
    "  • g2f_labels_fold{ii}_{stem}__{embedding}__{model}.csv\n",
    "  • (optional) g2f_labels_fold{ii}_{stem}__{embedding}__{model}.jsonl\n",
    "  • cache/synth_embeds/{stem}__{embedding}.npy\n",
    "  • cache/synth_embeds/{stem}__index.csv\n",
    "  • teacher/hp_fold{ii}_{embedding}__{model}.json\n",
    "  • run.log, run_summary.csv, run_config.json\n",
    "- under outputs/a_static/results/ (from script a_static):\n",
    "  • baseline_{word2vec|fasttext}_vectors.npy\n",
    "- under outputs/b_frozen/results/ (computed here if missing):\n",
    "  • {embedding}_vectors.npy\n",
    "- under models/teacher/:\n",
    "  • teacher_fold{ii}_{embedding}__{model}.pkl\n",
    "\"\"\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Imports\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sentence_transformers\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Device, cores, seeding\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "DEVICE_STR = device.type\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        torch.manual_seed(SEED)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize(); torch.cuda.empty_cache()\n",
    "        if DEVICE_STR == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Paths\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def project_root(marker: str = \"LICENSE\") -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for d in (here, *here.parents):\n",
    "        if (d / marker).is_file():\n",
    "            return d\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "ROOT = project_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "G1_DIR  = ROOT / \"outputs\" / \"e_1_synth_augmentation\"\n",
    "G2_DIR  = ROOT / \"outputs\" / \"e_2_teacher_labeling\"\n",
    "\n",
    "STATIC_RESULTS_DIR = ROOT / \"outputs\" / \"a_static\" / \"results\"   # baseline_{emb}_vectors.npy\n",
    "FROZEN_RESULTS_DIR = ROOT / \"outputs\" / \"b_frozen\" / \"results\"   # {emb}_vectors.npy\n",
    "\n",
    "CACHE_DIR = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "TEACHER_HP_DIR = G2_DIR / \"teacher\"\n",
    "MODELS_TEACHER_DIR = ROOT / \"models\" / \"teacher\"\n",
    "\n",
    "for p in (G2_DIR, TEACHER_HP_DIR, MODELS_TEACHER_DIR, CACHE_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_SUMMARY = G2_DIR / \"run_summary.csv\"\n",
    "LOG_FILE = G2_DIR / \"run.log\"\n",
    "\n",
    "for fpath in (RUN_SUMMARY, LOG_FILE):\n",
    "    fpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Logging\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for h in list(logging.root.handlers):\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename=str(LOG_FILE),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "console = logging.StreamHandler(sys.stdout)\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Configuration\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Modes\n",
    "REVIEW_MODE = True         # artifact-only summary, no labeling/tuning\n",
    "TUNE_ONLY   = False        # tune HP JSONs only; no teacher fitting or labeling\n",
    "\n",
    "# Embedding registry & selection\n",
    "EMBEDDING_SPECS: Dict[str, str] = {\n",
    "    \"e5_base\": \"embaas/sentence-transformers-multilingual-e5-base\",\n",
    "    # \"e5_large\":         \"embaas/sentence-transformers-multilingual-e5-large\",\n",
    "    # \"simcse_xlmr_base\": \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "    # \"sbert_bert\":       \"jegormeister/bert-base-dutch-cased-snli\",\n",
    "}\n",
    "SELECTED_EMBEDDINGS: List[str] = [\"e5_base\"]  # choose from EMBEDDING_SPECS\n",
    "for k in SELECTED_EMBEDDINGS:\n",
    "    if k not in EMBEDDING_SPECS:\n",
    "        raise ValueError(f\"Unknown embedding alias: '{k}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "\n",
    "# Modeling\n",
    "ALL_MODELS: List[str] = [\"chain_ERCcv_lr\"]   # others available: \"local_lasso\", \"local_rf\", \"chain_ERCcv_rf\", \"global_rf\"\n",
    "MODELS_TO_TUNE: List[str] = [\"chain_ERCcv_lr\"]  # subset of ALL_MODELS\n",
    "\n",
    "# Text/encoder limits\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Folds to run: \"all\", \"0,1,2\", \"10-20\"\n",
    "FOLD_SPEC: str = \"all\"\n",
    "\n",
    "# Script e_1 inputs (optional filters)\n",
    "INCLUDE_G_FILES: List[str] = []  # e.g. [\"g_final_n192_gemma.csv\"]\n",
    "GLOB_OVERRIDE: str = \"\"          # e.g. \"g_final_n*_gemma.csv\"\n",
    "\n",
    "# Outputs/serialization\n",
    "WRITE_JSONL        = True\n",
    "SAVE_FOLD_TEACHER  = True\n",
    "\n",
    "# Cores\n",
    "N_CORES = 6\n",
    "\n",
    "# Common target columns used in prior steps\n",
    "TARGET_COLS = [f\"domain{i}\" for i in range(1, 15)]\n",
    "\n",
    "print(f\"[e_2 (per-fold tuned)] Using device: {DEVICE_STR} | N_CORES={N_CORES}\")\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  0. Sentence encoders\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "_ST_MODELS: Dict[str, SentenceTransformer] = {}\n",
    "\n",
    "def get_encoder(emb_key: str) -> SentenceTransformer:\n",
    "    \"\"\"Load/cache a SentenceTransformer for the given embedding alias.\"\"\"\n",
    "    if emb_key in _ST_MODELS:\n",
    "        return _ST_MODELS[emb_key]\n",
    "    repo = EMBEDDING_SPECS[emb_key]\n",
    "    log.info(f\"Loading SentenceTransformer [{emb_key}]: {repo} → device={DEVICE_STR}\")\n",
    "    mdl = SentenceTransformer(repo, device=DEVICE_STR)\n",
    "    mdl.max_seq_length = MAX_SEQ_LEN\n",
    "    _ST_MODELS[emb_key] = mdl\n",
    "    return mdl\n",
    "\n",
    "def encode_texts(emb_key: str, texts: List[str], batch_size: int = 64,\n",
    "                 show_progress_bar: bool = False) -> np.ndarray:\n",
    "    mdl = get_encoder(emb_key)\n",
    "    vecs = mdl.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=show_progress_bar,\n",
    "        normalize_embeddings=False\n",
    "    )\n",
    "    return vecs.astype(np.float32)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  1. Chain models\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "class RegressorChainCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Chain regressor with out-of-fold (OOF) meta-features per target.\n",
    "    Baseline-aligned RNG: check_random_state(...) and pass rng into KFold.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator, order=None, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.order = order\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.chain_models_ = []\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples, self.n_targets_ = Y.shape\n",
    "        if self.order is None:\n",
    "            self.order = np.arange(self.n_targets_)\n",
    "        kf = KFold(n_splits=self.cv_splits, shuffle=True, random_state=rng)\n",
    "\n",
    "        # OOF predictions appended as features\n",
    "        oof_cols = []\n",
    "        X_chain = np.copy(X)\n",
    "        for target_idx in self.order:\n",
    "            y = Y[:, target_idx]\n",
    "            oof = np.zeros(n_samples)\n",
    "            for tr, va in kf.split(X_chain):\n",
    "                m = clone(self.base_estimator)\n",
    "                m.fit(X_chain[tr], y[tr])\n",
    "                oof[va] = m.predict(X_chain[va])\n",
    "            oof = oof.reshape(-1, 1)\n",
    "            oof_cols.append(oof)\n",
    "            X_chain = np.hstack([X_chain, oof])\n",
    "\n",
    "        # Final per-target models trained on full data with accumulated OOF cols\n",
    "        self.chain_models_ = []\n",
    "        X_full = np.copy(X)\n",
    "        acc = []\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            if i > 0:\n",
    "                acc.append(oof_cols[i-1])\n",
    "                X_full = np.hstack([X, np.hstack(acc)])\n",
    "            m = clone(self.base_estimator)\n",
    "            m.fit(X_full, Y[:, target_idx])\n",
    "            self.chain_models_.append(m)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ext = np.copy(X)\n",
    "        n = X.shape[0]\n",
    "        Yh = np.zeros((n, self.n_targets_), dtype=float)\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            m = self.chain_models_[i]\n",
    "            yhat = m.predict(X_ext).reshape(-1, 1)\n",
    "            Yh[:, target_idx] = yhat[:, 0]\n",
    "            X_ext = np.hstack([X_ext, yhat])\n",
    "        return Yh\n",
    "\n",
    "\n",
    "class EnsembleRegressorChainsCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Ensemble of RegressorChainCV with random target orders; predictions averaged.\n",
    "    Baseline-aligned RNG via check_random_state(...).\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator, n_chains=5, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_chains = n_chains\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.ensemble_ = None\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self.n_targets_ = Y.shape[1]\n",
    "        self.ensemble_ = []\n",
    "        for _ in range(self.n_chains):\n",
    "            order = np.arange(self.n_targets_)\n",
    "            rng.shuffle(order)\n",
    "            chain = RegressorChainCV(\n",
    "                self.base_estimator,\n",
    "                order=order,\n",
    "                cv_splits=self.cv_splits,\n",
    "                random_state=rng.randint(0, 1_000_000)\n",
    "            )\n",
    "            chain.fit(X, Y)\n",
    "            self.ensemble_.append((order, chain))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [chain.predict(X) for (_, chain) in self.ensemble_]\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  3. Data loaders & caches\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_data_min() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rebuild data_merged with domain1..domain14 + 'question' in the same way\n",
    "    as earlier steps, to ensure Y aligns with cached seed vectors.\n",
    "    Sorted by activity_id for deterministic fold order.\n",
    "    \"\"\"\n",
    "    activity_scores_path = DATA_DIR / \"activity_scores.csv\"\n",
    "    activities_path      = DATA_DIR / \"activities.csv\"\n",
    "\n",
    "    df_scores     = pd.read_csv(activity_scores_path)\n",
    "    df_activities = pd.read_csv(activities_path)\n",
    "\n",
    "    dm = df_scores.pivot(index=\"activity_id\", columns=\"domain_id\", values=\"score\").reset_index()\n",
    "    dm = dm.rename(columns=lambda x: f\"domain{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "    dm = pd.merge(dm, df_activities[[\"activity_id\", \"question\"]], on=\"activity_id\", how=\"left\")\n",
    "    dm = dm.sort_values(\"activity_id\").reset_index(drop=True)\n",
    "    return dm\n",
    "\n",
    "def ensure_seed_vectors(dm: pd.DataFrame, emb_key: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load seed embeddings for `emb_key` from disk. If missing:\n",
    "      - STATIC (word2vec/fasttext): require precomputed baseline vectors (a_static).\n",
    "      - FROZEN (all others): compute via HF encoder and save under b_frozen/results.\n",
    "    \"\"\"\n",
    "    expected_rows = len(dm)\n",
    "\n",
    "    # Candidate locations, in order of preference\n",
    "    candidates: List[Path] = []\n",
    "    if emb_key in {\"word2vec\", \"fasttext\"}:\n",
    "        candidates.append(STATIC_RESULTS_DIR / f\"baseline_{emb_key}_vectors.npy\")  \n",
    "    else:\n",
    "        candidates.append(FROZEN_RESULTS_DIR / f\"{emb_key}_vectors.npy\")          \n",
    "\n",
    "    # Try to load from disk\n",
    "    for vec_path in candidates:\n",
    "        if vec_path.exists():\n",
    "            try:\n",
    "                X = np.load(vec_path, mmap_mode=\"r\")\n",
    "                if X.shape[0] == expected_rows:\n",
    "                    log.info(\"✔ [%s] Loaded cached seed embeddings (%d×%s) from %s\",\n",
    "                             emb_key, X.shape[0],\n",
    "                             X.shape[1] if X.ndim == 2 else \"?\",  # defensive\n",
    "                             vec_path.relative_to(ROOT))\n",
    "                    return np.array(X, dtype=np.float32, copy=True)  # drop mmap\n",
    "                else:\n",
    "                    log.warning(\"↻ [%s] Cached rows (%d) != expected seeds (%d) at %s; continuing search.\",\n",
    "                                emb_key, X.shape[0], expected_rows, vec_path.relative_to(ROOT))\n",
    "            except Exception as e:\n",
    "                log.warning(\"↻ [%s] Could not read cached seed vectors (%s): %s; continuing search.\",\n",
    "                            emb_key, vec_path.name, e)\n",
    "\n",
    "    # Not found / invalid → compute only for frozen\n",
    "    if emb_key in {\"word2vec\", \"fasttext\"}:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Static seed vectors for '{emb_key}' not found. Expected at \"\n",
    "            f\"{(STATIC_RESULTS_DIR / f'baseline_{emb_key}_vectors.npy').relative_to(ROOT)}. \"\n",
    "            \"Please run `a_static.ipynb` in compute mode first.\"\n",
    "        )\n",
    "\n",
    "    # Compute frozen seed vectors now and cache under b_frozen/results\n",
    "    log.info(\"… [%s] Cached seed vectors not found/valid → encoding %d seeds via HF model.\",\n",
    "             emb_key, expected_rows)\n",
    "    texts = dm[\"question\"].astype(str).fillna(\"\").tolist()\n",
    "    X = encode_texts(emb_key, texts, batch_size=64, show_progress_bar=True).astype(np.float32)\n",
    "\n",
    "    out_path = FROZEN_RESULTS_DIR / f\"{emb_key}_vectors.npy\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(out_path, X)\n",
    "    log.info(\"✔ [%s] Saved seed vectors → %s\", emb_key, out_path.relative_to(ROOT))\n",
    "    return X\n",
    "\n",
    "def discover_g1_sources() -> List[Path]:\n",
    "    paths = sorted(G1_DIR.glob(GLOB_OVERRIDE)) if GLOB_OVERRIDE else sorted(G1_DIR.glob(\"g_final_n*_*.csv\"))\n",
    "    if INCLUDE_G_FILES:\n",
    "        names = set(INCLUDE_G_FILES)\n",
    "        paths = [p for p in paths if p.name in names]\n",
    "    return [p for p in paths if p.suffix.lower() == \".csv\" and p.stat().st_size]\n",
    "\n",
    "def synth_cache_paths(src: Path, emb_key: str) -> tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Return (X_synth_npy_path, index_csv_path) for a Script e_1 CSV and embedding key,\n",
    "    using the dedicated cache folder only (for strict reproducibility).\n",
    "    \"\"\"\n",
    "    stem = src.stem  # e.g., g_final_n384_gemma\n",
    "    return (\n",
    "        CACHE_DIR / f\"{stem}__{emb_key}.npy\",\n",
    "        CACHE_DIR / f\"{stem}__index.csv\"\n",
    "    )\n",
    "\n",
    "def ensure_synth_embeddings(src: Path, emb_key: str) -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For a Script e_1 CSV, ensure we have:\n",
    "      • X_synth npy cache (emb_key vectors)\n",
    "      • index CSV (provenance + text)\n",
    "    \"\"\"\n",
    "    X_path, idx_path = synth_cache_paths(src, emb_key)\n",
    "    if idx_path.exists() and X_path.exists():\n",
    "        try:\n",
    "            df_idx = pd.read_csv(idx_path)\n",
    "            X = np.load(X_path)\n",
    "            if len(df_idx) == X.shape[0] and \"text\" in df_idx.columns:\n",
    "                return X, df_idx\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    df = pd.read_csv(src)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(f\"{src.name}: missing 'text' column.\")\n",
    "\n",
    "    texts = df[\"text\"].astype(str).fillna(\"\").tolist()\n",
    "    log.info(\"[%s] Computing synthetic embeddings (%s): %d rows\", emb_key, src.name, len(texts))\n",
    "    X = encode_texts(emb_key, texts, batch_size=64, show_progress_bar=True)\n",
    "    df_idx = df.copy()\n",
    "\n",
    "    np.save(X_path, X)\n",
    "    df_idx.to_csv(idx_path, index=False)\n",
    "    log.info(\"✔ [%s] Cached synth embeddings/index → %s | %s\",\n",
    "             emb_key, X_path.relative_to(ROOT), idx_path.relative_to(ROOT))\n",
    "    return X, df_idx\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  4. Fold utilities & naming\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def parse_folds(n_seeds: int, spec: str = FOLD_SPEC) -> List[int]:\n",
    "    s = (spec or \"all\").strip().lower()\n",
    "    if s in {\"\", \"all\", \"alles\"}:\n",
    "        return list(range(n_seeds))\n",
    "    parts = [p.strip() for p in spec.split(\",\") if p.strip()]\n",
    "    out: List[int] = []\n",
    "    for p in parts:\n",
    "        if \"-\" in p:\n",
    "            a, b = p.split(\"-\", 1)\n",
    "            out.extend(list(range(int(a), int(b) + 1)))\n",
    "        else:\n",
    "            out.append(int(p))\n",
    "    out = sorted(set(i for i in out if 0 <= i < n_seeds))\n",
    "    return out or list(range(n_seeds))\n",
    "\n",
    "def out_base_for(src: Path) -> str:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      g_final_n384_gemma.csv  →  base = n384_gemma\n",
    "    \"\"\"\n",
    "    m = re.match(r\"g_final_(n\\d+)_([A-Za-z0-9_]+)\\.csv$\", src.name)\n",
    "    if m:\n",
    "        size_tag, method_tag = m.group(1), m.group(2)\n",
    "        base = f\"{size_tag}_{method_tag}\"\n",
    "    else:\n",
    "        base = src.stem\n",
    "    return base\n",
    "\n",
    "def out_paths_for_fold(src: Path, fold_idx: int, model_key: str, emb_key: str) -> Tuple[Path, Path]:\n",
    "    base = out_base_for(src)\n",
    "    csv_out  = G2_DIR / f\"g2f_labels_fold{fold_idx:02d}_{base}__{emb_key}__{model_key}.csv\"\n",
    "    json_out = G2_DIR / f\"g2f_labels_fold{fold_idx:02d}_{base}__{emb_key}__{model_key}.jsonl\"\n",
    "    return csv_out, json_out\n",
    "\n",
    "def hp_json_path(fold_idx: int, model_key: str, emb_key: str) -> Path:\n",
    "    return TEACHER_HP_DIR / f\"hp_fold{fold_idx:02d}_{emb_key}__{model_key}.json\"\n",
    "\n",
    "def teacher_pkl_path(fold_idx: int, model_key: str, emb_key: str) -> Path:\n",
    "    return MODELS_TEACHER_DIR / f\"teacher_fold{fold_idx:02d}_{emb_key}__{model_key}.pkl\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  5. Pipelines & grids\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def make_pipeline(model_key: str) -> Pipeline:\n",
    "    if model_key == \"local_lasso\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED)),\n",
    "            (\"reg\", MultiOutputRegressor(Lasso(random_state=SEED, max_iter=10_000))),\n",
    "        ])\n",
    "    elif model_key == \"local_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED)),\n",
    "            (\"reg\", MultiOutputRegressor(RandomForestRegressor(random_state=SEED, n_jobs=1))),\n",
    "        ])\n",
    "    elif model_key == \"global_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED)),\n",
    "            (\"reg\", RandomForestRegressor(random_state=SEED, n_jobs=1)),\n",
    "        ])\n",
    "    elif model_key == \"chain_ERCcv_lr\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED)),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(base_estimator=LinearRegression(), random_state=SEED)),\n",
    "        ])\n",
    "    elif model_key == \"chain_ERCcv_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED)),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(\n",
    "                base_estimator=RandomForestRegressor(random_state=SEED, n_jobs=1), random_state=SEED\n",
    "            )),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_key: {model_key}\")\n",
    "\n",
    "def small_grid(model_key: str) -> Dict[str, List]:\n",
    "    if model_key == \"local_lasso\":\n",
    "        return {\n",
    "            \"pca__n_components\": [0.7, 0.8, 0.9],\n",
    "            \"reg__estimator__alpha\": [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        }\n",
    "    elif model_key == \"local_rf\":\n",
    "        return {\n",
    "            \"pca__n_components\": [0.7, 0.8, 0.9],\n",
    "            \"reg__estimator__n_estimators\": [50, 100],\n",
    "            \"reg__estimator__max_depth\": [None, 5, 10],\n",
    "            \"reg__estimator__min_samples_leaf\": [1],\n",
    "        }\n",
    "    elif model_key == \"global_rf\":\n",
    "        return {\n",
    "            \"pca__n_components\": [0.7, 0.8, 0.9],\n",
    "            \"reg__n_estimators\": [50, 100],\n",
    "            \"reg__max_depth\": [None, 5, 10],\n",
    "            \"reg__min_samples_leaf\": [1],\n",
    "        }\n",
    "    elif model_key == \"chain_ERCcv_lr\":\n",
    "        return {\n",
    "            \"pca__n_components\": [0.7, 0.8, 0.9],\n",
    "            \"chain__n_chains\": [3, 5],\n",
    "            \"chain__cv_splits\": [3, 5],\n",
    "        }\n",
    "    elif model_key == \"chain_ERCcv_rf\":\n",
    "        # \"small\" grid: do not tune RF inner hyperparams here (runtime-friendly)\n",
    "        return {\n",
    "            \"pca__n_components\": [0.7, 0.8, 0.9],\n",
    "            \"chain__n_chains\": [3, 5],\n",
    "            \"chain__cv_splits\": [3, 5],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_key: {model_key}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6. HP tuning (per fold) and teacher build/load\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def ensure_best_params_for_fold(model_key: str, X_seed: np.ndarray, Y_seed: np.ndarray,\n",
    "                                train_idx: np.ndarray, fold_idx: int, emb_key: str) -> Dict[str, object]:\n",
    "    \"\"\"Return best_params dict for (embedding, model, fold), reusing JSON if present.\"\"\"\n",
    "    path = hp_json_path(fold_idx, model_key, emb_key)\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                payload = json.load(f)\n",
    "            if (payload.get(\"model\") == model_key and\n",
    "                payload.get(\"fold\") == int(fold_idx) and\n",
    "                payload.get(\"embedding\") == emb_key and\n",
    "                \"best_params\" in payload):\n",
    "                return payload[\"best_params\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Tune with GridSearchCV (default R² scoring) on the 95 training seeds\n",
    "    pipe = make_pipeline(model_key)\n",
    "    grid = small_grid(model_key)\n",
    "    ncomb = 1\n",
    "    for v in grid.values(): ncomb *= len(v)\n",
    "    log.info(f\"  [{emb_key}] [Fold {fold_idx:02d} | {model_key}] Tuning {ncomb} combos via GridSearchCV…\")\n",
    "    gs = GridSearchCV(pipe, param_grid=grid, cv=5, n_jobs=1, refit=True)\n",
    "    gs.fit(X_seed[train_idx], Y_seed[train_idx])\n",
    "    best_params = gs.best_params_\n",
    "    log.info(f\"    [{emb_key}] Best params: {best_params}\")\n",
    "\n",
    "    # Persist per-fold HP JSON (idempotency)\n",
    "    payload = {\n",
    "        \"embedding\": emb_key,\n",
    "        \"model\": model_key,\n",
    "        \"fold\": int(fold_idx),\n",
    "        \"best_params\": best_params,\n",
    "        \"selection_metric\": \"GridSearchCV default R² (cv=5) on fold training seeds\",\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "    return best_params\n",
    "\n",
    "def load_or_fit_teacher(model_key: str, X_seed: np.ndarray, Y_seed: np.ndarray,\n",
    "                        train_idx: np.ndarray, fold_idx: int, emb_key: str,\n",
    "                        save_pickle: bool = True):\n",
    "    \"\"\"\n",
    "    Load teacher pickle for (embedding, model, fold) if available; else fit using\n",
    "    per-fold best params and return.\n",
    "    \"\"\"\n",
    "    tp = teacher_pkl_path(fold_idx, model_key, emb_key)\n",
    "    if tp.exists():\n",
    "        try:\n",
    "            with open(tp, \"rb\") as f:\n",
    "                teacher = pickle.load(f)\n",
    "            log.info(f\"    [{emb_key}] Loaded cached teacher for fold {fold_idx:02d}, model={model_key}\")\n",
    "            return teacher\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Ensure best params, then fit on fold's training seeds\n",
    "    best = ensure_best_params_for_fold(model_key, X_seed, Y_seed, train_idx, fold_idx, emb_key)\n",
    "    pipe = make_pipeline(model_key)\n",
    "    pipe.set_params(**best)\n",
    "    pipe.fit(X_seed[train_idx], Y_seed[train_idx])\n",
    "\n",
    "    if save_pickle and SAVE_FOLD_TEACHER:\n",
    "        try:\n",
    "            with open(tp, \"wb\") as f:\n",
    "                pickle.dump(pipe, f)\n",
    "            log.info(f\"    [{emb_key}] Saved teacher fold {fold_idx:02d} → {tp.relative_to(ROOT)}\")\n",
    "        except Exception as e:\n",
    "            log.warning(f\"    [{emb_key}] Could not save teacher fold {fold_idx:02d}: {e}\")\n",
    "    return pipe\n",
    "\n",
    "def label_with_teacher(X_synth: np.ndarray, df_index: pd.DataFrame, teacher) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict 14 targets for X_synth; return df with original columns + domain1..domain14.\n",
    "    \"\"\"\n",
    "    Y_hat = teacher.predict(X_synth)  # (n, 14)\n",
    "    if Y_hat.ndim != 2 or Y_hat.shape[1] != 14:\n",
    "        raise RuntimeError(f\"Teacher returned shape {Y_hat.shape}, expected (n, 14).\")\n",
    "    out = df_index.copy()\n",
    "    for i, col in enumerate(TARGET_COLS):\n",
    "        out[col] = Y_hat[:, i].astype(float)\n",
    "    return out\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  7. Main functions\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def run_pipeline_compute():\n",
    "    \"\"\"Full pipeline (tuning + optional teacher fitting + labeling).\"\"\"\n",
    "    log.info(\"=== step e_2 (per-fold tuned; multi-embedding) started ===\")\n",
    "    log.info(f\"Embeddings to use: {SELECTED_EMBEDDINGS}\")\n",
    "    log.info(f\"Models to tune/label: {MODELS_TO_TUNE}\")\n",
    "\n",
    "    # 0) Load seeds table (texts + targets)\n",
    "    dm = load_data_min()\n",
    "    Y_seed = dm[TARGET_COLS].to_numpy(dtype=float)\n",
    "    n_seeds = len(dm)\n",
    "    assert n_seeds == 96, f\"Expected 96 seeds, found {n_seeds}.\"\n",
    "\n",
    "    # 1) (guarded) Discover Script e_1 inputs\n",
    "    if not TUNE_ONLY:\n",
    "        sources = discover_g1_sources()\n",
    "        if not sources:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No inputs found under: {G1_DIR} \"\n",
    "                f\"(look for g_final_n*_*.csv; optionally use GLOB_OVERRIDE / INCLUDE_G_FILES).\"\n",
    "            )\n",
    "        log.info(f\"Found {len(sources)} input file(s) from step e_1.\")\n",
    "    else:\n",
    "        sources = []  # not used in tune-only\n",
    "\n",
    "    # 2) Which folds to run?\n",
    "    fold_list = parse_folds(n_seeds, FOLD_SPEC)\n",
    "    log.info(f\"Running folds: {fold_list[0]}..{fold_list[-1]} (total {len(fold_list)})\")\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    # 3) Process each embedding independently (full idempotency per embedding)\n",
    "    for emb_key in SELECTED_EMBEDDINGS:\n",
    "        log.info(f\"— Embedding: {emb_key}\")\n",
    "\n",
    "        # Seed vectors for this embedding\n",
    "        X_seed = ensure_seed_vectors(dm, emb_key)\n",
    "\n",
    "        # ── Always tune per (model, fold) to ensure HP JSON exists\n",
    "        for model_key in MODELS_TO_TUNE:\n",
    "            log.info(f\"  Tuning per-fold HPs for model: {model_key}\")\n",
    "            for fold_idx in fold_list:\n",
    "                train_idx = np.array([i for i in range(n_seeds) if i != fold_idx], dtype=int)\n",
    "                _ = ensure_best_params_for_fold(\n",
    "                    model_key, X_seed, Y_seed, train_idx, fold_idx, emb_key\n",
    "                )\n",
    "                # Optional: show where it went\n",
    "                log.info(f\"    ✔ [{emb_key}] fold {fold_idx:02d} HPs saved → \"\n",
    "                         f\"{hp_json_path(fold_idx, model_key, emb_key).relative_to(ROOT)}\")\n",
    "\n",
    "        # ── Tune-only guard: stop before fitting teachers / labeling synthetics\n",
    "        if TUNE_ONLY:\n",
    "            log.info(\"[tune-only] Skipping teacher pickles and synthetic labeling.\")\n",
    "            clear_gpu_memory()\n",
    "            # Write a small manifest and exit early\n",
    "            manifest = {\n",
    "                \"embeddings\": SELECTED_EMBEDDINGS,\n",
    "                \"embedding_repos\": {k: EMBEDDING_SPECS[k] for k in SELECTED_EMBEDDINGS},\n",
    "                \"models\": MODELS_TO_TUNE,\n",
    "                \"folds_used\": len(fold_list),\n",
    "                \"device\": DEVICE_STR,\n",
    "                \"n_cores\": N_CORES,\n",
    "                \"labels_root\": str(G2_DIR.relative_to(ROOT)),\n",
    "                \"e1_root\": str(G1_DIR.relative_to(ROOT)),\n",
    "                \"hp_cache_pattern\": \"outputs/e_2_teacher_labeling/teacher/hp_fold{ii}_{embedding}__{model}.json\",\n",
    "                \"teacher_pickles_pattern\": \"models/teacher/teacher_fold{ii}_{embedding}__{model}.pkl\",\n",
    "                \"results_roots\": {\n",
    "                    \"static\": str(STATIC_RESULTS_DIR.relative_to(ROOT)),\n",
    "                    \"frozen\": str(FROZEN_RESULTS_DIR.relative_to(ROOT))\n",
    "                },\n",
    "                \"seed_vectors_patterns\": {\n",
    "                    \"static\": \"outputs/a_static/results/baseline_{embedding}_vectors.npy\",\n",
    "                    \"frozen\": \"outputs/b_frozen/results/{embedding}_vectors.npy\"\n",
    "                },\n",
    "                \"synth_vectors_cache_pattern\": \"outputs/e_2_teacher_labeling/cache/synth_embeds/{stem}__{embedding}.npy\",\n",
    "                \"tune_only\": True,\n",
    "            }\n",
    "            with open(G2_DIR / \"run_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(manifest, f, indent=2)\n",
    "            log.info(\"=== step e_2 (tune-only) completed ===\")\n",
    "            print(\"Done (tune-only). Artifacts in:\", G2_DIR)\n",
    "            return  # ← STOP SCRIPT HERE\n",
    "\n",
    "        # ── (normal path) Pre-compute/load teachers per (model, fold) — reused across all sources\n",
    "        teachers: Dict[Tuple[str, int], object] = {}\n",
    "        for model_key in MODELS_TO_TUNE:\n",
    "            log.info(f\"  Preparing teachers for model: {model_key}\")\n",
    "            for fold_idx in fold_list:\n",
    "                train_idx = np.array([i for i in range(n_seeds) if i != fold_idx], dtype=int)\n",
    "                t0 = time.time()\n",
    "                teacher_i = load_or_fit_teacher(\n",
    "                    model_key, X_seed, Y_seed, train_idx, fold_idx, emb_key, save_pickle=True\n",
    "                )\n",
    "                teachers[(model_key, fold_idx)] = teacher_i\n",
    "                log.info(f\"    ✔ [{emb_key}] fold {fold_idx:02d} teacher ready in {time.time()-t0:.1f}s\")\n",
    "                clear_gpu_memory()\n",
    "\n",
    "        # For each source, ensure synth vectors once; then loop folds × models\n",
    "        for s_idx, src in enumerate(sources, 1):\n",
    "            log.info(f\"  [{emb_key}] [{s_idx}/{len(sources)}] Source → {src.relative_to(ROOT)}\")\n",
    "            X_synth, df_index = ensure_synth_embeddings(src, emb_key)\n",
    "            n_synth = X_synth.shape[0]\n",
    "\n",
    "            for fold_idx in fold_list:\n",
    "                for model_key in MODELS_TO_TUNE:\n",
    "                    csv_out, json_out = out_paths_for_fold(src, fold_idx, model_key, emb_key)\n",
    "\n",
    "                    # Idempotent skip if file already complete\n",
    "                    if csv_out.exists():\n",
    "                        try:\n",
    "                            n_rows = sum(1 for _ in open(csv_out, \"r\", encoding=\"utf-8\")) - 1\n",
    "                            if max(0, n_rows) >= n_synth:\n",
    "                                log.info(f\"    ↻ [{emb_key}] {model_key} | fold {fold_idx:02d}: found complete labels ({n_rows}) → skip\")\n",
    "                                summary_rows.append({\n",
    "                                    \"embedding\": emb_key, \"model\": model_key, \"input\": src.name,\n",
    "                                    \"fold\": fold_idx, \"output_csv\": csv_out.name, \"n_synth\": n_synth,\n",
    "                                    \"status\": \"skip_existing\", \"dt_s\": 0.0\n",
    "                                })\n",
    "                                continue\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    # Label with prebuilt teacher\n",
    "                    t0 = time.time()\n",
    "                    teacher_i = teachers[(model_key, fold_idx)]\n",
    "                    df_labeled = label_with_teacher(X_synth, df_index, teacher_i)\n",
    "                    dt = time.time() - t0\n",
    "                    log.info(f\"    ✔ [{emb_key}] {model_key} | fold {fold_idx:02d}: labeled {len(df_labeled)} rows in {dt:.1f}s → {csv_out.name}\")\n",
    "\n",
    "                    # Write CSV (overwrite to keep file atomic & consistent)\n",
    "                    df_labeled.to_csv(csv_out, index=False)\n",
    "\n",
    "                    # Optional JSONL\n",
    "                    if WRITE_JSONL:\n",
    "                        with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "                            for obj in df_labeled.to_dict(orient=\"records\"):\n",
    "                                f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                    clear_gpu_memory()\n",
    "\n",
    "                    # Summarize\n",
    "                    summary_rows.append({\n",
    "                        \"embedding\": emb_key, \"model\": model_key, \"input\": src.name,\n",
    "                        \"fold\": fold_idx, \"output_csv\": csv_out.name, \"n_synth\": n_synth,\n",
    "                        \"status\": \"labeled\", \"dt_s\": round(dt, 3)\n",
    "                    })\n",
    "\n",
    "    # 4) Run summary (only in non-tune-only path; tune-only returned already)\n",
    "    if summary_rows:\n",
    "        cols = [\"embedding\", \"model\", \"input\", \"fold\", \"output_csv\", \"n_synth\", \"status\", \"dt_s\"]\n",
    "        df_sum = pd.DataFrame(summary_rows, columns=cols)\n",
    "        if RUN_SUMMARY.exists() and RUN_SUMMARY.stat().st_size:\n",
    "            df_sum.to_csv(RUN_SUMMARY, mode=\"a\", header=False, index=False)\n",
    "        else:\n",
    "            df_sum.to_csv(RUN_SUMMARY, index=False)\n",
    "\n",
    "    # 5) Manifest (compute path)\n",
    "    manifest = {\n",
    "        \"embeddings\": SELECTED_EMBEDDINGS,\n",
    "        \"embedding_repos\": {k: EMBEDDING_SPECS[k] for k in SELECTED_EMBEDDINGS},\n",
    "        \"models\": MODELS_TO_TUNE,\n",
    "        \"folds_used\": len(fold_list),\n",
    "        \"device\": DEVICE_STR,\n",
    "        \"n_cores\": N_CORES,\n",
    "        \"labels_root\": str(G2_DIR.relative_to(ROOT)),\n",
    "        \"e1_root\": str(G1_DIR.relative_to(ROOT)),\n",
    "        \"hp_cache_pattern\": \"outputs/e_2_teacher_labeling/teacher/hp_fold{ii}_{embedding}__{model}.json\",\n",
    "        \"teacher_pickles_pattern\": \"models/teacher/teacher_fold{ii}_{embedding}__{model}.pkl\",\n",
    "        \"results_roots\": {\n",
    "            \"static\": str(STATIC_RESULTS_DIR.relative_to(ROOT)),\n",
    "            \"frozen\": str(FROZEN_RESULTS_DIR.relative_to(ROOT))\n",
    "        },\n",
    "        \"seed_vectors_patterns\": {\n",
    "            \"static\": \"outputs/a_static/results/baseline_{embedding}_vectors.npy\",\n",
    "            \"frozen\": \"outputs/b_frozen/results/{embedding}_vectors.npy\"\n",
    "        },\n",
    "        \"synth_vectors_cache_pattern\": \"outputs/e_2_teacher_labeling/cache/synth_embeds/{stem}__{embedding}.npy\",\n",
    "        \"tune_only\": False,\n",
    "    }\n",
    "\n",
    "    with open(G2_DIR / \"run_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    log.info(\"=== step e_2 (per-fold tuned; multi-embedding) completed ===\")\n",
    "    print(\"Done. Artifacts in:\", G2_DIR)\n",
    "\n",
    "\n",
    "_LABEL_RE = re.compile(\n",
    "    r\"^g2f_labels_fold(?P<fold>\\d{2})_(?P<base>.+?)__(?P<emb>[A-Za-z0-9_]+)__(?P<model>[A-Za-z0-9_]+)\\.csv$\"\n",
    ")\n",
    "\n",
    "def _parse_label_file(p: Path):\n",
    "    m = _LABEL_RE.match(p.name)\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"fold\"]  = int(d[\"fold\"])\n",
    "    d[\"path\"]  = str(p)\n",
    "    return d\n",
    "\n",
    "def _count_csv_rows(path: Path) -> int:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return max(0, sum(1 for _ in f) - 1)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def run_pipeline_review():\n",
    "    \"\"\"\n",
    "    Fast, artifact-only review:\n",
    "      - Lists label CSVs by filename.\n",
    "      - Gets expected_rows per base from cache index\n",
    "        'cache/synth_embeds/g_final_{base}__index.csv'.\n",
    "      - Checks presence of synth .npy, HP JSONs, and teacher pickles.\n",
    "      - Writes run_summary.csv and run_config.json.\n",
    "    \"\"\"\n",
    "    log.info(\"=== step e_2 REVIEW_MODE: artifact-only scan started ===\")\n",
    "\n",
    "    read_root = CACHE_DIR\n",
    "    try:\n",
    "        log.info(\"Using synth cache dir: %s\", read_root.relative_to(ROOT))\n",
    "    except Exception:\n",
    "        log.info(\"Using synth cache dir: %s\", read_root)\n",
    "\n",
    "    label_files = sorted([p for p in G2_DIR.glob(\"g2f_labels_fold*__*.csv\") if p.stat().st_size])\n",
    "    if not label_files:\n",
    "        # Write empty summary & a minimal but informative manifest\n",
    "        pd.DataFrame(columns=[\n",
    "            \"embedding\", \"model\", \"fold\", \"base\", \"output_csv\",\n",
    "            \"expected_rows\", \"has_index\", \"has_synth_npy\", \"has_hp_json\", \"has_teacher_pkl\", \"status\"\n",
    "        ]).to_csv(RUN_SUMMARY, index=False)\n",
    "        manifest = {\n",
    "            \"review_mode\": True,\n",
    "            \"labels_root\": str(G2_DIR.relative_to(ROOT)),\n",
    "            \"synth_cache_dir\": str(read_root.relative_to(ROOT)),\n",
    "            \"results_roots\": {\n",
    "                \"static\": str(STATIC_RESULTS_DIR.relative_to(ROOT)),\n",
    "                \"frozen\": str(FROZEN_RESULTS_DIR.relative_to(ROOT))\n",
    "            },\n",
    "            \"seed_vectors_patterns\": {\n",
    "                \"static\": \"outputs/a_static/results/baseline_{embedding}_vectors.npy\",\n",
    "                \"frozen\": \"outputs/b_frozen/results/{embedding}_vectors.npy\"\n",
    "            },\n",
    "            \"embeddings_discovered\": [],\n",
    "            \"models_discovered\": [],\n",
    "            \"folds_discovered\": [],\n",
    "            \"fast_scan\": True,\n",
    "            \"bases_indexed\": 0,\n",
    "            \"label_files\": 0,\n",
    "        }\n",
    "        (G2_DIR / \"run_config.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "        log.info(\"✔ Wrote empty run_summary.csv (no labels found)\")\n",
    "        print(\"Review complete. Summary in:\", RUN_SUMMARY)\n",
    "        return\n",
    "\n",
    "    # Parse filenames and collect unique bases (e.g., 'n384_gemma')\n",
    "    metas, bases = [], set()\n",
    "    _LABEL_RE = re.compile(\n",
    "        r\"^g2f_labels_fold(?P<fold>\\d{2})_(?P<base>.+?)__(?P<emb>[A-Za-z0-9_]+)__(?P<model>[A-Za-z0-9_]+)\\.csv$\"\n",
    "    )\n",
    "    for p in label_files:\n",
    "        m = _LABEL_RE.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        d = m.groupdict()\n",
    "        d[\"fold\"] = int(d[\"fold\"])\n",
    "        d[\"path\"] = str(p)\n",
    "        metas.append(d)\n",
    "        bases.add(d[\"base\"])\n",
    "\n",
    "    # For each base, read expected row count ONCE from its index.csv (small)\n",
    "    def _idx_path_for(base: str) -> Path:\n",
    "        return read_root / f\"g_final_{base}__index.csv\"\n",
    "\n",
    "    expected_by_base: Dict[str, Optional[int]] = {}\n",
    "    for b in bases:\n",
    "        idx = _idx_path_for(b)\n",
    "        try:\n",
    "            expected_by_base[b] = (\n",
    "                max(0, sum(1 for _ in open(idx, \"r\", encoding=\"utf-8\", errors=\"ignore\")) - 1)\n",
    "                if idx.exists() else None\n",
    "            )\n",
    "        except Exception:\n",
    "            expected_by_base[b] = None\n",
    "\n",
    "    rows, emb_set, model_set, fold_set = [], set(), set(), set()\n",
    "    for meta in metas:\n",
    "        fold = meta[\"fold\"]\n",
    "        base = meta[\"base\"]\n",
    "        emb = meta[\"emb\"]\n",
    "        model = meta[\"model\"]\n",
    "        p = Path(meta[\"path\"])\n",
    "\n",
    "        emb_set.add(emb)\n",
    "        model_set.add(model)\n",
    "        fold_set.add(fold)\n",
    "        idx_name = f\"g_final_{base}__index.csv\"\n",
    "        npy_name = f\"g_final_{base}__{emb}.npy\"\n",
    "\n",
    "        has_index = (read_root / idx_name).exists()\n",
    "        has_npy = (read_root / npy_name).exists()\n",
    "        hp_ok = (TEACHER_HP_DIR / f\"hp_fold{fold:02d}_{emb}__{model}.json\").exists()\n",
    "        tp_ok = (MODELS_TEACHER_DIR / f\"teacher_fold{fold:02d}_{emb}__{model}.pkl\").exists()\n",
    "\n",
    "        rows.append({\n",
    "            \"embedding\": emb,\n",
    "            \"model\": model,\n",
    "            \"fold\": fold,\n",
    "            \"base\": base,\n",
    "            \"output_csv\": p.name,\n",
    "            \"expected_rows\": expected_by_base.get(base, None),  # from index.csv (fast, reliable)\n",
    "            \"has_index\": has_index,\n",
    "            \"has_synth_npy\": has_npy,\n",
    "            \"has_hp_json\": hp_ok,\n",
    "            \"has_teacher_pkl\": tp_ok,\n",
    "            \"status\": \"present\"\n",
    "        })\n",
    "\n",
    "    df = (pd.DataFrame(rows, columns=[\n",
    "        \"embedding\", \"model\", \"fold\", \"base\", \"output_csv\",\n",
    "        \"expected_rows\", \"has_index\", \"has_synth_npy\", \"has_hp_json\", \"has_teacher_pkl\", \"status\"\n",
    "    ]).sort_values([\"embedding\", \"model\", \"fold\", \"base\", \"output_csv\"]))\n",
    "    df.to_csv(RUN_SUMMARY, index=False)\n",
    "    log.info(\"✔ Wrote run_summary.csv with %d rows\", len(df))\n",
    "\n",
    "    manifest = {\n",
    "        \"review_mode\": True,\n",
    "        \"labels_root\": str(G2_DIR.relative_to(ROOT)),\n",
    "        \"synth_cache_dir\": str(read_root.relative_to(ROOT)),\n",
    "        \"results_roots\": {\n",
    "            \"static\": str(STATIC_RESULTS_DIR.relative_to(ROOT)),\n",
    "            \"frozen\": str(FROZEN_RESULTS_DIR.relative_to(ROOT))\n",
    "        },\n",
    "        \"seed_vectors_patterns\": {\n",
    "            \"static\": \"outputs/a_static/results/baseline_{embedding}_vectors.npy\",\n",
    "            \"frozen\": \"outputs/b_frozen/results/{embedding}_vectors.npy\"\n",
    "        },\n",
    "        \"embeddings_discovered\": sorted(emb_set),\n",
    "        \"models_discovered\": sorted(model_set),\n",
    "        \"folds_discovered\": sorted(fold_set),\n",
    "        \"fast_scan\": True,\n",
    "        \"bases_indexed\": len(bases),\n",
    "        \"label_files\": len(metas),\n",
    "    }\n",
    "    (G2_DIR / \"run_config.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    log.info(\"=== step e_2 REVIEW_MODE completed ===\")\n",
    "    print(\"Review complete. Summary in:\", RUN_SUMMARY)\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  8. Entry-point\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        if REVIEW_MODE:\n",
    "            run_pipeline_review()\n",
    "        else:\n",
    "            run_pipeline_compute()\n",
    "        print(\"Run completed.\")\n",
    "    except Exception as e:\n",
    "        log.exception(\"Fatal error in step e_2: %s\", e)\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36962da-01c1-4a3e-bab6-21630ef058d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr8cht_embedding_comparison",
   "language": "python",
   "name": "kr8cht_embedding_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
