{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9b52c5-afdd-44ea-a111-f887323e9edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kr8cht_review_anonymous/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/kr8cht_review_anonymous/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "2025-10-17 01:54:47 INFO: run_config.json written.\n",
      "[review] Summaries written. (combined, per label-source, legacy-compat files)\n",
      "Run completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_3_student_scoring_b.ipynb\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "LOOCV student scoring with per-fold hyperparameters reused from teacher models\n",
    "(no tuning) and percent-based augmentation sizes. Student embedding is selectable;\n",
    "labels are read from a configurable teacher label source.\n",
    "\n",
    "This script:\n",
    "1) Loads seed data and resources\n",
    "   - Reads 96 seeds (texts + 14 targets).\n",
    "   - Loads or computes seed sentence embeddings for the selected student embedding.\n",
    "   - Discovers augmentation methods and their M_max from tuned teacher outputs.\n",
    "   - Preloads synthetic text embeddings for the selected student embedding and\n",
    "     per-fold tuned labels from the label source (e.g., e5_base + chain_ERCcv_lr).\n",
    "\n",
    "2) Builds student regressors from teacher hyperparameters (no tuning)\n",
    "   - Students: chain_ERCcv_lr, local_lasso, local_rf, global_rf, chain_ERCcv_rf.\n",
    "   - Extracts per-fold HPs from teacher pickles (step_g_2a) for the same\n",
    "     student embedding; raises an error if missing (no silent defaults).\n",
    "\n",
    "3) Evaluates the baseline (seeds-only)\n",
    "   - For each LOOCV fold and student, loads that fold’s teacher estimator\n",
    "     (if present) and predicts the held-out seed; otherwise rebuilds the\n",
    "     same pipeline from the teacher’s stored HPs and predicts.\n",
    "   - Writes per-pct baseline files for naming symmetry.\n",
    "\n",
    "4) Evaluates the augmented “full” variant (seeds + synthetics)\n",
    "   - Percent mode: P ∈ {10, 20, 50, 100, 200, 400}; K = round(P% of 96).\n",
    "   - For each fold, takes the first K items from that fold’s M_max labeled set\n",
    "     (labels from the label source), fits the student with the per-fold HPs,\n",
    "     and predicts the held-out seed.\n",
    "   - Idempotent: computes only missing folds and preserves already complete CSVs.\n",
    "   - Legacy reuse for chain_ERCcv_lr at {100,200,400} applies only when the\n",
    "     student embedding equals the label-source embedding (e.g., e5_base).\n",
    "\n",
    "5) Produces unified summary tables (no PRIMARY/APPENDIX split)\n",
    "   - Aggregates per-fold median RRMSE into a wide summary with columns:\n",
    "     baseline, full, baseline_vs_full.\n",
    "   - Writes a comparison table with ΔRRMSE and relative % change.\n",
    "\n",
    "6) Performs statistical analyses\n",
    "   - Per-config Wilcoxon signed-rank tests (one-sided, alternative=\"less\",\n",
    "     zero_method=\"pratt\") with Holm correction grouped by (regressor, method).\n",
    "   - Paired Cliff’s delta per configuration.\n",
    "   - Pooled Wilcoxon and Cliff’s delta across students per (method, pct).\n",
    "   - Hierarchical bootstrap (folds × domains micro-bootstrap) for Δ median with 95% CI.\n",
    "\n",
    "7) Visualizes performance\n",
    "   - RRMSE vs %K and ΔRRMSE vs %K for the configured PCTS_FOR_PLOTS.\n",
    "\n",
    "Idempotency & disk reuse:\n",
    "- Reuses seed embeddings if shape and max_seq_len match; otherwise rebuilds.\n",
    "- Reuses synthetic embeddings (cache); builds them if missing.\n",
    "- Skips FULL computations for (student, method, pct) whose per-fold CSV is complete.\n",
    "- Optional: skip recomputing baseline if all baseline CSVs are already complete.\n",
    "- Optional: summarization-only mode to build tables/plots from existing CSVs.\n",
    "\n",
    "Inputs:\n",
    "- data/activity_scores.csv\n",
    "- data/activities.csv\n",
    "- outputs/b_frozen/results/{student_embedding}_vectors.npy\n",
    "- outputs/e_1_synth_augmentation/g_final_n{M}_{method}.csv\n",
    "- outputs/e_2_teacher_labeling/g2f_labels_fold{ii}_n{M}_{method}__{label_embedding}__{label_model}.csv\n",
    "- models/teacher/teacher_fold{ii}_{student_embedding}__{student}.pkl\n",
    "- outputs/e_2_teacher_labeling/cache/synth_embeds/*__{student_embedding}.npy and *__index.csv\n",
    "- (legacy reuse, only if student_embedding==label_embedding==e5_base and student==chain_ERCcv_lr)\n",
    "  outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n96_sps1__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n192_sps2__full.csv\n",
    "    rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__n384_sps4__full.csv\n",
    "\n",
    "Outputs:\n",
    "- outputs/e_3_student_scoring/results/\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}__baseline.csv\n",
    "    rrmse_perfold_{student_embedding}__{regressor}__{method}__pct{P}_K{K}__Mmax{Mmax}__full.csv\n",
    "- outputs/e_3_student_scoring/hp_perfold/\n",
    "    {regressor}/{method}/n{Mmax}/pct{P}_K{K}/fold{ii}_best.json\n",
    "- outputs/e_3_student_scoring/tables/\n",
    "    summary_median_rrmse.csv\n",
    "    summary_median_rrmse_with_delta.csv\n",
    "    wilcoxon_holm_vs_baseline.csv\n",
    "    cliffs_delta_vs_baseline.csv\n",
    "    wilcoxon_pooled_vs_baseline.csv\n",
    "    cliffs_delta_pooled_vs_baseline.csv\n",
    "    bootstrap_delta_ci.csv\n",
    "- outputs/e_3_student_scoring/plots/\n",
    "    combined_rrmse_vs_pct__{method}.png\n",
    "    combined_delta_vs_pct__{method}.png\n",
    "    combined_rrmse_vs_pct__{method}__{embedding|all}.png\n",
    "    combined_delta_vs_pct__{method}__{embedding|all}.png\n",
    "- outputs/e_3_student_scoring/\n",
    "    cache/, run.log, run_config.json\n",
    "\"\"\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Imports\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Paths \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def project_root(marker: str = \"LICENSE\") -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for d in (here, *here.parents):\n",
    "        if (d / marker).is_file():\n",
    "            return d\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "ROOT = project_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "G1_DIR  = ROOT / \"outputs\" / \"e_1_synth_augmentation\"\n",
    "G2_DIR  = ROOT / \"outputs\" / \"e_2_teacher_labeling\"\n",
    "SEED_VECTORS_DIR = ROOT / \"outputs\" / \"b_frozen\" / \"results\"\n",
    "\n",
    "G3A_DIR = ROOT / \"outputs\" / \"e_3_student_scoring\"\n",
    "RES_DIR     = G3A_DIR / \"results\"\n",
    "TABLES_DIR  = G3A_DIR / \"tables\"\n",
    "CACHE_DIR   = G3A_DIR / \"cache\"\n",
    "HP_PERFOLD  = G3A_DIR / \"hp_perfold\"\n",
    "FIGS_DIR    = G3A_DIR / \"plots\"\n",
    "\n",
    "for p in (G3A_DIR, RES_DIR, TABLES_DIR, CACHE_DIR, HP_PERFOLD, FIGS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TABLES = TABLES_DIR\n",
    "PLOTS_DIR = FIGS_DIR\n",
    "OUT_PLOTS  = FIGS_DIR\n",
    "\n",
    "LOG_FILE = G3A_DIR / \"run.log\"\n",
    "for h in list(logging.root.handlers):\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Logging \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\"),\n",
    "        # use the uncaptured stdout to bypass notebook/pytest capturing\n",
    "        logging.StreamHandler(getattr(sys, \"__stdout__\", sys.stdout)),\n",
    "    ],\n",
    "    force=True,\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "_say_logger = logging.getLogger(\"sayfile\")\n",
    "if not _say_logger.handlers:\n",
    "    _say_logger.setLevel(logging.INFO)\n",
    "    _say_logger.propagate = False  # do NOT bubble to root (prevents console echo)\n",
    "    _fh = logging.FileHandler(str(LOG_FILE), mode=\"a\", encoding=\"utf-8\")\n",
    "    _fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "                                       datefmt=\"%Y-%m-%d %H:%M:%S\"))\n",
    "    _say_logger.addHandler(_fh)\n",
    "    \n",
    "RUN_ID: str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Config\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "REVIEW_MODE: bool = True\n",
    "\n",
    "N_TARGETS = 14\n",
    "\n",
    "METHODS: Optional[List[str]] = None\n",
    "\n",
    "PCT_LIST: List[int]       = [10, 20, 50, 100, 200, 400]\n",
    "PCTS_FOR_PLOTS: List[int] = [10, 20, 50, 100, 200, 400]\n",
    "PRIMARY_PCTS = PCT_LIST\n",
    "\n",
    "SAVE_PRED_PERFOLD: bool = True\n",
    "LOG_EVERY: int          = 16\n",
    "MAX_FOLDS: int          = 0  # 0 ⇒ all 96\n",
    "\n",
    "# Which student regressors to run\n",
    "STUDENTS: List[str] = [\"chain_ERCcv_lr\", \"local_lasso\", \"local_rf\", \"global_rf\", \"chain_ERCcv_rf\"]  \n",
    "\n",
    "TARGET_COLS = [f\"domain{i}\" for i in range(1, 15)]\n",
    "_DOM_PREFIX = \"rrmse_domain\"\n",
    "BOOT_B: int = 5000\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Device, cores, seeding\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "DETERMINISTIC = True\n",
    "N_JOBS: int = min(os.cpu_count() or 6, 6)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\",        str(N_JOBS))\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\",   str(N_JOBS))\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\",        str(N_JOBS))\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", str(N_JOBS))\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\",    str(N_JOBS))\n",
    "try:\n",
    "    torch.set_num_threads(N_JOBS)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "DEVICE_STR = device.type\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import random\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        torch.manual_seed(SEED)\n",
    "    if DETERMINISTIC:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Embedding registry and selection\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "EMBEDDING_SPECS: Dict[str, str] = {\n",
    "    \"e5_base\":          \"embaas/sentence-transformers-multilingual-e5-base\",\n",
    "    #\"e5_large\":         \"embaas/sentence-transformers-multilingual-e5-large\",\n",
    "    #\"simcse_xlmr_base\": \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\",\n",
    "    #\"sbert_bert\":       \"jegormeister/bert-base-dutch-cased-snli\",\n",
    "}\n",
    "\n",
    "STUDENT_EMBEDDINGS: List[str] = [\"e5_base\"]\n",
    "def _normalize_embeddings(x):\n",
    "    if isinstance(x, str):\n",
    "        return [s.strip() for s in x.split(\",\") if s.strip()]\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [str(s).strip() for s in x if str(s).strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Normalize & dedupe\n",
    "_seen = set()\n",
    "STUDENT_EMBEDDINGS = [e for e in STUDENT_EMBEDDINGS if not (e in _seen or _seen.add(e))]\n",
    "# After deduping STUDENT_EMBEDDINGS\n",
    "STUDENT_EMB = STUDENT_EMBEDDINGS[0]\n",
    "\n",
    "# Label source (teacher that produced the labels for synthetics)\n",
    "LABEL_EMB: str   = \"e5_base\"\n",
    "LABEL_MODEL: str = \"local_lasso\"\n",
    "if LABEL_EMB not in EMBEDDING_SPECS:\n",
    "    raise ValueError(f\"Unknown LABEL_SOURCE_EMBEDDING='{LABEL_EMB}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "\n",
    "MAX_SEQ_LEN: int = 512\n",
    "\n",
    "# Optional extra pass: train chain_ERCcv_lr student on labels from local_lasso teacher (same embedding)\n",
    "EXTRA_STUDENT_LABEL_JOBS = [\n",
    "    {\n",
    "        \"student\": \"chain_ERCcv_lr\",\n",
    "        \"label_emb\": \"e5_base\",\n",
    "        \"label_model\": \"local_lasso\",\n",
    "        \"labels_tag\": \"labels_local_lasso\",\n",
    "    },\n",
    "        {\n",
    "        \"student\": \"local_lasso\",\n",
    "        \"label_emb\": \"e5_base\",\n",
    "        \"label_model\": \"local_lasso\",\n",
    "        \"labels_tag\": \"labels_local_lasso\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Per-embedding tables/plots\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def _auto_flag(val: str, default_auto: bool) -> bool:\n",
    "    v = (val or \"\").strip().lower()\n",
    "    if v in {\"1\",\"true\",\"yes\",\"y\"}:  return True\n",
    "    if v in {\"0\",\"false\",\"no\",\"n\"}:  return False\n",
    "    return default_auto\n",
    "\n",
    "MULTI_EMBED = len(STUDENT_EMBEDDINGS) > 1\n",
    "WRITE_PER_EMBED_TABLES: bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "WRITE_PER_EMBED_PLOTS:  bool = _auto_flag(\"auto\", default_auto=not MULTI_EMBED)\n",
    "\n",
    "if not WRITE_PER_EMBED_TABLES:\n",
    "    log.info(\"[guard] Per-embedding tables disabled (combined report will handle tables).\")\n",
    "if not WRITE_PER_EMBED_PLOTS:\n",
    "    log.info(\"[guard] Per-embedding plots disabled (combined report will draw plots).\")\n",
    "\n",
    "def write_run_config():\n",
    "    \"\"\"Dump a lightweight snapshot of env + resolved settings for reproducibility.\"\"\"\n",
    "    try:\n",
    "        cfg = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"device\": DEVICE_STR,\n",
    "            \"seed\": SEED,\n",
    "            \"n_jobs\": N_JOBS,\n",
    "            \"student_embeddings\": STUDENT_EMBEDDINGS,\n",
    "            \"current_student_embedding\": STUDENT_EMB,\n",
    "            \"students\": STUDENTS,\n",
    "            \"methods_filter_env\": os.getenv(\"METHODS\", \"\").strip() or None,\n",
    "            \"pct_list\": PCT_LIST,\n",
    "            \"pcts_for_plots\": PCTS_FOR_PLOTS,\n",
    "            \"label_source_embedding\": LABEL_EMB,\n",
    "            \"label_source_model\": LABEL_MODEL,\n",
    "            \"max_folds\": MAX_FOLDS,\n",
    "            \"boot_B\": BOOT_B,\n",
    "            \"write_per_embed_tables\": WRITE_PER_EMBED_TABLES,\n",
    "            \"write_per_embed_plots\": WRITE_PER_EMBED_PLOTS,\n",
    "            \"skip_baseline_if_present\": os.getenv(\"SKIP_BASELINE_IF_PRESENT\", \"0\"),\n",
    "            \"do_refits\": os.getenv(\"DO_REFITS\", \"1\"),\n",
    "            \"paths\": {\n",
    "                \"root\": str(ROOT),\n",
    "                \"g1_dir\": str(G1_DIR),\n",
    "                \"g2_dir\": str(G2_DIR),\n",
    "                \"g3a_dir\": str(G3A_DIR),\n",
    "                \"results_dir\": str(RES_DIR),\n",
    "                \"tables_dir\": str(TABLES_DIR),\n",
    "                \"figs_dir\": str(FIGS_DIR),\n",
    "            },\n",
    "        }\n",
    "        (G3A_DIR / \"run_config.json\").write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "        log.info(\"run_config.json written.\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Could not write run_config.json: {e}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Sentence encoders\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "_ENCODERS: Dict[str, SentenceTransformer] = {}\n",
    "\n",
    "def get_encoder(emb_key: str) -> SentenceTransformer:\n",
    "    if emb_key in _ENCODERS:\n",
    "        return _ENCODERS[emb_key]\n",
    "    repo = EMBEDDING_SPECS[emb_key]\n",
    "    log.info(f\"Loading SentenceTransformer [{emb_key}]: {repo} → device={DEVICE_STR}\")\n",
    "    m = SentenceTransformer(repo, device=DEVICE_STR)\n",
    "    m.max_seq_length = MAX_SEQ_LEN\n",
    "    _ENCODERS[emb_key] = m\n",
    "    return m\n",
    "\n",
    "def encode_texts(emb_key: str, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    mdl = get_encoder(emb_key)\n",
    "    X = mdl.encode(texts, batch_size=batch_size, convert_to_numpy=True,\n",
    "                   show_progress_bar=False, normalize_embeddings=False)\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Student models\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "class RegressorChainCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Cross-validated out-of-fold chaining with randomized target order support.\"\"\"\n",
    "    def __init__(self, base_estimator, order=None, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.order = order\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.chain_models_ = []\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples, self.n_targets_ = Y.shape\n",
    "        if self.order is None:\n",
    "            self.order = np.arange(self.n_targets_)\n",
    "        kf = KFold(n_splits=self.cv_splits, shuffle=True, random_state=rng)\n",
    "        X_chain = np.ascontiguousarray(X, dtype=np.float32)\n",
    "\n",
    "        oof_cols = []\n",
    "        for target_idx in self.order:\n",
    "            y = Y[:, target_idx]\n",
    "            oof = np.zeros(n_samples, dtype=np.float32)\n",
    "            for tr, va in kf.split(X_chain):\n",
    "                est = clone(self.base_estimator)\n",
    "                est.fit(X_chain[tr], y[tr])\n",
    "                oof[va] = est.predict(X_chain[va])\n",
    "            oof_cols.append(oof.reshape(-1, 1))\n",
    "            X_chain = np.hstack([X_chain, oof.reshape(-1,1)])\n",
    "\n",
    "        self.chain_models_ = []\n",
    "        acc = []\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            if i == 0:\n",
    "                X_full = X\n",
    "            else:\n",
    "                acc.append(oof_cols[i-1])\n",
    "                X_full = np.hstack([X, np.hstack(acc)])\n",
    "            est = clone(self.base_estimator)\n",
    "            est.fit(X_full, Y[:, target_idx])\n",
    "            self.chain_models_.append(est)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ext = np.ascontiguousarray(X, dtype=np.float32)\n",
    "        n = X.shape[0]\n",
    "        preds = np.zeros((n, self.n_targets_), dtype=np.float32)\n",
    "        for i, target_idx in enumerate(self.order):\n",
    "            yhat = self.chain_models_[i].predict(X_ext).reshape(-1, 1)\n",
    "            preds[:, target_idx] = yhat[:, 0]\n",
    "            X_ext = np.hstack([X_ext, yhat])\n",
    "        return preds\n",
    "\n",
    "class EnsembleRegressorChainsCV(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Ensemble over random target orders; average predictions.\"\"\"\n",
    "    def __init__(self, base_estimator, n_chains=5, cv_splits=5, random_state=SEED):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_chains = n_chains\n",
    "        self.cv_splits = cv_splits\n",
    "        self.random_state = random_state\n",
    "        self.ensemble_ = None\n",
    "        self.n_targets_ = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self.n_targets_ = Y.shape[1]\n",
    "        self.ensemble_ = []\n",
    "        for _ in range(self.n_chains):\n",
    "            order = np.arange(self.n_targets_)\n",
    "            rng.shuffle(order)\n",
    "            chain = RegressorChainCV(\n",
    "                base_estimator=self.base_estimator,\n",
    "                order=order,\n",
    "                cv_splits=self.cv_splits,\n",
    "                random_state=rng.randint(0, 1_000_000),\n",
    "            )\n",
    "            chain.fit(X, Y)\n",
    "            self.ensemble_.append((order, chain))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [chain.predict(X) for (_, chain) in self.ensemble_]\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "def build_with_hp(student_key: str, hp: Dict[str, float | int]) -> Pipeline:\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(\n",
    "                base_estimator=LinearRegression(),\n",
    "                n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                random_state=SEED\n",
    "            )),\n",
    "        ])\n",
    "    elif student_key == \"local_lasso\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", MultiOutputRegressor(Lasso(alpha=float(hp[\"reg__estimator__alpha\"]),\n",
    "                                               random_state=SEED, max_iter=10_000))),\n",
    "        ])\n",
    "    elif student_key == \"local_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", MultiOutputRegressor(RandomForestRegressor(\n",
    "                n_estimators=int(hp[\"reg__estimator__n_estimators\"]),\n",
    "                max_depth=None if (hp.get(\"reg__estimator__max_depth\", None) in [None, \"None\"]) else int(hp[\"reg__estimator__max_depth\"]),\n",
    "                min_samples_leaf=int(hp.get(\"reg__estimator__min_samples_leaf\", 1)),\n",
    "                random_state=SEED, n_jobs=1\n",
    "            ))),\n",
    "        ])\n",
    "    elif student_key == \"global_rf\":\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"reg\", RandomForestRegressor(\n",
    "                n_estimators=int(hp[\"reg__n_estimators\"]),\n",
    "                max_depth=None if (hp.get(\"reg__max_depth\", None) in [None, \"None\"]) else int(hp[\"reg__max_depth\"]),\n",
    "                min_samples_leaf=int(hp.get(\"reg__min_samples_leaf\", 1)),\n",
    "                random_state=SEED, n_jobs=1\n",
    "            )),\n",
    "        ])\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        base_rf = RandomForestRegressor(\n",
    "            n_estimators=int(hp.get(\"chain__base_rf__n_estimators\", 100)),\n",
    "            max_depth=None if (hp.get(\"chain__base_rf__max_depth\", None) in [None, \"None\"]) else int(hp[\"chain__base_rf__max_depth\"]),\n",
    "            min_samples_leaf=int(hp.get(\"chain__base_rf__min_samples_leaf\", 1)),\n",
    "            random_state=SEED, n_jobs=1\n",
    "        )\n",
    "        return Pipeline([\n",
    "            (\"pca\", PCA(random_state=SEED, n_components=float(hp[\"pca__n_components\"]))),\n",
    "            (\"chain\", EnsembleRegressorChainsCV(\n",
    "                base_estimator=base_rf,\n",
    "                n_chains=int(hp[\"chain__n_chains\"]),\n",
    "                cv_splits=int(hp[\"chain__cv_splits\"]),\n",
    "                random_state=SEED\n",
    "            )),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown student_key: {student_key}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Load per-fold teacher pickle & extract HPs\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def teacher_pickle_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    # output/g_synth_augmented/g_2a_teacher_labeling_loocv_tuned/teacher/teacher_fold{ii}_{emb}__{student}.pkl\n",
    "    return ROOT / \"models\" / \"teacher\" / f\"teacher_fold{fold_idx:02d}_{emb_key}__{student_key}.pkl\"\n",
    "\n",
    "def _hp_json_path(fold_idx: int, student_key: str, emb_key: str) -> Path:\n",
    "    return G2_DIR / \"teacher\" / f\"hp_fold{fold_idx:02d}_{emb_key}__{student_key}.json\"\n",
    "\n",
    "def hp_from_teacher(est: Pipeline, student_key: str) -> Dict[str, float | int]:\n",
    "    hp: Dict[str, float | int] = {}\n",
    "\n",
    "    if \"pca\" in est.named_steps:\n",
    "        hp[\"pca__n_components\"] = float(getattr(est.named_steps[\"pca\"], \"n_components\", 0.8))\n",
    "\n",
    "    if student_key == \"chain_ERCcv_lr\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "\n",
    "    elif student_key == \"local_lasso\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__alpha\"] = float(getattr(reg, \"alpha\", 0.01))\n",
    "\n",
    "    elif student_key == \"local_rf\":\n",
    "        reg = est.named_steps[\"reg\"].estimator\n",
    "        hp[\"reg__estimator__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__estimator__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__estimator__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "\n",
    "    elif student_key == \"global_rf\":\n",
    "        reg = est.named_steps[\"reg\"]\n",
    "        hp[\"reg__n_estimators\"] = int(getattr(reg, \"n_estimators\", 100))\n",
    "        hp[\"reg__max_depth\"] = getattr(reg, \"max_depth\", None)\n",
    "        hp[\"reg__min_samples_leaf\"] = int(getattr(reg, \"min_samples_leaf\", 1))\n",
    "\n",
    "    elif student_key == \"chain_ERCcv_rf\":\n",
    "        ch = est.named_steps[\"chain\"]\n",
    "        hp[\"chain__n_chains\"] = int(getattr(ch, \"n_chains\", 5))\n",
    "        hp[\"chain__cv_splits\"] = int(getattr(ch, \"cv_splits\", 5))\n",
    "\n",
    "        base_rf = getattr(ch, \"base_estimator\", None)\n",
    "        if base_rf is None and hasattr(ch, \"base_estimator_\"):\n",
    "            base_rf = ch.base_estimator_\n",
    "\n",
    "        if isinstance(base_rf, RandomForestRegressor):\n",
    "            hp[\"chain__base_rf__n_estimators\"] = int(getattr(base_rf, \"n_estimators\", 100))\n",
    "            hp[\"chain__base_rf__max_depth\"] = getattr(base_rf, \"max_depth\", None)\n",
    "            hp[\"chain__base_rf__min_samples_leaf\"] = int(getattr(base_rf, \"min_samples_leaf\", 1))\n",
    "    else:\n",
    "        raise ValueError(student_key)\n",
    "\n",
    "    return hp\n",
    "\n",
    "def load_teacher_fold_estimator(fold_idx: int, student_key: str, emb_key: str, retries: int = 3, sleep_s: float = 1.5) -> Optional[Pipeline]:\n",
    "    \"\"\"Load fitted teacher pipeline with a few retries for flaky network/cloud files.\"\"\"\n",
    "    p = teacher_pickle_path(fold_idx, student_key, emb_key)\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if not p.exists():\n",
    "            break\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            if \"timed out\" in str(e).lower() or isinstance(e, OSError):\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}) [attempt {attempt}/{retries}]: {e}\")\n",
    "                if attempt < retries:\n",
    "                    time.sleep(sleep_s * attempt)\n",
    "                    continue\n",
    "            else:\n",
    "                log.warning(f\"Could not load teacher pickle ({p.name}): {e}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def hp_from_teacher_or_json(fold_idx: int, student_key: str, emb_key: str, est: Optional[Pipeline]) -> Dict[str, float | int]:\n",
    "    \"\"\"\n",
    "    Extract HPs from teacher estimator if provided; else from the per-fold hp_*.json.\n",
    "    If neither is available, RAISE an error (no hardcoded defaults).\n",
    "    \"\"\"\n",
    "    # 1) Teacher pickle provided\n",
    "    if est is not None:\n",
    "        return hp_from_teacher(est, student_key)\n",
    "\n",
    "    # 2) JSON fallback\n",
    "    j = _hp_json_path(fold_idx, student_key, emb_key)\n",
    "    if j.exists():\n",
    "        try:\n",
    "            payload = json.loads(j.read_text(encoding=\"utf-8\"))\n",
    "            # Some older JSONs may store the whole payload; accept either {\"best_params\": {...}} or the flat dict\n",
    "            if isinstance(payload, dict) and \"best_params\" in payload and isinstance(payload[\"best_params\"], dict):\n",
    "                return payload[\"best_params\"]\n",
    "            if isinstance(payload, dict):\n",
    "                return payload  # assume it already is the HP dict\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"HP JSON unreadable: {j.name} | {e}\") from e\n",
    "\n",
    "    # 3) Hard stop\n",
    "    raise RuntimeError(\n",
    "        f\"Missing per-fold HPs for fold={fold_idx}, student={student_key}, emb={emb_key}. \"\n",
    "        f\"Expected either teacher pickle ({teacher_pickle_path(fold_idx, student_key, emb_key).name}) \"\n",
    "        f\"or HP JSON ({_hp_json_path(fold_idx, student_key, emb_key).name}).\"\n",
    "    )\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Data loading (seeds + synthetics)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def load_seeds() -> pd.DataFrame:\n",
    "    scores = pd.read_csv(DATA_DIR / \"activity_scores.csv\")\n",
    "    acts   = pd.read_csv(DATA_DIR / \"activities.csv\")\n",
    "    dm = scores.pivot(index=\"activity_id\", columns=\"domain_id\", values=\"score\").reset_index()\n",
    "    dm = dm.rename(columns=lambda x: f\"domain{x}\" if isinstance(x, (int, np.integer)) else x)\n",
    "    dm = dm.merge(acts[[\"activity_id\", \"question\"]], on=\"activity_id\", how=\"left\")\n",
    "    dm = dm.rename(columns={\"activity_id\":\"seed_id\", \"question\":\"text\"})\n",
    "    dm = dm.sort_values(\"seed_id\").reset_index(drop=True)\n",
    "    assert len(dm) == 96, f\"Expected 96 seeds, got {len(dm)}\"\n",
    "    return dm[[\"seed_id\",\"text\", *TARGET_COLS]]\n",
    "\n",
    "def ensure_seed_vectors(seeds_df: pd.DataFrame, emb_key: str) -> np.ndarray:\n",
    "    vec_path  = SEED_VECTORS_DIR / f\"{emb_key}_vectors.npy\"\n",
    "    meta_path = vec_path.with_suffix(\".meta.json\")\n",
    "\n",
    "    # Prefer reuse if rows match; meta is optional\n",
    "    if vec_path.exists():\n",
    "        try:\n",
    "            X = np.load(vec_path)\n",
    "            ok_rows = (X.shape[0] == len(seeds_df))\n",
    "            ok_len = True  # default to OK if no meta\n",
    "            if meta_path.exists():\n",
    "                meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "                ok_len = int(meta.get(\"max_seq_len\", MAX_SEQ_LEN)) == int(MAX_SEQ_LEN)\n",
    "            if ok_rows and ok_len:\n",
    "                np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X)\n",
    "                log.info(\"✔ Reusing seed vectors → %s\", vec_path.relative_to(ROOT))\n",
    "                return X.astype(np.float32, copy=False)\n",
    "            else:\n",
    "                log.warning(\"[%s] Rebuilding seed vectors (rows_ok=%s, max_seq_len_ok=%s).\",\n",
    "                            emb_key, ok_rows, ok_len)\n",
    "        except Exception as e:\n",
    "            log.warning(\"[%s] Existing vectors unusable (%s); rebuilding.\", emb_key, e)\n",
    "\n",
    "    # Compute and save to the frozen location if nothing reusable was found\n",
    "    SEED_VECTORS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(\"Computing %s embeddings for seeds …\", emb_key)\n",
    "    X = encode_texts(emb_key, seeds_df[\"text\"].astype(str).tolist(), batch_size=64)\n",
    "    np.save(vec_path, X.astype(np.float32, copy=False))\n",
    "    np.save(CACHE_DIR / f\"X_seed_{emb_key}.npy\", X.astype(np.float32, copy=False))\n",
    "\n",
    "    # (optional) write meta\n",
    "    try:\n",
    "        meta = {\n",
    "            \"embedding\": emb_key,\n",
    "            \"repo\": EMBEDDING_SPECS[emb_key],\n",
    "            \"max_seq_len\": int(MAX_SEQ_LEN),\n",
    "            \"n_rows\": int(X.shape[0]),\n",
    "            \"n_dim\": int(X.shape[1]),\n",
    "        }\n",
    "        meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    log.info(\"✔ Seed vectors saved → %s\", vec_path.relative_to(ROOT))\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def base_from_method_and_M(method: str, M: int) -> str:\n",
    "    return f\"n{M}_{method}\"\n",
    "\n",
    "def g2_label_file(\n",
    "    fold_idx: int,\n",
    "    method: str,\n",
    "    M: int,\n",
    "    label_emb: str,\n",
    "    label_model: str,\n",
    "    strict_model: bool = False\n",
    ") -> Path:\n",
    "    base = base_from_method_and_M(method, M)\n",
    "    preferred = G2_DIR / f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__{label_model}.csv\"\n",
    "    if preferred.exists():\n",
    "        return preferred\n",
    "    if strict_model:\n",
    "        return preferred  # let caller raise later if missing\n",
    "    # fallback: any model with the label embedding (keeps label source embedding fixed)\n",
    "    cand = sorted(G2_DIR.glob(f\"g2f_labels_fold{fold_idx:02d}_{base}__{label_emb}__*.csv\"))\n",
    "    if cand:\n",
    "        log.warning(\n",
    "            \"Falling back to %s for fold=%02d, method=%s, M=%d (label_model=%s missing).\",\n",
    "            cand[0].name, fold_idx, method, M, label_model\n",
    "        )\n",
    "        return cand[0]\n",
    "    return preferred  # will raise later if missing\n",
    "\n",
    "def discover_methods_and_M_from_g2(label_emb: str) -> Dict[str, List[int]]:\n",
    "    pats = sorted(G2_DIR.glob(f\"g2f_labels_fold00_n*_*__{label_emb}__*.csv\"))\n",
    "    rows: Dict[str, set] = {}\n",
    "    for p in pats:\n",
    "        m = re.match(rf\"g2f_labels_fold00_n(\\d+)_([A-Za-z0-9_]+)__{label_emb}__.*\\.csv$\", p.name)\n",
    "        if not m: \n",
    "            continue\n",
    "        M = int(m.group(1)); method = m.group(2)\n",
    "        rows.setdefault(method, set()).add(M)\n",
    "    return {k: sorted(v) for k, v in rows.items()}\n",
    "\n",
    "def synth_cache_paths_from_g2c(g1_path: Path, emb_key: str) -> Tuple[Path, Path]:\n",
    "    base = g1_path.stem\n",
    "    m = re.match(r\"g_final_(n\\d+_[A-Za-z0-9_]+)$\", base)\n",
    "    if m:\n",
    "        base = m.group(1)\n",
    "    cache_dir = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "    candidates = [\n",
    "        (cache_dir / f\"g_final_{base}__{emb_key}.npy\",  cache_dir / f\"g_final_{base}__index.csv\"),\n",
    "        (cache_dir / f\"{base}__{emb_key}.npy\",          cache_dir / f\"{base}__index.csv\"),\n",
    "    ]\n",
    "    for npy, idx in candidates:\n",
    "        if npy.exists() and idx.exists():\n",
    "            return npy, idx\n",
    "    nearby = sorted(p.name for p in cache_dir.glob(f\"*{base}*\"))\n",
    "    tried  = \" ; \".join([npy.name for npy, _ in candidates] + [idx.name for _, idx in candidates])\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing synth cache for base='{base}' and embedding='{emb_key}'. Tried: {tried}. \"\n",
    "        f\"Found near-matches in cache: {nearby}\"\n",
    "    )\n",
    "\n",
    "def g1_source_csv(method: str, M: int) -> Path:\n",
    "    p = G1_DIR / f\"g_final_n{M}_{method}.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing Script-A source: {p}\")\n",
    "    return p\n",
    "\n",
    "def ensure_synth_cache(method: str, M: int, emb_key: str) -> Tuple[Path, Path]:\n",
    "    \"\"\"Ensure (npy, index.csv) exist for Script-A synthetics under the given embedding.\"\"\"\n",
    "    src_csv = g1_source_csv(method, M)\n",
    "    base = src_csv.stem\n",
    "    m = re.match(r\"g_final_(n\\d+_[A-Za-z0-9_]+)$\", base)\n",
    "    base = m.group(1) if m else base\n",
    "\n",
    "    cache_dir = G2_DIR / \"cache\" / \"synth_embeds\"\n",
    "    npy  = cache_dir / f\"g_final_{base}__{emb_key}.npy\"\n",
    "    idx  = cache_dir / f\"g_final_{base}__index.csv\"\n",
    "    if npy.exists() and idx.exists():\n",
    "        return npy, idx\n",
    "\n",
    "    df = pd.read_csv(src_csv)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(f\"{src_csv.name}: missing 'text' column\")\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "    log.info(f\"[cache] building synth embeds for method={method}, M={M}, emb={emb_key} …\")\n",
    "    Xs = encode_texts(emb_key, texts, batch_size=64).astype(np.float32, copy=False)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(npy, Xs)\n",
    "    pd.DataFrame({\"text\": texts}).to_csv(idx, index=False)\n",
    "    log.info(f\"[cache] ✔ saved → {npy.relative_to(ROOT)} ; {idx.relative_to(ROOT)}\")\n",
    "    return npy, idx\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Metrics \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def rmse(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(np.mean((a - b) ** 2, axis=0))\n",
    "\n",
    "def rrmse_vs_dummy(y_true: np.ndarray, y_pred: np.ndarray, y_dummy: np.ndarray) -> np.ndarray:\n",
    "    rm = rmse(y_true, y_pred)\n",
    "    rd = rmse(y_true, y_dummy)\n",
    "    return rm / np.maximum(rd, 1e-12)\n",
    "\n",
    "def _fmt_dt(sec: float) -> str:\n",
    "    m, s = divmod(sec, 60.0)\n",
    "    h, m = divmod(m, 60.0)\n",
    "    if h >= 1: return f\"{int(h)}h{int(m):02d}m{s:04.1f}s\"\n",
    "    if m >= 1: return f\"{int(m)}m{s:04.1f}s\"\n",
    "    return f\"{s:0.2f}s\"\n",
    "\n",
    "def paired_cliffs_delta(full: np.ndarray, base: np.ndarray):\n",
    "    diff = base - full  # positive when full improves (lower is better)\n",
    "    n_pos = int(np.sum(diff > 0))\n",
    "    n_neg = int(np.sum(diff < 0))\n",
    "    n_zero = int(np.sum(diff == 0))\n",
    "    denom = max(1, (n_pos + n_neg))\n",
    "    delta = (n_pos - n_neg) / denom\n",
    "    mag = (\"negligible\" if abs(delta) < 0.147 else\n",
    "           \"small\"       if abs(delta) < 0.33  else\n",
    "           \"medium\"      if abs(delta) < 0.474 else\n",
    "           \"large\")\n",
    "    return float(delta), n_pos, n_neg, n_zero, mag\n",
    "\n",
    "def pct_to_K(pct: int, n_seeds: int = 96) -> int:\n",
    "    return max(1, int(round((pct / 100.0) * n_seeds)))\n",
    "\n",
    "def _dom_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return [c for c in df.columns if c.startswith(_DOM_PREFIX)]\n",
    "\n",
    "def _global_median_from_file(p: Path) -> float:\n",
    "    df = pd.read_csv(p)\n",
    "    cols = _dom_cols(df)\n",
    "    if not cols:\n",
    "        raise RuntimeError(f\"{p.name}: no '{_DOM_PREFIX}*' columns present.\")\n",
    "    return float(np.median(df[cols].to_numpy(dtype=np.float32).ravel()))\n",
    "\n",
    "def _flat_arrays(p_base: Path, p_full: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    db = pd.read_csv(p_base)\n",
    "    df = pd.read_csv(p_full)\n",
    "    cols = _dom_cols(db)\n",
    "    if not cols or not set(cols).issubset(df.columns):\n",
    "        missing = sorted(set(cols) - set(df.columns))\n",
    "        raise RuntimeError(f\"{p_full.name}: missing '{_DOM_PREFIX}*' columns: {missing}\")\n",
    "    xb = db[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    xf = df[cols].to_numpy(dtype=np.float32).ravel()\n",
    "    return xb, xf\n",
    "\n",
    "def section(title):\n",
    "    \"\"\"Print section header\"\"\"\n",
    "    bar = \"═\" * len(title)\n",
    "    print(f\"\\n{bar}\\n{title}\\n{bar}\")\n",
    "\n",
    "def _save_and_show(fig, path: str):\n",
    "    \"\"\"Save and display figure\"\"\"\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Plot saved → {path}\")\n",
    "\n",
    "def aligned_ranks(mat):\n",
    "    \"\"\"Hodges–Lehmann alignment + ranking along rows (lower is better)\"\"\"\n",
    "    aligned = mat - np.median(mat, axis=1, keepdims=True)\n",
    "    return np.apply_along_axis(lambda r: np.argsort(np.argsort(r)) + 1, 1, aligned)\n",
    "\n",
    "def friedman_aligned(mat):\n",
    "    \"\"\"Aligned-Friedman χ² and Iman–Davenport F-statistic (expects ranks or aligned data)\"\"\"\n",
    "    k = mat.shape[1]\n",
    "    chi2, _ = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff = ((mat.shape[0] - 1) * chi2) / (mat.shape[0] * (k - 1) - chi2)\n",
    "    return chi2, Ff\n",
    "\n",
    "def wilcoxon_matrix(mat, labels):\n",
    "    \"\"\"Pairwise two-sided Wilcoxon (zero-method = zsplit)\"\"\"\n",
    "    df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for i, j in combinations(range(len(labels)), 2):\n",
    "        diff = mat[:, i] - mat[:, j]\n",
    "        p = 1.0 if np.allclose(diff, 0) else wilcoxon(diff, zero_method=\"zsplit\")[1]\n",
    "        df.iat[i, j] = df.iat[j, i] = p\n",
    "    return df.round(4)\n",
    "\n",
    "def holm_correct_and_effects(raw_p, data, labels):\n",
    "    \"\"\"Holm–Bonferroni correction and Cliff's Δ effect sizes\"\"\"\n",
    "    idx = list(combinations(range(len(labels)), 2))\n",
    "    pvals = [raw_p.iat[i, j] for i, j in idx]\n",
    "    _, p_adj, _, _ = multipletests(pvals, method=\"holm\")\n",
    "\n",
    "    adj_df = raw_p.copy()\n",
    "    for (i, j), p in zip(idx, p_adj):\n",
    "        adj_df.iat[i, j] = adj_df.iat[j, i] = p\n",
    "    adj_df[np.eye(len(labels), dtype=bool)] = 1.0\n",
    "\n",
    "    def cliffs_delta(x, y):\n",
    "        diffs = np.subtract.outer(x, y)\n",
    "        n = len(x) * len(y)\n",
    "        return (np.sum(diffs > 0) - np.sum(diffs < 0)) / n\n",
    "\n",
    "    delta_df = pd.DataFrame(np.ones((len(labels), len(labels))), index=labels, columns=labels)\n",
    "    for (i, j) in idx:\n",
    "        d_ij = cliffs_delta(data[:, i], data[:, j])\n",
    "        delta_df.iat[i, j] = d_ij\n",
    "        delta_df.iat[j, i] = -d_ij\n",
    "\n",
    "    return adj_df.round(4), delta_df.round(3)\n",
    "\n",
    "def conover_posthoc(ranks, labels, fname_tag):\n",
    "    \"\"\"Conover–Iman test with Holm correction\"\"\"\n",
    "    p_df = sp.posthoc_conover_friedman(ranks, p_adjust=\"holm\")\n",
    "    p_df.index = p_df.columns = labels\n",
    "    out = TABLES_DIR / f\"{fname_tag}_conover_p.csv\"\n",
    "    p_df.to_csv(out)\n",
    "    print(\"\\nConover–Iman post-hoc p-values (Holm-adjusted):\")\n",
    "    print(p_df.round(4).to_string())\n",
    "    print(\"  ↳ saved →\", out)\n",
    "    return p_df\n",
    "\n",
    "def run_friedman(mat, block_name, col_labels, fname_tag):\n",
    "    \"\"\"Generic routine for Friedman analysis with post-hoc tests\"\"\"\n",
    "    k = len(col_labels)\n",
    "    nblocks = mat.shape[0]\n",
    "\n",
    "    # Save & print medians (PRINT SORTED low→high; CSV keeps original order)\n",
    "    col_meds = pd.Series(np.median(mat, axis=0), index=col_labels)\n",
    "    med_path = TABLES_DIR / f\"{fname_tag}_median.csv\"\n",
    "    col_meds.to_csv(med_path, header=[\"median_rrmse\"])\n",
    "    print(f\"\\nMedian RRMSE per {block_name[:-1] if block_name.endswith('s') else block_name} (sorted low→high):\")\n",
    "    print(col_meds.sort_values().round(3).to_string())\n",
    "    print(\"  ↳ saved →\", med_path)\n",
    "\n",
    "    if nblocks == 2:\n",
    "        print(f\"\\nOnly two {block_name} → skipping Friedman/post-hoc.\")\n",
    "        wilc = wilcoxon_matrix(mat, col_labels)\n",
    "        print(\"\\nWilcoxon pairwise p-values:\")\n",
    "        print(wilc.round(4).to_string())\n",
    "        wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "        wilc.to_csv(wilc_path)\n",
    "        print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "        adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "        print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "        print(adj.round(4).to_string())\n",
    "        adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "        adj.to_csv(adj_path)\n",
    "        print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "        print(\"\\nCliff's Δ effect sizes:\")\n",
    "        print(delta.round(3).to_string())\n",
    "        delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "        delta.to_csv(delta_path)\n",
    "        print(\"  ↳ saved →\", delta_path)\n",
    "        return\n",
    "\n",
    "    if k == 2:\n",
    "        p = wilcoxon(mat[:, 0], mat[:, 1], zero_method=\"zsplit\")[1]\n",
    "        print(f\"\\nPaired Wilcoxon ({col_labels[0]} vs {col_labels[1]}): p = {p:.5g}\")\n",
    "        return\n",
    "\n",
    "    # Friedman statistics\n",
    "    ranks = aligned_ranks(mat)\n",
    "    chi2_a, Ff_a = friedman_aligned(ranks)\n",
    "    chi2_o, p_o = friedmanchisquare(*[mat[:, i] for i in range(k)])\n",
    "    Ff_o = ((nblocks - 1) * chi2_o) / (nblocks * (k - 1) - chi2_o)\n",
    "\n",
    "    print(f\"\\n*Aligned-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_a:.3f}    F_F = {Ff_a:.3f}\")\n",
    "    print(f\"\\n*Original-Friedman* (blocks = {block_name})\")\n",
    "    print(f\"  χ²_F = {chi2_o:.3f}    p = {p_o:.3g}    F_F = {Ff_o:.3f}\")\n",
    "\n",
    "    # Post-hoc tests\n",
    "    if nblocks < 10:\n",
    "        conover_posthoc(ranks, col_labels, fname_tag)\n",
    "    else:\n",
    "        pvals_nem = sp.posthoc_nemenyi_friedman(ranks)\n",
    "        pvals_nem.index = pvals_nem.columns = col_labels\n",
    "        nem_path = TABLES_DIR / f\"{fname_tag}_nemenyi_p.csv\"\n",
    "        pvals_nem.to_csv(nem_path)\n",
    "        print(\"\\nNemenyi p-values (aligned post-hoc):\")\n",
    "        print(pvals_nem.round(4).to_string())\n",
    "        print(\"  ↳ saved →\", nem_path)\n",
    "\n",
    "    # 1) Pair-wise Wilcoxon\n",
    "    wilc = wilcoxon_matrix(mat, col_labels)\n",
    "    print(\"\\nWilcoxon pairwise p-values:\")\n",
    "    print(wilc.round(4).to_string())\n",
    "    wilc_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_raw_p.csv\"\n",
    "    wilc.to_csv(wilc_path)\n",
    "    print(\"  ↳ saved →\", wilc_path)\n",
    "\n",
    "    # 2) Holm–Bonferroni adjustment + Cliff’s Δ\n",
    "    adj, delta = holm_correct_and_effects(wilc, mat, col_labels)\n",
    "\n",
    "    print(\"\\nHolm–Bonferroni adjusted p-values:\")\n",
    "    print(adj.round(4).to_string())\n",
    "    adj_path = TABLES_DIR / f\"{fname_tag}_wilcoxon_holm_p.csv\"\n",
    "    adj.to_csv(adj_path)\n",
    "    print(\"  ↳ saved →\", adj_path)\n",
    "\n",
    "    print(\"\\nCliff's Δ effect sizes:\")\n",
    "    print(delta.round(3).to_string())\n",
    "    delta_path = TABLES_DIR / f\"{fname_tag}_cliffs_delta.csv\"\n",
    "    delta.to_csv(delta_path)\n",
    "    print(\"  ↳ saved →\", delta_path)\n",
    "\n",
    "def vector_per_target(rrmse_array):\n",
    "    \"\"\"Collapse (folds × targets) → (targets,) by median across folds\"\"\"\n",
    "    return np.median(rrmse_array, axis=0) if getattr(rrmse_array, \"ndim\", 1) == 2 else rrmse_array\n",
    "\n",
    "def matrix_per_target_compare_models(data_dict, model_list, embedding_list):\n",
    "    \"\"\"Build matrix: rows = targets, cols = models, aggregated across embeddings\"\"\"\n",
    "    return np.column_stack([\n",
    "        np.median(\n",
    "            np.concatenate([\n",
    "                vector_per_target(data_dict[emb][model])\n",
    "                for emb in embedding_list if emb in data_dict\n",
    "            ], axis=0).reshape(-1, N_TARGETS),\n",
    "            axis=0\n",
    "        )\n",
    "        for model in model_list\n",
    "    ])\n",
    "\n",
    "# --- PATCH: show full statistics right under each CD diagram -----------------\n",
    "\n",
    "def cd_plot(matrix, labels, title, fname):\n",
    "    \"\"\"Critical-distance diagram with robust p-value alignment to labels.\"\"\"\n",
    "    if matrix.shape[1] < 2:\n",
    "        print(f\"⚠  Skipping CD-plot '{title}' (need ≥2 methods, got {matrix.shape[1]})\")\n",
    "        return\n",
    "\n",
    "    ranks = aligned_ranks(matrix)\n",
    "\n",
    "    # Compute post-hoc p-values and FORCE index/columns to match `labels`\n",
    "    pvals_raw = sp.posthoc_nemenyi_friedman(ranks)\n",
    "    if not isinstance(pvals_raw, pd.DataFrame):\n",
    "        pvals = pd.DataFrame(pvals_raw, index=range(len(labels)), columns=range(len(labels)))\n",
    "    else:\n",
    "        pvals = pvals_raw.copy()\n",
    "\n",
    "    # Defensive shape fix (trim/pad unlikely; trim covers rare inconsistencies)\n",
    "    if pvals.shape != (len(labels), len(labels)):\n",
    "        pvals = pvals.iloc[:len(labels), :len(labels)]\n",
    "        if pvals.shape != (len(labels), len(labels)):\n",
    "            # Last resort: identity p-values (no significant lines)\n",
    "            pvals = pd.DataFrame(np.ones((len(labels), len(labels))), index=range(len(labels)), columns=range(len(labels)))\n",
    "\n",
    "    # Align names to your model labels, sanitize & symmetrize\n",
    "    pvals.index = labels\n",
    "    pvals.columns = labels\n",
    "    pvals = pvals.astype(float).fillna(1.0)\n",
    "    pvals = pd.DataFrame(np.minimum(pvals.values, pvals.values.T), index=labels, columns=labels)\n",
    "    np.fill_diagonal(pvals.values, 1.0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks.mean(0), index=labels), pvals, ax=ax)\n",
    "    ax.set_title(title, pad=10)\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    # full report printed under the plot\n",
    "    tag = Path(fname).stem\n",
    "    run_friedman(matrix, block_name=\"folds\", col_labels=labels, fname_tag=tag)\n",
    "\n",
    "\n",
    "def cd_plot_dual(matrix1, labels1, matrix2, labels2, title1, title2, fname):\n",
    "    \"\"\"Two CD-diagrams side-by-side with robust p-value alignment.\"\"\"\n",
    "    if matrix1.shape[1] < 2 or matrix2.shape[1] < 2:\n",
    "        print(\"⚠  Skipping dual CD-plot (need ≥2 methods for both)\")\n",
    "        return\n",
    "\n",
    "    def _aligned_pvals(M, lbls):\n",
    "        r = aligned_ranks(M)\n",
    "        raw = sp.posthoc_nemenyi_friedman(r)\n",
    "        if not isinstance(raw, pd.DataFrame):\n",
    "            P = pd.DataFrame(raw, index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        else:\n",
    "            P = raw.copy()\n",
    "        if P.shape != (len(lbls), len(lbls)):\n",
    "            P = P.iloc[:len(lbls), :len(lbls)]\n",
    "            if P.shape != (len(lbls), len(lbls)):\n",
    "                P = pd.DataFrame(np.ones((len(lbls), len(lbls))), index=range(len(lbls)), columns=range(len(lbls)))\n",
    "        P.index = lbls\n",
    "        P.columns = lbls\n",
    "        P = P.astype(float).fillna(1.0)\n",
    "        P = pd.DataFrame(np.minimum(P.values, P.values.T), index=lbls, columns=lbls)\n",
    "        np.fill_diagonal(P.values, 1.0)\n",
    "        return r, P\n",
    "\n",
    "    ranks1, pvals1 = _aligned_pvals(matrix1, labels1)\n",
    "    ranks2, pvals2 = _aligned_pvals(matrix2, labels2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 3), dpi=120)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks1.mean(0), index=labels1), pvals1, ax=ax1)\n",
    "    ax1.set_title(title1, pad=10)\n",
    "    sp.critical_difference_diagram(pd.Series(ranks2.mean(0), index=labels2), pvals2, ax=ax2)\n",
    "    ax2.set_title(title2, pad=10)\n",
    "    plt.tight_layout()\n",
    "    _save_and_show(fig, PLOTS_DIR / fname)\n",
    "\n",
    "    base_tag = Path(fname).stem\n",
    "    section(f\"Full statistics — LEFT panel: {title1}\")\n",
    "    run_friedman(matrix1, block_name=\"folds\", col_labels=labels1, fname_tag=f\"{base_tag}__left\")\n",
    "\n",
    "    section(f\"Full statistics — RIGHT panel: {title2}\")\n",
    "    run_friedman(matrix2, block_name=\"folds\", col_labels=labels2, fname_tag=f\"{base_tag}__right\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Reuse for chain_ERCcv_lr — only for e5_base student == label source\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def result_path(student_emb: str, regressor: str, method: str,\n",
    "                pct: int, K: int, Mmax: int, variant: str,\n",
    "                labels_tag: Optional[str] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Build results path for baseline/full CSVs.\n",
    "    If labels_tag is provided (e.g., 'labels_local_lasso'), it is inserted\n",
    "    before the final '__{variant}' segment to keep files disambiguated.\n",
    "    \"\"\"\n",
    "    tag = f\"__{labels_tag}\" if labels_tag else \"\"\n",
    "    fname = f\"rrmse_perfold_{student_emb}__{regressor}__{method}__pct{pct}_K{K}__Mmax{Mmax}{tag}__{variant}.csv\"\n",
    "    return RES_DIR / fname\n",
    "\n",
    "def ensure_chainlr_from_legacy_pct(student_emb: str, method: str, pct: int, K: int,\n",
    "                                   N_SEEDS: int, Mmax: int, labels_tag: Optional[str] = None) -> bool:\n",
    "    # Only reuse legacy artifacts for the DEFAULT label source (no tag).\n",
    "    if labels_tag:\n",
    "        return False\n",
    "    if not (student_emb == \"e5_base\" and LABEL_EMB == \"e5_base\"):\n",
    "        return False\n",
    "    mapping = {100: (\"n96_sps1\", 96), 200: (\"n192_sps2\", 192), 400: (\"n384_sps4\", 384)}\n",
    "    if pct not in mapping:\n",
    "        return False\n",
    "    tag, _ = mapping[pct]\n",
    "    legacy_root = RES_DIR\n",
    "    legacy_full = legacy_root / f\"rrmse_perfold_e5_base__chain_ERCcv_lr__{method}__{tag}__full.csv\"\n",
    "    if not legacy_full.exists():\n",
    "        log.info(f\"[reuse/chain_lr] Missing legacy {legacy_full.name} → recompute instead.\")\n",
    "        return False\n",
    "\n",
    "    new_full = result_path(student_emb, \"chain_ERCcv_lr\", method, pct, K, Mmax, \"full\", labels_tag=None)\n",
    "    try:\n",
    "        if new_full.exists():\n",
    "            n_rows = sum(1 for _ in open(new_full, \"r\", encoding=\"utf-8\")) - 1\n",
    "            if n_rows >= N_SEEDS:\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df_old = pd.read_csv(legacy_full).iloc[:N_SEEDS].copy()\n",
    "    keep_cols = [c for c in df_old.columns if c.startswith(\"rrmse_domain\")]\n",
    "    if \"median_rrmse_fold\" in df_old.columns:\n",
    "        keep_cols = [\"median_rrmse_fold\"] + keep_cols\n",
    "\n",
    "    fold_col = df_old[\"fold\"].astype(int).values if \"fold\" in df_old.columns else np.arange(N_SEEDS, dtype=int)\n",
    "    df_new = pd.DataFrame({\n",
    "        \"fold\": fold_col,\n",
    "        \"method\": method,\n",
    "        \"pct\": pct,\n",
    "        \"K\": K,\n",
    "        \"M_max\": Mmax,\n",
    "        \"student\": \"S1\",\n",
    "        \"embedding\": student_emb,\n",
    "        \"regressor\": \"chain_ERCcv_lr\",\n",
    "        \"variant\": \"full\",\n",
    "    })\n",
    "    for c in keep_cols:\n",
    "        df_new[c] = df_old[c].values\n",
    "\n",
    "    new_full.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new.to_csv(new_full, index=False)\n",
    "    log.info(f\"[reuse/chain_lr] Wrote {new_full.name} from legacy {legacy_full.name}\")\n",
    "    return True\n",
    "\n",
    "def run_for_embedding(emb_key: str):\n",
    "    \"\"\"Run the whole pipeline for a single student embedding key.\"\"\"\n",
    "    global STUDENT_EMB\n",
    "    if emb_key not in EMBEDDING_SPECS:\n",
    "        raise ValueError(f\"Unknown embedding '{emb_key}'. Allowed: {list(EMBEDDING_SPECS)}\")\n",
    "    prev = STUDENT_EMB\n",
    "    try:\n",
    "        STUDENT_EMB = emb_key\n",
    "        log.info(f\"=== BEGIN run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "        _run_core(labels_tag=None)\n",
    "        log.info(f\"=== END   run() for STUDENT_EMBEDDING={emb_key} ===\")\n",
    "    finally:\n",
    "        STUDENT_EMB = prev\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Combined reports \n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _matrix_for_method_pct(emb: str, method: str, pct: int, Mmax: int, regressors: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build mat of shape (n_folds, n_regressors) using FULL variant,\n",
    "    each cell = median over domain-wise RRMSE for that fold.\n",
    "    Only keeps folds present for ALL regressors (inner join on 'fold').\n",
    "    \"\"\"\n",
    "    K = pct_to_K(pct, 96)\n",
    "    fold_sets = []\n",
    "    per_reg = {}\n",
    "\n",
    "    for reg in regressors:\n",
    "        p_full = RES_DIR / f\"rrmse_perfold_{emb}__{reg}__{method}__pct{pct}_K{K}__Mmax{Mmax}__full.csv\"\n",
    "        if not p_full.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(p_full)\n",
    "        if \"median_rrmse_fold\" in df.columns:\n",
    "            z = df[[\"fold\", \"median_rrmse_fold\"]].rename(columns={\"median_rrmse_fold\":\"med\"})\n",
    "        else:\n",
    "            cols = [c for c in df.columns if c.startswith(\"rrmse_domain\")]\n",
    "            if not cols:\n",
    "                continue\n",
    "            z = df[[\"fold\", *cols]].copy()\n",
    "            z[\"med\"] = z[cols].median(axis=1)\n",
    "            z = z[[\"fold\", \"med\"]]\n",
    "        per_reg[reg] = z\n",
    "        fold_sets.append(set(z[\"fold\"].astype(int).tolist()))\n",
    "\n",
    "    regs = sorted(per_reg.keys())\n",
    "    if len(regs) < 2:\n",
    "        return np.empty((0, 0)), []\n",
    "\n",
    "    common_folds = sorted(set.intersection(*fold_sets)) if fold_sets else []\n",
    "    if len(common_folds) < 2:\n",
    "        return np.empty((0, 0)), []\n",
    "\n",
    "    mats = []\n",
    "    for reg in regs:\n",
    "        z = per_reg[reg]\n",
    "        z = z[z[\"fold\"].isin(common_folds)].sort_values(\"fold\")\n",
    "        mats.append(z[\"med\"].to_numpy(dtype=np.float32))\n",
    "\n",
    "    mat = np.vstack(mats).T  # rows = folds, cols = regs\n",
    "    return mat, regs\n",
    "\n",
    "def say(msg: str):\n",
    "    \"\"\"Print once to screen and write once to run.log (no console duplicates).\"\"\"\n",
    "    stream = getattr(sys, \"__stdout__\", sys.stdout)\n",
    "    print(msg, file=stream, flush=True)  \n",
    "    _say_logger.info(msg)          \n",
    "\n",
    "\n",
    "def _bootstrap_delta_ci_from_files(p_base: Path, p_full: Path, B: int = BOOT_B, seed: int = SEED):\n",
    "    \"\"\"Hierarchical micro-bootstrap for Δ median (baseline - full) with 95% CI.\"\"\"\n",
    "    db = pd.read_csv(p_base)\n",
    "    df = pd.read_csv(p_full)\n",
    "    cols = [c for c in db.columns if c.startswith(_DOM_PREFIX)]\n",
    "    if not cols:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    folds = sorted(set(db['fold']).intersection(set(df['fold'])))\n",
    "    if not folds:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    xb = db.set_index('fold')[cols].loc[folds].to_numpy(dtype=np.float32)\n",
    "    xf = df.set_index('fold')[cols].loc[folds].to_numpy(dtype=np.float32)\n",
    "    n_f, n_d = xb.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    deltas = np.empty(B, dtype=np.float32)\n",
    "    for b in range(B):\n",
    "        f_idx = rng.integers(0, n_f, size=n_f, endpoint=False)\n",
    "        d_idx = rng.integers(0, n_d, size=n_d, endpoint=False)\n",
    "        xb_s = xb[f_idx][:, d_idx]\n",
    "        xf_s = xf[f_idx][:, d_idx]\n",
    "        deltas[b] = np.median(xb_s) - np.median(xf_s)\n",
    "    lo, hi = np.percentile(deltas, [2.5, 97.5])\n",
    "    return float(np.median(deltas)), float(lo), float(hi)\n",
    "\n",
    "\n",
    "def build_combined_reports_across_embeddings(include_alt_labels: bool = True):\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    methods = sorted(avail.keys()) if (METHODS is None) else [m for m in sorted(avail.keys()) if m in METHODS]\n",
    "\n",
    "    pat = re.compile(\n",
    "        r\"^rrmse_perfold_(?P<emb>.+?)__(?P<reg>.+?)__(?P<meth>.+?)__pct(?P<pct>\\d+)_K(?P<K>\\d+)__Mmax(?P<M>\\d+)\"\n",
    "        r\"(?:__(?P<labels>labels_[A-Za-z0-9_]+))?__(?P<var>baseline|full)\\.csv$\"\n",
    "    )\n",
    "\n",
    "    summary_rows, wil_rows, cliffs_rows = [], [], []\n",
    "    pooled_pairs = {}             # key=(meth,pct,labels_source) -> {\"base\":[...], \"full\":[...]}\n",
    "    pooled_pairs_by_embed = {}    # key=(emb,meth,pct,labels_source) -> {\"base\":[...], \"full\":[...]}\n",
    "    bootstrap_rows = []\n",
    "\n",
    "    for emb in STUDENT_EMBEDDINGS:\n",
    "        files = sorted([p for p in RES_DIR.glob(f\"rrmse_perfold_{emb}__*__full.csv\") if pat.match(p.name)])\n",
    "        for f_full in files:\n",
    "            m = pat.match(f_full.name)\n",
    "            if not m: \n",
    "                continue\n",
    "            d = m.groupdict()\n",
    "            reg, meth = d[\"reg\"], d[\"meth\"]\n",
    "            pct, K, Mmax = int(d[\"pct\"]), int(d[\"K\"]), int(d[\"M\"])\n",
    "            labels_tag = (d.get(\"labels\") or \"\").strip()\n",
    "            labels_source = labels_tag if labels_tag else \"default\"\n",
    "            if (not include_alt_labels) and labels_tag:\n",
    "                continue\n",
    "            if pct not in PCT_LIST or (methods and meth not in methods): \n",
    "                continue\n",
    "\n",
    "            p_full = f_full\n",
    "            p_base = result_path(emb, reg, meth, pct, K, Mmax, \"baseline\",\n",
    "                                 labels_tag=None if labels_source == \"default\" else labels_source)\n",
    "            if not p_base.exists(): \n",
    "                continue\n",
    "\n",
    "            # global medians\n",
    "            g_base = _global_median_from_file(p_base)\n",
    "            g_full = _global_median_from_file(p_full)\n",
    "            summary_rows.append({\n",
    "                \"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "                \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"baseline\":g_base,\"full\":g_full,\n",
    "                \"labels_source\":labels_source,\n",
    "            })\n",
    "\n",
    "            # paired arrays (fold×domain flattened)\n",
    "            xb, xf = _flat_arrays(p_base, p_full)\n",
    "            pooled_pairs.setdefault((meth, pct, labels_source), {\"base\":[], \"full\":[]})\n",
    "            pooled_pairs[(meth, pct, labels_source)][\"base\"].extend(xb.tolist())\n",
    "            pooled_pairs[(meth, pct, labels_source)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "            pooled_pairs_by_embed.setdefault((emb, meth, pct, labels_source), {\"base\":[], \"full\":[]})\n",
    "            pooled_pairs_by_embed[(emb, meth, pct, labels_source)][\"base\"].extend(xb.tolist())\n",
    "            pooled_pairs_by_embed[(emb, meth, pct, labels_source)][\"full\"].extend(xf.tolist())\n",
    "\n",
    "            # wilcoxon per-config (one-sided less)\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "            except Exception:\n",
    "                p_raw = 1.0\n",
    "            dlt, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "            wil_rows.append({\"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "                             \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"labels_source\":labels_source,\"p_raw\":float(p_raw)})\n",
    "            cliffs_rows.append({\"student\":\"S1\",\"embedding\":emb,\"regressor\":reg,\"method\":meth,\n",
    "                                \"pct\":pct,\"K\":K,\"M_max\":Mmax,\"labels_source\":labels_source,\n",
    "                                \"cliffs_delta_paired\":float(dlt),\"npos\":int(npos),\"nneg\":int(nneg),\n",
    "                                \"nzero\":int(nzero),\"magnitude\":mag})\n",
    "\n",
    "            # bootstrap Δ median CI\n",
    "            d_med, ci_lo, ci_hi = _bootstrap_delta_ci_from_files(p_base, p_full, B=BOOT_B, seed=SEED)\n",
    "            bootstrap_rows.append({\n",
    "                \"embedding\":emb,\"regressor\":reg,\"method\":meth,\"pct\":pct,\"K\":K,\"M_max\":Mmax,\n",
    "                \"labels_source\":labels_source,\"delta_median\":d_med,\"ci_lo\":ci_lo,\"ci_hi\":ci_hi\n",
    "            })\n",
    "\n",
    "    if not summary_rows:\n",
    "        say(\"[review] No result pairs found to summarise.\")\n",
    "        return\n",
    "\n",
    "    # ---------- summaries ----------\n",
    "    wide = pd.DataFrame(summary_rows).sort_values(\n",
    "        [\"labels_source\",\"embedding\",\"regressor\",\"method\",\"pct\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    tol = 1e-9\n",
    "    def _cmp(row):\n",
    "        b, f = row[\"baseline\"], row[\"full\"]\n",
    "        if pd.isna(b) or pd.isna(f): return \"n/a\"\n",
    "        if (b - f) > tol:  return \"better\"\n",
    "        if (f - b) > tol:  return \"worse\"\n",
    "        return \"same\"\n",
    "    wide[\"baseline_vs_full\"] = wide.apply(_cmp, axis=1)\n",
    "\n",
    "    OUT_TABLES.mkdir(parents=True, exist_ok=True)\n",
    "    wide.to_csv(OUT_TABLES / \"summary_median_rrmse__ALL.csv\", index=False)\n",
    "\n",
    "    comp = wide.copy()\n",
    "    comp[\"delta\"] = comp[\"baseline\"] - comp[\"full\"]\n",
    "    comp[\"rel_change_pct\"] = 100.0 * comp[\"delta\"] / comp[\"baseline\"].replace(0, np.nan)\n",
    "    comp.to_csv(OUT_TABLES / \"summary_median_rrmse_with_delta__ALL.csv\", index=False)\n",
    "\n",
    "    # legacy default filenames prefer 'default' source\n",
    "    default_sub = wide[wide[\"labels_source\"] == \"default\"]\n",
    "    if not default_sub.empty:\n",
    "        default_sub.to_csv(OUT_TABLES / \"summary_median_rrmse.csv\", index=False)\n",
    "        comp[comp[\"labels_source\"] == \"default\"].to_csv(\n",
    "            OUT_TABLES / \"summary_median_rrmse_with_delta.csv\", index=False\n",
    "        )\n",
    "    else:\n",
    "        wide.to_csv(OUT_TABLES / \"summary_median_rrmse.csv\", index=False)\n",
    "        comp.to_csv(OUT_TABLES / \"summary_median_rrmse_with_delta.csv\", index=False)\n",
    "\n",
    "    # per label-source CSVs (default / labels_local_lasso / …)\n",
    "    for lbl in sorted(wide[\"labels_source\"].unique()):\n",
    "        sub = wide[wide[\"labels_source\"] == lbl].copy()\n",
    "        sub.to_csv(OUT_TABLES / f\"summary_median_rrmse__{lbl}.csv\", index=False)\n",
    "        sc = comp[comp[\"labels_source\"] == lbl].copy()\n",
    "        sc.to_csv(OUT_TABLES / f\"summary_median_rrmse_with_delta__{lbl}.csv\", index=False)\n",
    "\n",
    "    # per-embedding CSVs (compat: __e5_base copies)\n",
    "    for emb in sorted(wide[\"embedding\"].unique()):\n",
    "        wide[wide[\"embedding\"] == emb].to_csv(OUT_TABLES / f\"summary_median_rrmse__{emb}.csv\", index=False)\n",
    "        comp[comp[\"embedding\"] == emb].to_csv(OUT_TABLES / f\"summary_median_rrmse_with_delta__{emb}.csv\", index=False)\n",
    "\n",
    "    # ---------- pooled tests (ALL + per-embed) ----------\n",
    "    pooled_rows = []\n",
    "    for (meth, pct, lbl), pair in pooled_pairs.items():\n",
    "        xb = np.array(pair[\"base\"], dtype=np.float32)\n",
    "        xf = np.array(pair[\"full\"], dtype=np.float32)\n",
    "        if xb.size == 0 or xf.size == 0: \n",
    "            continue\n",
    "        try:\n",
    "            _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "        except Exception:\n",
    "            p_raw = 1.0\n",
    "        dlt, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "        pooled_rows.append({\"method\":meth,\"pct\":pct,\"labels_source\":lbl,\n",
    "                            \"p_raw\":float(p_raw),\"cliffs_delta\":float(dlt),\n",
    "                            \"npos\":int(npos),\"nneg\":int(nneg),\"nzero\":int(nzero),\"magnitude\":mag})\n",
    "    pooled_df = pd.DataFrame(pooled_rows).sort_values([\"labels_source\",\"method\",\"pct\"])\n",
    "    if not pooled_df.empty:\n",
    "        pooled_df.to_csv(OUT_TABLES / \"wilcoxon_cliffs_pooled__ALL.csv\", index=False)\n",
    "        # compat split files\n",
    "        pooled_df.rename(columns={\"p_raw\":\"p_value\"}).to_csv(OUT_TABLES / \"wilcoxon_pooled_vs_baseline.csv\", index=False)\n",
    "        pooled_df[[\"method\",\"pct\",\"labels_source\",\"cliffs_delta\",\"magnitude\",\"npos\",\"nneg\",\"nzero\"]].to_csv(\n",
    "            OUT_TABLES / \"cliffs_delta_pooled_vs_baseline.csv\", index=False\n",
    "        )\n",
    "\n",
    "    # per-embed pooled\n",
    "    pooled_embed_rows = []\n",
    "    for (emb, meth, pct, lbl), pair in pooled_pairs_by_embed.items():\n",
    "        xb = np.array(pair[\"base\"], dtype=np.float32)\n",
    "        xf = np.array(pair[\"full\"], dtype=np.float32)\n",
    "        if xb.size == 0 or xf.size == 0:\n",
    "            continue\n",
    "        try:\n",
    "            _, p_raw = wilcoxon(xf, xb, zero_method=\"pratt\", alternative=\"less\")\n",
    "        except Exception:\n",
    "            p_raw = 1.0\n",
    "        dlt, npos, nneg, nzero, mag = paired_cliffs_delta(xf, xb)\n",
    "        pooled_embed_rows.append({\"embedding\":emb,\"method\":meth,\"pct\":pct,\"labels_source\":lbl,\n",
    "                                  \"p_raw\":float(p_raw),\"cliffs_delta\":float(dlt),\n",
    "                                  \"npos\":int(npos),\"nneg\":int(nneg),\"nzero\":int(nzero),\"magnitude\":mag})\n",
    "    pooled_e_df = pd.DataFrame(pooled_embed_rows).sort_values([\"embedding\",\"labels_source\",\"method\",\"pct\"])\n",
    "    if not pooled_e_df.empty:\n",
    "        # compat: write __e5_base copies if present\n",
    "        for emb in pooled_e_df[\"embedding\"].unique():\n",
    "            sub = pooled_e_df[pooled_e_df[\"embedding\"]==emb].drop(columns=[\"embedding\"])\n",
    "            sub.rename(columns={\"p_raw\":\"p_value\"}).to_csv(OUT_TABLES / f\"wilcoxon_pooled_vs_baseline__{emb}.csv\", index=False)\n",
    "            sub[[\"method\",\"pct\",\"labels_source\",\"cliffs_delta\",\"magnitude\",\"npos\",\"nneg\",\"nzero\"]].to_csv(\n",
    "                OUT_TABLES / f\"cliffs_delta_pooled_vs_baseline__{emb}.csv\", index=False\n",
    "            )\n",
    "\n",
    "    # ---------- per-config wilcoxon (raw + Holm) & cliffs (compat names) ----------\n",
    "    if wil_rows:\n",
    "        wil_df = pd.DataFrame(wil_rows).sort_values(\n",
    "            [\"labels_source\",\"embedding\",\"regressor\",\"method\",\"pct\"]\n",
    "        )\n",
    "        wil_df.to_csv(OUT_TABLES / \"wilcoxon_vs_baseline__ALL.csv\", index=False)\n",
    "\n",
    "        # Holm within each (labels_source, embedding, regressor, method)\n",
    "        holm_chunks = []\n",
    "        for key, grp in wil_df.groupby([\"labels_source\",\"embedding\",\"regressor\",\"method\"], dropna=False):\n",
    "            _, p_adj, _, _ = multipletests(grp[\"p_raw\"].values, method=\"holm\")\n",
    "            g2 = grp.copy()\n",
    "            g2[\"p_holm\"] = p_adj\n",
    "            holm_chunks.append(g2)\n",
    "        holm_df = pd.concat(holm_chunks, axis=0).sort_values(\n",
    "            [\"labels_source\",\"embedding\",\"regressor\",\"method\",\"pct\"]\n",
    "        )\n",
    "        holm_df.to_csv(OUT_TABLES / \"wilcoxon_holm_vs_baseline.csv\", index=False)\n",
    "        for emb in holm_df[\"embedding\"].unique():\n",
    "            holm_df[holm_df[\"embedding\"]==emb].to_csv(OUT_TABLES / f\"wilcoxon_holm_vs_baseline__{emb}.csv\", index=False)\n",
    "\n",
    "    if cliffs_rows:\n",
    "        cliffs_df = pd.DataFrame(cliffs_rows).sort_values(\n",
    "            [\"labels_source\",\"embedding\",\"regressor\",\"method\",\"pct\"]\n",
    "        )\n",
    "        cliffs_df.to_csv(OUT_TABLES / \"cliffs_delta_paired__ALL.csv\", index=False)\n",
    "        # compat names\n",
    "        cliffs_df.to_csv(OUT_TABLES / \"cliffs_delta_vs_baseline.csv\", index=False)\n",
    "        for emb in cliffs_df[\"embedding\"].unique():\n",
    "            cliffs_df[cliffs_df[\"embedding\"]==emb].to_csv(OUT_TABLES / f\"cliffs_delta_vs_baseline__{emb}.csv\", index=False)\n",
    "\n",
    "    # ---------- bootstrap Δ CI ----------\n",
    "    if bootstrap_rows:\n",
    "        pd.DataFrame(bootstrap_rows).sort_values(\n",
    "            [\"labels_source\",\"embedding\",\"regressor\",\"method\",\"pct\"]\n",
    "        ).to_csv(OUT_TABLES / \"bootstrap_delta_ci.csv\", index=False)\n",
    "\n",
    "    # ---------- plots ----------\n",
    "    # A) per label-source (you already write __{lbl}); keep that.\n",
    "    for meth in sorted(wide['method'].unique()):\n",
    "        for lbl in sorted(wide['labels_source'].unique()):\n",
    "            sub = wide[(wide['method']==meth) & (wide['labels_source']==lbl)]\n",
    "            if sub.empty: \n",
    "                continue\n",
    "            grp = sub.groupby('pct', as_index=False)[['baseline', 'full']].median()\n",
    "            fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "            ax.plot(grp['pct'], grp['baseline'], marker='o', label='baseline')\n",
    "            ax.plot(grp['pct'], grp['full'],     marker='o', label='full')\n",
    "            ax.set_xlabel('%K'); ax.set_ylabel('Global median RRMSE')\n",
    "            ax.set_title(f'{meth} [{lbl}]'); ax.legend()\n",
    "            fig.savefig(PLOTS_DIR / f\"combined_rrmse_vs_pct__{meth}__{lbl}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "            # Δ plots per label-source\n",
    "            grp['delta'] = grp['baseline'] - grp['full']\n",
    "            fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "            ax.plot(grp['pct'], grp['delta'], marker='o')\n",
    "            ax.set_xlabel('%K'); ax.set_ylabel('Δ median RRMSE (baseline - full)')\n",
    "            ax.set_title(f'{meth} [{lbl}] Δ vs %K')\n",
    "            fig.savefig(PLOTS_DIR / f\"combined_delta_vs_pct__{meth}__{lbl}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "        # B) overlay across label sources (FULL)\n",
    "        piv = (wide[wide['method']==meth]\n",
    "               .groupby(['labels_source','pct'], as_index=False)['full']\n",
    "               .median())\n",
    "        if not piv.empty and piv['labels_source'].nunique() >= 2:\n",
    "            fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "            for lbl, chunk in piv.groupby('labels_source'):\n",
    "                ax.plot(chunk['pct'], chunk['full'], marker='o', label=f'{lbl} (full)')\n",
    "            ax.set_xlabel('%K'); ax.set_ylabel('Global median RRMSE')\n",
    "            ax.set_title(f'{meth} — label sources (FULL)'); ax.legend()\n",
    "            fig.savefig(PLOTS_DIR / f\"combined_rrmse_vs_pct__{meth}__compare_labels.png\", bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "        # C) compat “no-suffix” and per-embedding figures (use default label-source)\n",
    "        dsub = wide[(wide['method']==meth) & (wide['labels_source']=='default')]\n",
    "        if not dsub.empty:\n",
    "            g = dsub.groupby('pct', as_index=False)[['baseline','full']].median()\n",
    "            # no-suffix rrMSE\n",
    "            fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "            ax.plot(g['pct'], g['baseline'], marker='o', label='baseline')\n",
    "            ax.plot(g['pct'], g['full'],     marker='o', label='full')\n",
    "            ax.set_xlabel('%K'); ax.set_ylabel('Global median RRMSE')\n",
    "            ax.set_title(f'{meth}'); ax.legend()\n",
    "            fig.savefig(PLOTS_DIR / f\"combined_rrmse_vs_pct__{meth}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close(fig)\n",
    "            # no-suffix Δ\n",
    "            g['delta'] = g['baseline'] - g['full']\n",
    "            fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "            ax.plot(g['pct'], g['delta'], marker='o')\n",
    "            ax.set_xlabel('%K'); ax.set_ylabel('Δ median RRMSE (baseline - full)')\n",
    "            ax.set_title(f'{meth} Δ vs %K')\n",
    "            fig.savefig(PLOTS_DIR / f\"combined_delta_vs_pct__{meth}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "            # per-embedding copies (e.g., __e5_base)\n",
    "            for emb in sorted(dsub['embedding'].unique()):\n",
    "                g_emb = dsub[dsub['embedding']==emb].groupby('pct', as_index=False)[['baseline','full']].median()\n",
    "                fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "                ax.plot(g_emb['pct'], g_emb['baseline'], marker='o', label='baseline')\n",
    "                ax.plot(g_emb['pct'], g_emb['full'],     marker='o', label='full')\n",
    "                ax.set_xlabel('%K'); ax.set_ylabel('Global median RRMSE')\n",
    "                ax.set_title(f'{meth} [{emb}]'); ax.legend()\n",
    "                fig.savefig(PLOTS_DIR / f\"combined_rrmse_vs_pct__{meth}__{emb}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "                g_emb['delta'] = g_emb['baseline'] - g_emb['full']\n",
    "                fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "                ax.plot(g_emb['pct'], g_emb['delta'], marker='o')\n",
    "                ax.set_xlabel('%K'); ax.set_ylabel('Δ median RRMSE (baseline - full)')\n",
    "                ax.set_title(f'{meth} [{emb}] Δ vs %K')\n",
    "                fig.savefig(PLOTS_DIR / f\"combined_delta_vs_pct__{meth}__{emb}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "    say(\"[review] Summaries written. (combined, per label-source, legacy-compat files)\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Core run (compute) with labels_tag support\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _run_core(labels_tag: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Compute pipeline body. Trains/evaluates students using the current global LABEL_EMB/LABEL_MODEL.\n",
    "    Results are written with an optional labels_tag to disambiguate alt label sources.\n",
    "    \"\"\"\n",
    "    t0_all = time.time()\n",
    "    write_run_config()\n",
    "\n",
    "    # 0) Load seeds and vectors\n",
    "    seeds_df = load_seeds()\n",
    "    N_SEEDS = len(seeds_df)\n",
    "    X_seed = ensure_seed_vectors(seeds_df, STUDENT_EMB)\n",
    "    Y_seed = seeds_df[TARGET_COLS].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 1) Discover available methods & pick M_max per method from e_2 labels\n",
    "    avail = discover_methods_and_M_from_g2(LABEL_EMB)\n",
    "    methods = sorted(avail.keys()) if (METHODS is None) else [m for m in sorted(avail.keys()) if m in METHODS]\n",
    "    if not methods:\n",
    "        raise FileNotFoundError(f\"No labeled synthetics found in {G2_DIR} for embedding={LABEL_EMB}\")\n",
    "\n",
    "    folds = list(range(N_SEEDS)) if MAX_FOLDS <= 0 else list(range(min(MAX_FOLDS, N_SEEDS)))\n",
    "    say(f\"[run] STUDENT_EMB={STUDENT_EMB} | LABEL_SRC={LABEL_EMB}+{LABEL_MODEL} | labels_tag={labels_tag or 'default'}\")\n",
    "    say(f\"[run] methods={methods} | folds={len(folds)} | PCT_LIST={PCT_LIST}\")\n",
    "\n",
    "    for method in methods:\n",
    "        Mmax = max(avail[method])\n",
    "        # Ensure synth cache exists and load once per method for student embedding\n",
    "        npy_path, idx_path = ensure_synth_cache(method, Mmax, STUDENT_EMB)\n",
    "        X_synth = np.load(npy_path)  # shape (Mmax, d)\n",
    "        n_synth_rows = X_synth.shape[0]\n",
    "\n",
    "        for pct in PCT_LIST:\n",
    "            K = pct_to_K(pct, N_SEEDS)\n",
    "            if K > n_synth_rows:\n",
    "                log.warning(f\"[{method}] pct={pct} → K={K} exceeds Mmax={n_synth_rows}; clipping to Mmax\")\n",
    "                K = n_synth_rows\n",
    "\n",
    "            for reg in STUDENTS:\n",
    "                # Output files (idempotent)\n",
    "                p_base = result_path(STUDENT_EMB, reg, method, pct, K, Mmax, \"baseline\", labels_tag=labels_tag)\n",
    "                p_full = result_path(STUDENT_EMB, reg, method, pct, K, Mmax, \"full\",      labels_tag=labels_tag)\n",
    "\n",
    "                # Skip if already complete\n",
    "                def _is_complete(p: Path) -> bool:\n",
    "                    try:\n",
    "                        if not p.exists(): return False\n",
    "                        n_rows = sum(1 for _ in open(p, \"r\", encoding=\"utf-8\")) - 1\n",
    "                        return max(0, n_rows) >= len(folds)\n",
    "                    except Exception:\n",
    "                        return False\n",
    "\n",
    "                base_done = _is_complete(p_base)\n",
    "                full_done = _is_complete(p_full)\n",
    "\n",
    "                # Optional legacy reuse for chain_lr (full only) — only for default tag\n",
    "                if (reg == \"chain_ERCcv_lr\") and (pct in {100,200,400}) and not full_done:\n",
    "                    reused = ensure_chainlr_from_legacy_pct(STUDENT_EMB, method, pct, K, N_SEEDS, Mmax, labels_tag=labels_tag)\n",
    "                    if reused:\n",
    "                        full_done = True\n",
    "\n",
    "                if base_done and full_done:\n",
    "                    log.info(f\"[skip] {p_base.name} & {p_full.name} complete.\")\n",
    "                    continue\n",
    "\n",
    "                rows_base, rows_full = [], []\n",
    "                t_start = time.time()\n",
    "\n",
    "                for i, fold_idx in enumerate(folds):\n",
    "                    if (i % LOG_EVERY) == 0:\n",
    "                        log.info(f\"[{method} | {reg} | pct={pct}] fold {i+1}/{len(folds)} …\")\n",
    "\n",
    "                    tr_idx = np.array([j for j in range(N_SEEDS) if j != fold_idx], dtype=int)\n",
    "                    y_dummy = np.mean(Y_seed[tr_idx], axis=0, dtype=np.float32)\n",
    "\n",
    "                    # Build model from HPs (teacher pickle or json)\n",
    "                    teacher_est = load_teacher_fold_estimator(fold_idx, reg, STUDENT_EMB)\n",
    "                    hp = hp_from_teacher_or_json(fold_idx, reg, STUDENT_EMB, teacher_est)\n",
    "                    hp_out = HP_PERFOLD / reg / method / f\"n{Mmax}\" / f\"pct{pct}_K{K}\"\n",
    "                    hp_out.mkdir(parents=True, exist_ok=True)\n",
    "                    (hp_out / f\"fold{fold_idx:02d}_best.json\").write_text(json.dumps(hp, indent=2))\n",
    "\n",
    "                    model_base = build_with_hp(reg, hp)\n",
    "\n",
    "                    # ---------- Baseline (seeds-only) ----------\n",
    "                    if not base_done:\n",
    "                        model_base.fit(X_seed[tr_idx], Y_seed[tr_idx])\n",
    "                        y_true = Y_seed[fold_idx:fold_idx+1]\n",
    "                        y_pred = model_base.predict(X_seed[fold_idx:fold_idx+1])\n",
    "                        rr = rrmse_vs_dummy(y_true, y_pred, y_dummy.reshape(1, -1)).flatten()\n",
    "                        row = {\n",
    "                            \"fold\": int(fold_idx),\n",
    "                            \"method\": method,\n",
    "                            \"pct\": int(pct),\n",
    "                            \"K\": int(K),\n",
    "                            \"M_max\": int(Mmax),\n",
    "                            \"student\": \"S1\",\n",
    "                            \"embedding\": STUDENT_EMB,\n",
    "                            \"regressor\": reg,\n",
    "                            \"variant\": \"baseline\",\n",
    "                            \"median_rrmse_fold\": float(np.median(rr)),\n",
    "                        }\n",
    "                        for t_idx in range(1, 15):\n",
    "                            row[f\"{_DOM_PREFIX}{t_idx}\"] = float(rr[t_idx-1])\n",
    "                        rows_base.append(row)\n",
    "\n",
    "                    # ---------- Full (seeds + synthetics) ----------\n",
    "                    if not full_done:\n",
    "                        # Labels for this fold, method, Mmax, label source\n",
    "                        lbl_csv = g2_label_file(\n",
    "                            fold_idx, method, Mmax, LABEL_EMB, LABEL_MODEL,\n",
    "                            strict_model=True\n",
    "                        )\n",
    "                        if not lbl_csv.exists():\n",
    "                            raise FileNotFoundError(f\"Missing labels: {lbl_csv}\")\n",
    "                        df_lbl = pd.read_csv(lbl_csv)\n",
    "                        if any(c not in df_lbl.columns for c in TARGET_COLS):\n",
    "                            raise RuntimeError(f\"{lbl_csv.name}: missing target cols {TARGET_COLS}\")\n",
    "                        # first K rows\n",
    "                        Y_syn = df_lbl[TARGET_COLS].to_numpy(dtype=np.float32)[:K, :]\n",
    "                        X_syn = X_synth[:K, :]\n",
    "\n",
    "                        model_full = build_with_hp(reg, hp)\n",
    "                        X_tr_full = np.vstack([X_seed[tr_idx], X_syn])\n",
    "                        Y_tr_full = np.vstack([Y_seed[tr_idx], Y_syn])\n",
    "                        model_full.fit(X_tr_full, Y_tr_full)\n",
    "\n",
    "                        y_true = Y_seed[fold_idx:fold_idx+1]\n",
    "                        y_pred = model_full.predict(X_seed[fold_idx:fold_idx+1])\n",
    "                        rr = rrmse_vs_dummy(y_true, y_pred, y_dummy.reshape(1, -1)).flatten()\n",
    "                        row = {\n",
    "                            \"fold\": int(fold_idx),\n",
    "                            \"method\": method,\n",
    "                            \"pct\": int(pct),\n",
    "                            \"K\": int(K),\n",
    "                            \"M_max\": int(Mmax),\n",
    "                            \"student\": \"S1\",\n",
    "                            \"embedding\": STUDENT_EMB,\n",
    "                            \"regressor\": reg,\n",
    "                            \"variant\": \"full\",\n",
    "                            \"median_rrmse_fold\": float(np.median(rr)),\n",
    "                        }\n",
    "                        for t_idx in range(1, 15):\n",
    "                            row[f\"{_DOM_PREFIX}{t_idx}\"] = float(rr[t_idx-1])\n",
    "                        rows_full.append(row)\n",
    "\n",
    "                # Write outputs\n",
    "                if rows_base and (not base_done):\n",
    "                    pd.DataFrame(rows_base).sort_values(\"fold\").to_csv(p_base, index=False)\n",
    "                    log.info(f\"[write] {p_base.name} ({len(rows_base)} rows, { _fmt_dt(time.time()-t_start) })\")\n",
    "                if rows_full and (not full_done):\n",
    "                    pd.DataFrame(rows_full).sort_values(\"fold\").to_csv(p_full, index=False)\n",
    "                    log.info(f\"[write] {p_full.name} ({len(rows_full)} rows, { _fmt_dt(time.time()-t_start) })\")\n",
    "\n",
    "    say(f\"[run] completed in { _fmt_dt(time.time()-t0_all) }\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Pipelines: compute & review\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _run_students_for_label_source(label_emb: str,\n",
    "                                   label_model: str,\n",
    "                                   labels_tag: Optional[str],\n",
    "                                   restrict_students: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    Execute the student-scoring pipeline using the given label source.\n",
    "    If restrict_students is provided, only run those student keys.\n",
    "    Results are written with an optional labels_tag appended in filenames.\n",
    "    \"\"\"\n",
    "    global LABEL_EMB, LABEL_MODEL, STUDENTS\n",
    "\n",
    "    _orig_label_emb, _orig_label_model = LABEL_EMB, LABEL_MODEL\n",
    "    students_backup = list(STUDENTS)\n",
    "    try:\n",
    "        LABEL_EMB, LABEL_MODEL = label_emb, label_model\n",
    "\n",
    "        if restrict_students is not None:\n",
    "            STUDENTS = [s for s in students_backup if s in restrict_students]\n",
    "            if not STUDENTS:\n",
    "                log.warning(\"No matching students in restrict_students=%s; skipping.\", restrict_students)\n",
    "                return\n",
    "\n",
    "        log.info(\"[labels] Running students %s on labels %s+%s (tag=%s)\", STUDENTS, LABEL_EMB, LABEL_MODEL, labels_tag)\n",
    "        _run_core(labels_tag=labels_tag)\n",
    "    finally:\n",
    "        LABEL_EMB, LABEL_MODEL = _orig_label_emb, _orig_label_model\n",
    "        STUDENTS = students_backup\n",
    "\n",
    "def run_pipeline_compute():\n",
    "    \"\"\"\n",
    "    Compute pipeline: \n",
    "      1) Default pass — all students on labels E5_base + chain_ERCcv_lr (no tag).\n",
    "      2) Extra pass  — chain_ERCcv_lr student on labels E5_base + local_lasso (tagged).\n",
    "    \"\"\"\n",
    "    # Pass 1 (default)\n",
    "    for emb_key in STUDENT_EMBEDDINGS:\n",
    "        run_for_embedding(emb_key)           # calls _run_core(labels_tag=None)\n",
    "\n",
    "    # Pass 2 (extras)\n",
    "    for job in EXTRA_STUDENT_LABEL_JOBS:\n",
    "        _run_students_for_label_source(\n",
    "            label_emb=job[\"label_emb\"],\n",
    "            label_model=job[\"label_model\"],\n",
    "            labels_tag=job.get(\"labels_tag\"),\n",
    "            restrict_students=[job[\"student\"]],\n",
    "        )\n",
    "\n",
    "def run_pipeline_review():\n",
    "    \"\"\"\n",
    "    Review pipeline (no compute): detect any precomputed student results and build tables/plots.\n",
    "    - Always loads default label-source artifacts if present.\n",
    "    - Also loads alt label-source artifacts (e.g., __labels_local_lasso) if present.\n",
    "    - Writes per-labels-source summaries AND a combined summary for convenience.\n",
    "    \"\"\"\n",
    "    build_combined_reports_across_embeddings(include_alt_labels=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Entry point\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        write_run_config()\n",
    "        if REVIEW_MODE:\n",
    "            run_pipeline_review()\n",
    "        else:\n",
    "            run_pipeline_compute()\n",
    "        print(\"Run completed.\")\n",
    "    except Exception as e:\n",
    "        log.exception(\"Fatal error in step e_3: %s\", e)\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c48683-d197-4b73-bda2-7f4bbe564579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr8cht_review_anonymous",
   "language": "python",
   "name": "kr8cht_review_anonymous"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
